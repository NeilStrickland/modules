\documentclass{amsart}
%\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{amsrefs}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[type={CC},modifier={by-nc-sa},version={3.0}]{doclicense}

\newif\ifscreen
\newif\iftwo
\newif\ifshowall
\newif\ifshowkeys
\screenfalse
\twotrue
\showallfalse
\showkeystrue

\input xypic

\ifshowkeys
\newcommand{\lbl}[1]{\label{#1}\textup{[\texttt{#1}]}\ \\}
\else
\newcommand{\lbl}{\label}
\fi

\input soldefs
%\input coldefs

\newcommand{\ann}       {\operatorname{ann}}

\newcommand{\bbm}       {\left[\begin{matrix}}
\newcommand{\bsm}       {\left[\begin{smallmatrix}}
\newcommand{\ebm}       {\end{matrix}\right]}
\newcommand{\esm}       {\end{smallmatrix}\right]}
\newcommand{\chr}       {\operatorname{char}}
\newcommand{\img}       {\operatorname{image}}
\newcommand{\tors}      {\operatorname{tors}}
\newcommand{\rank}      {\operatorname{rank}}
\newcommand{\spn}       {\operatorname{span}}

\newcommand{\C}         {{\mathbb{C}}}
\newcommand{\N}         {{\mathbb{N}}}
\newcommand{\Q}         {{\mathbb{Q}}}
\newcommand{\R}         {{\mathbb{R}}}
\newcommand{\Z}         {{\mathbb{Z}}}
\newcommand{\Zpl}       {{\mathbb{Z}_{(p)}}}      

\newcommand{\al}        {\alpha}
\newcommand{\alb}       {\overline{\alpha}}
\newcommand{\bt}        {\beta} 
\newcommand{\gm}        {\gamma}
\newcommand{\dl}        {\delta}
\newcommand{\zt}        {\zeta}
\newcommand{\lm}        {\lambda}
\newcommand{\sg}        {\sigma}

\newcommand{\ov}[1]     {\overline{#1}}
\newcommand{\un}[1]     {\underline{#1}}
\newcommand{\sm}        {\setminus}
\newcommand{\sse}       {\subseteq}
\newcommand{\tm}        {\times}
\newcommand{\CRR}       {C^\infty(\R,\R)}
\newcommand{\CRC}       {C^\infty(\R,\C)}
\newcommand{\xra}       {\xrightarrow}
\newcommand{\st}        {\;|\;}
\newcommand{\ip}[1]     {\langle #1\rangle}
\newcommand{\op}        {\oplus}
\newcommand{\half}      {{\textstyle\frac{1}{2}}}
\newcommand{\CP}        {{\mathcal{P}}}
\newcommand{\iffa}      {\Leftrightarrow}

\newcommand{\blockmat}[4]{
 \left(\begin{array}{c|c} #1&#2 \\ \hline #3&#4\end{array}\right)}
\newcommand{\blockvec}[2]{
 \left(\begin{array}{c} #1 \\ \hline #2\end{array}\right)}

\renewcommand{\:}{\colon}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}
% Exercises are numbered separately
\newtheorem{exercise}{Exercise}[section]

\renewenvironment{solution}{\SolutionAtEnd}{\endSolutionAtEnd}
%\renewenvironment{solution}{\SolutionHidden}{\endSolutionHidden}


\begin{document}
\title{Rings, modules and linear algebra}
\author{N.~P.~Strickland}
\date{\today}
\bibliographystyle{abbrv}

\maketitle 

\begin{center}
 This work is licensed under a 
 \href{https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en}{
  Creative Commons Attribution-NonCommercial-ShareAlike license}.
 
 \bigskip

 \doclicenseImage 
\end{center}

\section{Introduction}
\label{sec-intro} 

At the end of the course, you should be able to prove the following:
\begin{enumerate}
\item Let $G$ be an Abelian group of order $p^3$, where $p$ is prime.
 Then $G$ is isomorphic to either $Z_{p^3}$, $\Z_{p^2}\tm\Z_p$ or
 $\Z_p\tm\Z_p\tm\Z_p$.
\item Let $A$ be a $3\tm 3$ matrix of rational numbers satisfying
 $(A+I)^3=0$.  Then $A$ is conjugate to one of the following three
 matrices: 
 \[ \left[\begin{array}{ccc}
      -1 &  1 &  0 \\  0 & -1 &  1 \\  0 &  0 & -1 
    \end{array}\right] \hspace{4em}
    \left[\begin{array}{ccc}
      -1 &  1 &  0 \\  0 & -1 &  0 \\  0 &  0 & -1 
    \end{array}\right] \hspace{4em}
    \left[\begin{array}{ccc}
      -1 &  0 &  0 \\  0 & -1 &  0 \\  0 &  0 & -1 
    \end{array}\right]
 \]
 (In other words, there is an invertible matrix $P$ such that
 $PAP^{-1}$ is one of the three matrices listed.) 
\item Let $f\:\R\xra{}\C$ satisfy the differential equation
 $f''''+f=0$.  Then $f(x)=a e^x + b e^{-x} + c e^{ix} + d e^{-ix}$ for
 some constants $a,b,c,d$.
\end{enumerate}
The remarkable thing is that all these problems can be addressed using
essentially the same ideas: the theory of modules over a Euclidean
domain. 

\section{Rings and Fields}
\label{sec-rings}

A \emph{commutative ring} is a set $R$ of things that can be added,
negated and multiplied in a sensible way to get new elements of $R$.
More precisely, we require that the following axioms be satisfied:
\begin{itemize}
 \item[(a)] If $a,b\in R$ then $a+b\in R$.
  [closure under addition]
 \item[(b)] There is an element $0\in R$ such that $a+0=a$ for all
  $a\in R$.
  [additive identity]
 \item[(c)] For each element $a\in R$ there is an element $-a\in R$
  such that $a+(-a)=0$. 
  [additive inverses]
 \item[(d)] $a+(b+c)=(a+b)+c$ for all $a,b,c\in R$.
  [associativity of addition]
 \item[(e)] $a+b=b+a$ for all $a,b\in R$.
  [commutativity of addition]
 \item[(f)] If $a,b\in R$ then $ab\in R$.
  [closure under multiplication]
 \item[(g)] There is an element $1\in R$ such that $1a=a$ for all
  $a\in R$.
  [multiplicative identity]
 \item[(h)] $a(bc)=(ab)c$ for all $a,b,c\in R$.
  [associativity of multiplication]
 \item[(i)] $ab=ba$ for all $a,b\in R$.
  [commutativity of multiplication]
 \item[(j)] $a(b+c)=ab+ac$ for all $a,b,c\in R$.
  [distributivity]
\end{itemize}
Strictly speaking, we should say that a ring is a set $R$
\emph{together with a definition of addition, negation and
  multiplication} such that the axioms hold.  In all the examples that
we will consider, there is a unique obvious way to define these
operations.

We will not consider noncommutative rings in this course, so we will
just use the word ``ring'' to mean ``commutative ring''.  We will use
without comment various standard consequences of the axioms, such as
the facts that $-(-a)=a$, $0.a=0$ and $(-1).a=-a$.

\begin{definition}\lbl{defn-domain}
 A ring $R$ is an \emph{integral domain} if $1\neq 0$, and whenever
 $a,b\neq 0$ we also have $ab\neq 0$.
\end{definition}
\begin{definition}
 An element $a$ in a ring $R$ is \emph{invertible} if there is an
 element $b\in R$ such that $ab=1$.  If so, then this element $b$ is
 unique and we call it $a^{-1}$.  A \emph{field} is a ring $K$ such
 that $1\neq 0$ and every nonzero element is invertible.  Every field
 is an integral domain.
\end{definition}

\begin{example}\lbl{eg-Z-ring}
 The set $\Z$ of integers is a ring.  If we add, subtract or multiply
 any two integers, we get another integer, so axioms~(a), (c) and~(f)
 hold.  The numbers $0$ and $1$ are integers so axioms~(b) and~(g)
 hold.  The remaining axioms are familiar properties of addition,
 subtraction and multiplication.  It is also clear that $\Z$ is an
 integral domain.
\end{example}
\begin{example}\lbl{eg-N-not-ring}
 The set $\N$ of natural numbers is not a ring, because $0\not\in\N$,
 so there is no additive identity, in other words axiom~(b) does not
 hold.  Moreover, if $n\in\N$ then $-n\not\in\N$, so $\N$ does not
 have additive inverses.
\end{example}
\begin{example}\lbl{eg-even-not-ring}
 The set $2\Z$ of even integers is not a ring, because it does not
 contain the multiplicative identity element $1$.
\end{example}
\begin{example}\lbl{eg-QRC}
 The set $\Q$ of rational numbers, the set $\R$ of real numbers, and
 the set $\C$ of complex numbers, are all fields.
\end{example}
\begin{example}\lbl{eg-irrational}
 The set $X=\R\sm\Q$ of irrational numbers is not a ring.  Indeed, we
 have $\pi\in X$ and $-\pi\in X$ but $0=\pi+(-\pi)\not\in X$, so $X$
 is not closed under addition.  Moreover, $\sqrt{2}\in X$ but
 $\sqrt{2}.\sqrt{2}=2\not\in X$, so $X$ is not closed under
 multiplication.  Even more obviously, $X$ contains neither $0$ nor
 $1$, so it does not have an additive identity or a multiplicative
 identity.
\end{example}
\begin{example}\lbl{eg-Zn}
 For any natural number $n$, the set
 $\Z_n=\{\ov{0},\ov{1},\ldots,\ov{n-1}\}$ of integers modulo $n$ is a
 ring.  For example, we have
 \begin{align*}
  \Z_4 &= \{\ov{0},\ov{1},\ov{2},\ov{3}\}  \\
  \ov{2} + \ov{3} &= \ov{5} = \ov{1} \\
  \ov{1} - \ov{2} &= \ov{-1} = \ov{3} \\
  \ov{2} \,.\, \ov{3} &= \ov{6} = \ov{2}.  
 \end{align*}
 Note that $\Z_4$ is not an integral domain, because $\ov{2}\neq 0$
 but $\ov{2}.\ov{2}=0$.  In general, it can be shown that when $n$ is
 prime, $\Z_n$ is a field (and thus an integral domain), but when $n$
 is not prime, $\Z_n$ is neither a field nor an integral domain.
\end{example}
\begin{example}\lbl{eg-poly}
 We write $\Z[x]$ for the set of all polynomials with integer
 coefficients.  For example, $7x^3-22x+3$ and $x^{1001}$ are elements
 of $\Z[x]$ but $(x+1)/(x-1)$ and $x^2-1/2$ and $x-x^{-1}$ are not.
 The general form of an element of $\Z[x]$ is
 $f(x)=a_0+a_1x+\ldots+a_nx^n$ for some integer $n\geq 0$ and integers
 $a_0,\ldots,a_n$.  Integers are polynomials of degree zero, so
 $\Z\sse\Z[x]$.  The usual operations of addition, multiplication and
 negation of polynomials make $\Z[x]$ into a ring.

 More generally, given any ring $R$ we can consider the ring $R[x]$ of
 polynomials with coefficients in $R$.  For example, we can consider
 $f(x)=\ov{2}x^2+\ov{3}\in\Z_6[x]$ and $g(x)=\ov{3}x+\ov{2}\in\Z_6[x]$
 and we find that 
 \begin{align*}
  f(x)g(x) &= \ov{6} x^3 + \ov{4} x^2 + \ov{9} x + \ov{6} \\
           &= \ov{4} x^2 + \ov{3} x.\\
 \end{align*}
 
 This gives us rings $\Z[x]\sse\Q[x]\sse\R[x]\sse\C[x]$.  We can also
 use more than one variable; for example, we have a ring $\Q[x,y,z]$
 containing elements like $(x^2+y^2+z^2)/4$ or $1+xyz$ (but not $x/y$
 or $\sqrt{2}x$ or $e^{x+y}$).
\end{example}
\begin{example}\lbl{eg-diffop}
 Let $D$ denote the operation of differentiation with respect to $t$,
 so $D(t^3)=3t^2$, $D(\sin(t))=\cos(t)$, $D^3(f(t))=f'''(t)$ and so
 on.  Using this we can build more complicated operators like
 $(D^2+2D+3)(f(t))=f''(t)+2f'(t)+3f(t)$ and so on.  By a
 \emph{differential operator} we mean an operation of the form
 $a_0+a_1D+\ldots+a_nD^n$ for some $n\geq 0$ and some list of
 coefficients $a_i\in\R$.  The set of differential operators is the
 polynomial ring $\R[D]$; its is essentially the same as $\R[x]$
 except for the notation, and the fact that the elements are
 interpreted as operators rather than functions.
\end{example}
\begin{remark}\lbl{rem-diffop}
 Here we are only considering linear ordinary differential operators
 with constant coefficients.  For more serious work on differential
 equations one needs to work with more general operators, which
 generally form noncommutative rings.
\end{remark}
\begin{example}\lbl{eg-Zplocal}
 Let $p$ be a prime number.  Let $\Zpl$ be the set of rational numbers
 $x$ that can be written in the form $a/b$, where $a$ and $b$ are
 integers and $b$ is not divisible by $p$.  For example, $123/101$ and
 $-4/12=(-1)/3$ and $8=8/1$ are elements of $\Z_{(2)}$, but $5/6$ and
 $1/8$ are not.  As any integer $n$ can be written as $n/1$ and $1$ is
 not divisible by $p$, we see that $\Z\sse\Zpl$.  Now suppose that
 $x,y\in\Zpl$, so we can write $x=a/b$ and $y=c/d$ for some integers
 $a$, $b$, $c$ and $d$, where $b$ and $d$ are not divisible by $p$.
 As $p$ is prime this means that $bd$ is also not divisible by $p$.
 We have
 \begin{align*}
  x+y &= (ad+bc)/(bd) \\
  xy  &= (ac)/(bd)    \\
  -x  &= -a/b.
 \end{align*}
 As $ad+bc$, $ac$, $-a$, $b$ and $bd$ are integers, and $bd$ and $b$
 are not divisible by $p$, this means that $x+y$, $xy$ and $-x$ lie in
 $\Zpl$.  Thus $\Zpl$ is a subring of $\Q$, called the ring of
 integers localized at $p$.  (There is a long story coming from
 algebraic geometry that explains why the word ``localized'' is
 appropriate.)
\end{example}
\begin{example}\lbl{eg-gaussian}
 We write $\Z[i]$ for the set of complex numbers of the form $a+bi$,
 where $a$ and $b$ are integers (possibly zero).  Thus $7$, $6-4i$ and
 $12i$ are elements of $\Z[i]$, but $2/3$ and $1-i/5$ are not.  Note
 that 
 \begin{align*}
  (a+bi)+(c+di) &= (a+c) + (b+d)i \\
  (a+bi)(c+di) &= (ac-bd) + (ad+bc)i \\
  -(a+bi) &= (-a) + (-b)i.
 \end{align*}
 It follows easily that $\Z[i]$ is closed under addition,
 multiplication and negation, so it is a subring of $\C$.  The
 elements of $\Z[i]$ are called \emph{Gaussian integers}.  
\end{example}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{isring}
\begin{exercise}\exlabel{ex-which-ring}
 Which of the following are commutative rings?
 \begin{itemize}
  \item $R_0$ is the set of polynomials $f(x)\in\R[x]$ such that
   $f(-x)=f(x)$. 
  \item $R_1$ is the set of polynomials $f(x)\in\R[x]$ such that
   $f(-x)=-f(x)$.
  \item $R_2$ is the set of $2\tm 2$ matrices over $\R$, with the
   usual definition of matrix multiplication.
  \item $R_3$ is the set of $2\tm 2$ matrices over $\R$, with
   multiplication given by the definition
   \[ \bsm a & b \\ c & d \esm \bsm a' & b' \\ c' & d' \esm = 
       \bsm aa' & bb' \\ cc' & dd' \esm.
   \]
  \item $R_4$ is the set of vectors in $\R^3$, with multiplication
   given by the cross product:
   \[ \bsm x \\ y \\ z \esm \tm \bsm x' \\ y' \\ z' \esm =
       \bsm y z' - y' z \\ z x' - z' x \\ x y' - x' y \esm.
   \] 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item $R_0$ is a ring.  The main point is to observe that $R_0$ is
   closed under addition and multiplication, because if $f(-x)=f(x)$
   and $g(-x)=g(x)$ then
   \begin{align*}
    (f+g)(-x) &= f(-x) + g(-x) = f(x) + g(x) = (f+g)(x) \\
    fg(-x) &= f(-x) g(-x) = f(x) g(x) = fg(x).
   \end{align*}
  \item $R_1$ is not a ring, because it is not closed under
   multiplication: if $f$ and $g$ lie in $R_1$ then
   \[ fg(-x)=f(-x)g(-x)=(-f(x))(-g(x))=+fg(x)\neq -fg(x), \]
   so $fg\not\in R_1$ (except in trivial cases where $fg=0$).
  \item $R_2$ is not a commutative ring, because matrix multiplication
   is not commutative in general.  For example, if we take
   $a=\bsm 1 & 0 \\ 0 & 0 \esm$ and $b=\bsm 0 & 1 \\ 0 & 0 \esm$ then
   $ab=b$ and $ba=0$ so $ab\neq ba$.  All the other axioms are
   satisfied, however.
  \item $R_3$ is a ring.  The additive identity is the zero matrix,
   and the multiplicative identity is the matrix $\bsm 1&1\\1&1\esm$.
  \item $R_4$ is not a ring.  Firstly, it is not commutative, because
   $b\tm a=-a\tm b$.  It is not even associative, because
   \begin{align*}
    a\tm (b\tm c) &= (a.c) b - (a.b) c \\
    (a\tm b)\tm c &= (a.c) b - (b.c) a.
   \end{align*}
   There is also no multiplicative identity: if there were, then we
   would have $1\tm 1=1$, but $a\tm a$ is always zero for any vector
   $a$.  ($R_3$ is in fact an example of a \emph{Lie algebra}; these
   are rather different from rings, but also very important.)
 \end{itemize}
\end{solution}

%\ip{egelt}
\begin{exercise}\exlabel{ex-typical-elts}
 Choose two typical elements $a$ and $b$ of the ring $\Z_{(5)}$.  Find
 $a+b$ and $ab$ and check that they lie in $\Z_{(5)}$.  Repeat this
 for the rings $\Z[i]$, $\Q[x,y]$ and $\Z_{12}$.
\end{exercise}
\begin{solution}
 \begin{itemize}
 \item[(a)] In $\Z_{(5)}$ we could take $a=3/4$ and $b=6/7$; these
  both lie in $\Z_{(5)}$ because $4$ and $7$ are not divisible by $5$.
  We have $a+b=45/28$ and $ab=18/28=9/14$.  These both lie in
  $\Z_{(5)}$ because $28$ and $14$ are not divisible by $5$.
 \item[(b)] In $\Z[i]$ we could take $a=2+3i$ and $b=4-5i$.  We then
  have $a+b=6-2i$ and $ab=23+2i$; both of these clearly also lie in
  $\Z[i]$.
 \item[(c)] In $\Q[x,y]$ we could take $a=(x+y)/2$ and $b=(x-y)/2$.
  Then $a+b=x$ and $ab=(x^2-y^2)/4$, so $a+b$ and $ab$ are again
  elements of $\Q[x,y]$.
 \item[(d)] In $\Z_{12}$ we could take $a=\ov{3}$ and $b=\ov{4}$, so
  $a+b=\ov{7}=\ov{-5}$ and $ab=\ov{12}=\ov{0}$. 
 \end{itemize}
\end{solution}

%\ip{nonlocal}
\begin{exercise}\exlabel{ex-six-local}
 Let $R$ be the set of rational numbers that can be written in the
 form $a/b$, where $b$ is not divisible by $6$.  (We would call this
 $\Z_{(6)}$ if $6$ were prime, which of course it isn't).  Prove that
 $R$ is \emph{not} a subring of $\Q$.
\end{exercise}
\begin{solution}
 Put $a=1/2$ and $b=-1/3$.  Then $a,b\in R$ but $a+b=1/6\not\in R$ and
 $ab=-1/6\not\in R$, so $R$ is not closed under addition or
 multiplication. 
\end{solution}

%\ip{fielddom}
\begin{exercise}\exlabel{ex-field-domain}
 Let $K$ be a field; prove that $K$ is an integral domain.
\end{exercise}
\begin{solution}
 Let $a$ and $b$ be nonzero elements of $K$; we must prove that
 $ab\neq 0$.  As $K$ is a field, we know that $a$ and $b$ are
 invertible, so we can find elements $c,d\in K$ with $ac=1$ and
 $bd=1$.  It follows that $abcd=1$.  If $ab$ were zero we would also
 have $abcd=0$, so $1$ would be equal to $0$, contradicting the
 definition of a field.  We must thus have $ab\neq 0$ as required.
\end{solution}

%\ip{bool}
\begin{exercise}\exlabel{ex-PX-ring}
 Let $X$ be a set.  Let $R$ be the set of all subsets of $X$, and
 define addition and multiplication of subsets as follows.
 \begin{align*}
  A+B &= (A\cup B)\sm(A\cap B) \\
      &= \{ x\in X \st x\in A \text{ or } x\in B 
            \text{ but not both. } \} \\
  AB  &= A\cap B.
 \end{align*}
 For any $A\in R$, we define a function $\chi_A\:X\xra{}\Z_2$ by
 \[ \chi_A(x) =
      \begin{cases}
        \ov{1} & \text{ if } x\in A \\
        \ov{0} & \text{ if } x\not\in A
      \end{cases}
 \]
 \begin{itemize}
  \item[(a)] Check that $A+\emptyset=A$ and $A+A=\emptyset$.
  \item[(b)] Show that $\chi_{A+B}(x)=\chi_A(x)+\chi_B(x)$ and
   $\chi_{AB}(x)=\chi_A(x)\chi_B(x)$.
  \item[(c)] Show that if $\chi_A=\chi_B$ then $A=B$.
  \item[(d)] Prove that the definitions above make $R$ into a
   commutative ring.  (You may wish to use~(b) and~(c) to help check
   some of the axioms.)  
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We have $A\cup\emptyset=A$ and $A\cap\emptyset=\emptyset$
   so $A+\emptyset=A\sm\emptyset=A$.  Similarly, we have
   $A\cup A=A=A\cap A$, so $A+A=A\sm A=\emptyset$.
  \item[(b)] For the equation $\chi_{A+B}(x)=\chi_A(x)+\chi_B(x)$,
   there are four cases to consider.
   \begin{itemize}
   \item[(i)] $x$ lies in both $A$ and $B$, so $x$ does not lie in
    $A+B$, so $\chi_{A+B}(x)=\ov{0}$.  Here we have
    $\chi_A(x)+\chi_B(x)=\ov{1}+\ov{1}=\ov{2}=\ov{0}$ (because we are
    working in $\Z_2$), so $\chi_{A+B}(x)=\chi_A(x)+\chi_B(x)$ as
    required.
   \item[(ii)] $x$ lies in $A$ but not in $B$, so $x\in A+B$, so
    $\chi_{A+B}(x)=\ov{1}$.    Here we have
    $\chi_A(x)+\chi_B(x)=\ov{1}+\ov{0}=\ov{1}=\chi_{A+B}(x)$ as
    required. 
   \item[(iii)] $x$ lies in $B$ but not in $A$; this works the same
    way as in~(ii).
   \item[(iv)] $x$ lies in neither $A$ nor $B$.  Here it is clear that
    $\chi_{A+B}(x)=\ov{0}=\ov{0}+\ov{0}=\chi_A(x)+\chi_B(x)$.
   \end{itemize}
   The argument is similar but easier for the equation
   $\chi_{AB}(x)=\chi_A(x)\chi_B(x)$.  
  \item[(c)] It is clear that $\{x\in X\st \chi_A(x)=\ov{1}\}=A$.  If
   $\chi_A=\chi_B$ then
   $\{x\st\chi_A(x)=\ov{1}\}=\{x\st\chi_B(x)=\ov{1}\}$, so $A=B$.
  \item[(d)]
   It is clear that the above rules do indeed define subsets of $X$, so
   $R$ is closed under addition and multiplication.  It is easy to see
   that $AB=A\cap B=B\cap A=BA$ and
   $(AB)C=(A\cap B)\cap C=A\cap(B\cap C)=A(BC)$, so multiplication is
   commutative and associative.  Moreover, for $A\sse X$ we have
   $X\cap A=A$, so $X$ is a multiplicative identity element.

   It is also clear that $A+B=B+A$, so addition and is commutative.
   Part~(a) says that $\emptyset$ is an additive identity, and $A$ is
   an additive inverse for itself.  We next show that addition is
   associative.  By part~(b), we have
   \[ \chi_{A+(B+C)}(x) = \chi_A(x)+\chi_{B+C}(x)
       = \chi_A(x) + \chi_B(x) + \chi_C(x) =
       \chi_{A+B}(x) + \chi_C(x) = \chi_{(A+B)+C}(x).
   \]
   It follows using~(c) that $A+(B+C)=(A+B)+C$, as required.  

   All that is left is to check distributivity, which can be done by
   the same method.  We have
   \begin{align*}
    \chi_{A(B+C)}(x) &= \chi_A(x) \chi_{B+C}(x) \\
                     &= \chi_A(x) (\chi_B(x) + \chi_C(x)) \\
                     &= \chi_A(x)\chi_B(x) + \chi_A(x)\chi_C(x) \\
                     &= \chi_{AB}(x) + \chi_{AC}(x) \\
                     &= \chi_{AB+AC}(x),
   \end{align*}
   so $A(B+C)=AB+AC$ as required.
 \end{itemize}
\end{solution}

\section{Modules}
\label{sec-modules}

\begin{definition}\lbl{defn-module}
 Let $R$ be a ring.  A \emph{module} over $R$ is a set $M$ of things
 with a definition of $m+n$ for all $m,n\in M$ and a definition of
 $am$ for all $a\in R$ and $m\in M$ such that the following axioms are
 satisfied:
 \begin{itemize}
 \item[(a)] If $m,n\in M$ then $m+n\in M$. [closure under addition]
 \item[(b)] There is an element $0\in M$ such that $m+0=m$ for all
  $m\in M$. [additive identity]
 \item[(c)] For each $m\in M$ there is an element $-m\in M$ such that
  $m+(-m)=0$. [additive inverses]
 \item[(d)] $m+(n+p)=(m+n)+p$ for all $m,n,p\in M$.
  [associativity of addition]
 \item[(e)] $m+n=n+m$ for all $m,n\in M$.
  [commutativity of addition]
 \item[(f)] If $a\in R$ and $m\in M$ then $am\in M$.
  [closure of $M$ under multiplication by $R$]
 \item[(g)] $1.m=m$ for all $m\in M$.
 \item[(h)] $(ab)m=a(bm)$  for all $a,b\in R$ and $m\in M$.
  [associativity of multiplication]
 \item[(i)] $(a+b)m=am+bm$ for all $a,b\in R$ and $m\in M$.
  [left distributivity of multiplication]
 \item[(j)] $a(m+n)=am+an$ for all $a\in R$ and $m,n\in M$.
  [right distributivity of multiplication]
 \end{itemize}
\end{definition}
\begin{remark}\lbl{rem-additive-group}
 Note that axioms~(a) to~(e) say that $M$ is in particular an Abelian
 group under addition.
\end{remark}
\begin{example}\lbl{eg-free-module}
 Let $R$ be any ring, and let $d$ be a natural number.  We then write
 $R^d$ for the set of $d$-tuples $(x_1,\ldots,x_d)$ with
 $x_1,\ldots,x_d\in R$.  We make $R^d$ into a module over $R$ by
 defining 
 \begin{align*}
  (x_1,\ldots,x_d) + (y_1,\ldots,y_d) &= 
    (x_1+y_1,\ldots,x_d+y_d) \\
  a (x_1,\ldots,x_d) &= (ax_1,\ldots,ax_d).
 \end{align*}
 It is straightforward to check that the axioms are satisfied.  In
 particular, the case $d=1$ says that we can regard $R$ as a module
 over itself.  
\end{example}

If $R$ is a field, then an $R$-module is just a vector space over
$R$.  Modules are just the natural generalization of vector spaces
defined over arbitrary rings rather than just fields.  It is a basic
fact of linear algebra that if $K$ is a field and $V$ is a vector
space over $K$ with a finite spanning set, then $V$ is isomorphic to
$K^d$ for some integer $d$, called the \emph{dimension} of $V$.  The
situation for modules over non-fields is more complicated; a module is
usually not isomorphic to $R^d$ for any $d$.  The next simplest case
after fields is when $R$ is a Euclidean domain, and most of the course
will be devoted to the study of modules over such rings.  

\begin{proposition}\lbl{prop-Z-module}
 A $\Z$-module is just an Abelian group.  More precisely, if $M$ is an
 Abelian group (with the group operation written as addition) then
 there is a unique way to define $am$ for all $a\in\Z$ and $m\in M$
 such that axioms~(f) to~(j) hold, making $M$ a $\Z$-module.
\end{proposition}
\begin{proof}[Sketch proof]
 Rather than giving a complete proof of this, we will give an outline
 of the argument with examples.

 The basic idea is very simple.  We just define
 \begin{align*}
  3m  &= m + m + m\\
  -5m &= - (m+m+m+m+m) = (-m) + (-m) + (-m) + (-m) + (-m) 
 \end{align*}
 and so on.  This defines multiplication (of integers by group
 elements) in terms of addition and negation of group elements.  We
 actually have no choice about these definitions if we want the axioms
 to be satisfied: as $3=1+1+1$, axiom~(i) says we we must have
 $3m=(1+1+1)m=1m+1m+1m$, and axiom~(g) says that $1m=m$ so we must have
 $3m=m+m+m$, and so on.

 We now need to check that axioms~(f) to~(j) are satisfied.
 Axioms~(f) and~(g) are immediate.  The remaining axioms are easy to
 check when $a$ and $b$ are nonnegative: for example
 \begin{align*}
  2(3m) &= 2(m+m+m) \\
        &= (m+m+m)+(m+m+m) \\
        &= m+m+m+m+m+m \\
        &= (2\tm 3)m \\
  2m+3m &= (m+m)+(m+m+m) \\
        &= m+m+m+m+m \\
        &= (2+3)m \\
  3(m+n)&= (m+n) + (m+n) + (m+n) \\
        &= (m+m+m) + (n+n+n) \\
        &= 3m + 3n.
 \end{align*}
 If we allow $a$ or $b$ to be negative then there are quite a few more
 cases to check depending on the various possible combinations of
 signs, but they are all quite straightforward.  For example
 \begin{align*}
  5m + (-3)m &= (m+m+m+m+m) + ((-m)+(-m)+(-m)) \\
             &= m + m + (m+(-m)) + (m+(-m)) + (m+(-m)) \\
             &= m + m \\
             &= (5 + (-3)) m.
 \end{align*}
\end{proof}

We will next give an example involving differential operators.
To avoid annoying technicalities, it is best to restrict attention to
functions that can be differentiated as many times as we like.  We
therefore introduce the following definition.
\begin{definition}\lbl{defn-smooth}
 A function $f\:\R\xra{}\R$ is \emph{smooth} if the $n$'th derivatives
 $f^{(n)}(t)$ are defined and continuous everywhere on $\R$ for all
 $n\geq 0$.  In particular, the function $f=f^{(0)}$ itself must be
 defined and continuous everywhere.  

 For example, $\sin(t)$, $\cos(t)$, $e^t$, $t^2$ and so on are smooth.
 However, the functions $1/t$ and $\log(t)$ are not defined at $t=0$,
 so they are not smooth.  The function $f(t)=|t|$ is defined and
 continuous everywhere and $f'(t)=-1$ for $t<0$ and $f'(t)=1$ for
 $t>0$ but $f'(0)$ is undefined so $f$ is not smooth.  Similarly, if
 $g(t)=t^{1/3}$ then $g'(t)=t^{-2/3}/3$, which is undefined at $t=0$
 so $g$ is not smooth.

 We write $\CRR$ for the set of all smooth functions from $\R$
 to $\R$.  If $f$ and $g$ are smooth and $a$ is constant then one can
 show that $f+g$ and $af$ are smooth.  It follows that $\CRR$
 is a vector space over $\R$.  Similarly, the set $\CRC$ of
 smooth functions from $\R$ to $\C$ is a vector space over $\C$.
\end{definition}
\begin{example}\lbl{eg-smooth-module}
 $\CRR$ is a module over the ring $\R[D]$ of differential operators.
 For an operator $p(D)=a_0+a_1D+\ldots+a_nD^n$ and a smooth function
 $f(t)$, the product $p(D)f$ is defined by the usual rule
 \[ p(D)f = a_0f + a_1 f' + a_2f'' + \ldots + a_nf^{(n)}. \]
 One can check the axioms directly, or use a more abstract approach
 discussed in the next section.  Similarly, $\CRC$ is a module over
 $\C[D]$. 
\end{example}

\begin{example}\lbl{eg-diffop-calc}
 \begin{align*}
  (1+D+D^2).(1+t+t^2) &= (1+t+t^2) + (1+t+t^2)' + (1+t+t^2)'' \\
                      &= (1+t+t^2) + (1+2t) + (2) \\
                      &= 4 + 3t + t^2.
 \end{align*}
\end{example}
\begin{example}\lbl{eg-t-sint}
 Consider the function $f(t)=t\sin(t)$; I claim that $(D^2+1)^2f=0$.
 Indeed, we have
 \begin{align*}
  f'(t)  &= \sin(t) + t\cos(t) \\
  f''(t) &= 2\cos(t) -t\sin(t) \\
  ((D^2+1)f)(t) &= f(t) + f''(t) = 2\cos(t)   
 \end{align*}
 We also have $\cos'(t)=-\sin(t)$ and so $\cos''(t)=-\cos(t)$ so
 $(D^2+1)\cos=0$.  It follows that $(D^2+1)^2f=(D^2+1)(2\cos)=0$.
\end{example}
\begin{example}\lbl{eg-two-rates}
 Consider a function of the form $f(t)=e^{\lm t}+e^{\mu t}$.  I claim
 that 
 \[ (p(D)f)(t) = p(\lm) e^{\lm t} + p(\mu) e^{\mu t}. \]
 Indeed, we have
 \begin{align*}
  f'(t)  &= \lm e^{\lm t}+\mu e^{\mu t} \\
  f''(t) &= \lm^2 e^{\lm t} + \mu^2 e^{\mu t}
 \end{align*}
 and more generally $f^{(k)}(t)=\lm^k e^{\lm t}+\mu^k e^{\mu t}$ (as
 one can easily check by induction).  If $p(D)=a_0+a_1D+\ldots+a_mD^m$
 then we have
 \begin{align*}
  (p(D)f)(t) &= a_0(e^{\lm t} + e^{\mu t}) +  
                a_1(\lm e^{\lm t} + \mu e^{\mu t}) + \ldots + 
                a_m(\lm^m e^{\lm t} +\mu^m e^{\mu t}) \\
             &= (a_0 + a_1\lm + \ldots + a_m\lm^m) e^{\lm t} + 
                (a_0 + a_1\mu + \ldots + a_m\mu^m) e^{\mu t} \\
             &= p(\lm) e^{\lm t} + p(\mu) e^{\mu t}.
 \end{align*}
\end{example}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{Z10}
\begin{exercise}\exlabel{ex-Z-ten}
 List all the elements of the Abelian group $\Z_2\op\Z_5$.  Find an
 element that has order $10$.
\end{exercise}
\begin{solution}
 The elements are
 $(\ov{0},\ov{0}),(\ov{0},\ov{1}),(\ov{0},\ov{2}),(\ov{0},\ov{3}),
 (\ov{0},\ov{4}),(\ov{1},\ov{0}),(\ov{1},\ov{1}),(\ov{1},\ov{2}),
 (\ov{1},\ov{3})$ and $(\ov{1},\ov{4})$.  I claim that the element
 $x:=(\ov{1},\ov{1})$ has order $10$.  Indeed, we have
 $nx=(\ov{n},\ov{n})$.  The first $\ov{n}$ is in $\Z_2$, so it is zero
 iff $n$ is divisible by $2$.  The second $\ov{n}$ is in $\Z_5$, so it
 is zero iff $n$ is divisible by $5$.  Thus $nx=(\ov{0},\ov{0})$ iff
 $n$ is divisible by both $2$ and $5$, or equivalently iff $n$ is
 divisible by $10$.  This means that $x$ has order $10$ as claimed.
\end{solution}

%\ip{diffop}
\begin{exercise}\exlabel{ex-diffop-calc}
 \begin{itemize}
  \item[(a)] Calculate $(1+D+D^2/2+D^3/6).t^3$.  What do you notice?
   Can you guess a generalisation?
  \item[(b)] Put $f(t)=e^{-t}\sin(t)$ so $f\in\CRR$.  Calculate
   $(D+1)^2f$. 
  \item[(c)] Put $g_k(t)=t^ke^t$, so $g_k\in\CRR$.  Calculate
   $(D-1)g_k$ and thus $(D-1)^kg_k$.  (You may wish to try $k=3$
   first.)
  \item[(d)] Put $f(t)=te^t$.  Show that $(D^kf)(t)=(k+t)e^t$ for all
   $k\geq 0$ and thus that $(p(D)f)(t)=(p'(1)+p(1)t)e^t$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
 \item[(a)] We have $D.t^3=3t^2$ so $D^2.t^3=3D.t^2=6t$ so
  $D^3.t^3=6D.t=6$.  This means that $(D^2/2).t^3=3t$ and
  $(D^3/6).t^3=1$ so $(1+D+D^2/2+D^3/6).t^3=t^3+3t^2+3t+1$.  We notice
  that this is just $(t+1)^3$.  The generalisation is that 
  \[ (\sum_{k=0}^m D^k/k!).t^m = (t+1)^m. \]
  More generally, if $f(t)$ is any polynomial of degree less than or
  equal to $m$, it can be shown that 
  \[ (\sum_{k=0}^m D^k/k!).f(t) = f(t+1). \]
  This is essentially Taylor's theorem.
 \item[(b)] Put 
  \[ g(t)=((D+1)f)(t)=f'(t)+f(t)=
       (-e^{-t}\sin(t)+e^{-t}\cos(t))+e^{-t}\sin(t)=e^{-t}\cos(t).
  \]
  Then $((D+1)^2f)(t)=((D+1)g)(t)=g'(t)+g(t)=-e^{-t}\sin(t)$, by a
  similar calculation.  In other words $(D+1)^2f=-f$.
 \item[(c)] We have
  $((D-1)g_k)(t)=g'_k(t)-g_k(t)=(kt^{k-1}e^t+t^ke^t)-t^ke^t=kt^{k-1}e^t$,
  or in other words $(D-1)g_k=kg_{k-1}$.  It follows that
  \begin{align*}
   (D-1)^2g_k &= k(D-1)g_{k-1}=k(k-1)g_{k-2} \\
   (D-1)^3g_k &= k(k-1)(D-1)g_{k-2}=k(k-1)(k-2)g_{k-3}
  \end{align*}
  and so on.  We eventually find that $(D-1)^kg_k=k!g_0$, so
  $((D-1)^kg_k)(t)=k!e^t$.  
 \item[(d)] We certainly have $(D^0f)(t)=f(t)=(0+t)e^t$.  Assuming
  that $(D^kf)(t)=(k+t)e^t$ for some particular value of $k$, we have
  \[ (D^{k+1}f)(t) = D((k+t)e^t)= (k+t) D(e^t) + e^t D(k+t) = 
      (k+t) e^t + e^t = ((k+1) + t) e^t.
  \]
  It follows by induction that $(D^kf)(t)=(k+t)e^t$ for all $k$.
  Thus, for an operator $p(D)=\sum_k a_kD^k$, we have
  \[ (p(D)f)(t) = \sum_k a_k(k+t) e^t =
       ((\sum_k ka_k) + (\sum_k a_k)t) e^t.
  \]
  We also have $p(1)=\sum_k a_k.1^k=\sum_ka_k$.  Similarly, we have 
  $p'(D)=\sum_k ka_k D^{k-1}$, so $p'(1)=\sum_k ka_k$.  Putting these
  into our earlier formula gives
  \[ (p(D)f)(t) = (p'(1) + p(1)t) e^t, \]
  as claimed.
 \end{itemize}
\end{solution}

%\ip{hermite}
\begin{exercise}\exlabel{ex-gaussian-poly}
 Define $v(t)=e^{t^2/2}$, so $u\in\CRR$.  Let $V$ be the set of
 functions of the form $f(t)v(t)$, where $f$ is a polynomial.  For
 example, the function $(1+t+t^2)e^{t^2/2}$ is an element of $V$.
 \begin{itemize}
  \item[(a)] Prove that $V$ is an $\R[D]$-submodule of $\CRR$.
  \item[(b)] Calculate $D^kv$ for $0\leq k\leq 3$.
  \item[(c)] Show that for all $k\geq 0$ there is a polynomial
   $p_k(t)$ of the form $t^k+\text{ lower terms }$ such that
   $D^kv=p_k.v$. 
  \item[(d)] Show that if $q(D)$ is a nonzero element of $\R[D]$ then
   $q(D)v\neq 0$ (look at leading terms).  
  \item[(e)] Suppose that $f(t)$ is a polynomial of degree $k$, say
   $f(t)=at^k+\text{ lower terms }$.  Prove by induction on $k$ that
   $fv=q(D)v$ for some element $q(D)\in\R[D]$.
  \item[(f)] Deduce that $V\simeq\R[D]$ as an $\R[D]$-module. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We just need to check that $V$ is closed under
   differentiation.  Note that $v'(t)=te^{t^2/2}=tv(t)$, so 
   \[ \frac{d}{dt}f(t)v(t) = f(t)v'(t) + f'(t)v(t) = 
        (tf(t)+f'(t)) v(t).
   \]
   If $f(t)$ is a polynomial, then clearly $tf(t)+f'(t)$ is also a
   polynomial, so the function $(tf(t)+f'(t))v(t)$ lies in $V$ as
   required.
  \item[(b)] If we differentiate repeatedly using the above rule we
   find that 
   \begin{align*}
    (D^0 v)(t) &=   v(t) \\
    (D^1 v)(t) &= t v(t) \\
    (D^2 v)(t) &= (t^2+1) v(t) \\
    (D^3 v)(t) &= (t^3+3t) v(t).
   \end{align*}
  \item[(c)] As $V$ is an $\R[D]$-module, we must have $D^kv\in V$, so
   $D^kv=p_kv$ for some polynomial $p_k$.  (From part~(b) we see that
   $p_0(t)=1$, $p_1(t)=t$, $p_2(t)=t^2+1$ and $p_3(t)=t^3+3t$.)  Using
   part~(a) we see that $p_{k+1}(t)=tp_k(t)+p'_k(t)$.  The
   claim is that $p_k(t)=t^k+\text{ lower terms }$.  If this is true
   for some value of $k$, then $tp_k(t)=t^{k+1}+\text{ lower terms }$
   and $p'_k(t)=kt^{k-1}+\text{ lower terms }$, so
   $p_{k+1}(t)=t^{k+1}+\text{ lower terms }$, so the claim holds for
   the next value of $k$.  Moreover, the claim visibly holds for
   $k=0$, so it holds for all $k$ by induction.
  \item[(d)] Let $k$ be the degree of $q$, so
   $q(D)=a_0+a_1D+\ldots+a_kD^k$ for some $a_0,\ldots,a_k\in\R$ with
   $a_k\neq 0$.  Then $q(D)v=(a_0p_0+\ldots+a_kp_k)v$, and using
   part~(c) we see that
   $a_0p_0(t)+\ldots+a_kp_k(t)=a_kt^k+\text{ lower terms }$, so in
   particular it is not zero.
  \item[(e)] First suppose that $f$ has degree $0$, say $f(t)=c$ for
   all $t$, where $c\in\R$.  We can regard $c$ as an element in
   $\R[D]$, and $fv=cv$ as required; this proves the claim for $k=0$.

   Now suppose we have proved the claim for all polynomials of degree
   less than $k$, and that $f$ has degree $k$.  Then
   $f(t)=at^k+\text{ lower terms }$ for some $a\in\R$.  It follows
   that the function $g(t)=f(t)-ap_k(t)$ is a polynomial of degree
   less than $k$, so we have $gv=q(D)v$ for some $q(D)\in\R[D]$.  It
   follows that 
   \[ fv = gv + ap_kv = q(D)v + a D^kv = (q(D)+aD^k)v, \]
   so $fv\in\R[D]v$ as required.  The claim now follows for all
   degrees by induction.
  \item[(f)] We know from part~(e) that $v$ generates $V$ as an
   $\R[D]$-module.  It follows that $V\simeq\R[D]/I$, where
   $I=\{q(D)\in\R[D]\st q(D)v=0\}$.  Part~(d) tells us that $I=0$, so
   $V\simeq\R[D]$.
 \end{itemize}
\end{solution}


\section{Modules over polynomial rings}
\label{sec-poly-modules}

We next consider modules over $K[x]$, where $K$ is a field.  The
upshot here is that the study of modules over $K[x]$ is essentially
the same as the study of square matrices over $K$, or of endomorphisms
of vector spaces over $K$.

We start with some comments about the process of ``substituting a
matrix into a polynomial''.  Let $K$ be a field, and let $A$ be an
$n\tm n$ matrix over $K$.  Using the usual matrix multiplication we
can define $A^2$, $A^3$ and so on; all of these are again $n\tm n$
matrices over $K$.  Thus, given a polynomial
$f(x)=a_0+a_1x+\ldots+a_dx^d\in K[x]$ we can define another $n\tm n$
matrix $f(A)$ by $f(A)=a_0I+a_1A+\ldots+a_dA^d$.
\begin{example}\lbl{eg-fA}
 If $A=\bsm 1&2\\3 &4\esm$ and $f(x)=7+6x+5x^2$ then
 $A^2=\bsm 7&10\\ 15&22\esm$ and so
 \begin{align*}
  f(A) &= 7\bsm 1&0\\0&1 \esm +
          6\bsm 1&2\\3&4 \esm +
          5\bsm 7&10\\15&22 \esm \\
       &= \bsm 7&0\\0&7\esm +
          \bsm 6&12\\18&24\esm +
          \bsm 35&50\\75&110\esm \\
       &= \bsm 48&62\\93&141\esm.
 \end{align*}
\end{example}
\begin{example}\lbl{eg-diag-poly}
 Consider a diagonal matrix $A=\bsm \lm & 0\\0&\mu\esm$.  Then 
 \[ A^2 = \bbm \lm & 0\\0&\mu\ebm\bbm \lm & 0\\0&\mu\ebm
        = \bbm \lm^2 & 0\\0&\mu^2\ebm,
 \]
 and more generally it is not hard to see that 
 \[ A^k = \bbm \lm^k & 0\\0&\mu^k \ebm . \]
 (Exercise: prove this by induction.)  It follows that
 \begin{align*}
  f(A) &= a_0\bbm 1     & 0 \\ 0 & 1     \ebm +
          a_1\bbm \lm   & 0 \\ 0 & \mu   \ebm + \ldots +
          a_d\bbm \lm^d & 0 \\ 0 & \mu^d \ebm \\
       &= \bbm a_0 + a_1\lm + \ldots a_d\lm^d & 0 \\
               0 & a_0 + a_1\mu + \ldots a_d\mu^d \ebm \\
       &= \bbm f(\lm) & 0 \\ 0 & f(\mu) \ebm.
 \end{align*}
 More generally, if $A$ is an $n\tm n$ matrix with entries
 $\lm_1,\ldots,\lm_n$ on the diagonal and zeros elsewhere, then $f(A)$
 has entries $f(\lm_1),\ldots,f(\lm_n)$ on the diagonal and zeros
 elsewhere.  
\end{example}
\begin{example}\lbl{eg-jordan-poly}
 Consider the matrix $A=\bsm 1&1\\0&1\esm$.  It is easy to check that 
 \[ \bbm 1 & 1\\0 & 1\ebm \bbm 1 & k\\0 & 1 \ebm = 
    \bbm 1 & k+1 \\ 0 & 1 \ebm,
 \]
 and thus that $A^k=\bsm 1&k\\0&1\esm$ for all $k$.  It follows that 
 \begin{align*}
  f(A) &= a_0\bbm 1&0\\0&1\ebm + 
          a_1\bbm 1&1\\0&1\ebm + \ldots + 
          a_d\bbm 1&d\\0&1\ebm \\
       &= \bbm a_0+\ldots+a_d & a_1 + 2a_2+\ldots+da_d \\
               0 & a_0+\ldots+a_d \ebm.
 \end{align*}
 Note that $f(1)=a_0+\ldots+a_d$.  Note also that the derivative
 $f'(x)$ is given by $f'(x)=a_1+2a_2x+\ldots+da_dx^{d-1}$, so that
 $f'(1)=a_1+2a_2+\ldots+da_d$.  We can thus rewrite the above result
 as 
 \[ f(A) = \bbm f(1) & f'(1) \\ 0 & f(1) \ebm. \]
\end{example}

We next need to check that some things work out as they ``ought'' to
when we substitute matrices into polynomials.  (Recall that matrix
multiplication is noncommutative, there are nonzero matrices whose
square is zero, and numerous other funny things can happen; so we need
to be on our guard.)
\begin{proposition}\lbl{prop-func-calc}
 Let $A$ be an $n\tm n$ matrix over a field $K$.  Then for any two
 polynomials $f,g\in K[x]$ we have
 \begin{align*}
  (f+g)(A) &= f(A) + g(A) \\
  (fg)(A)  &= f(A) g(A).
 \end{align*}
\end{proposition}
\begin{proof}
 Suppose that $f(x)=\sum_i a_ix^i$ and $g(x)=\sum_jb_jx^j$.  Then
 $(f+g)(x)=\sum_i c_ix^i$ where $c_i=a_i+b_i$, and
 \[ (fg)(x)=(\sum_ia_ix^i)(\sum_jb_jx^j) = 
            \sum_{i,j} a_ib_j x^{i+j} =
            \sum_k d_kx^k, 
 \] 
 where $d_k=\sum_{i=0}^k a_ib_{k-i}$.

 Thus
 \begin{align*}
  (f+g)(A) &= \sum_i c_i A^i \\
           &= \sum_i (a_iA^i + b_iA^i) \\
           &= \sum_i a_iA^i + \sum_i b_iA^i \\
           &= f(A) + g(A).
 \end{align*}
 Similarly
 \begin{align*}
  (fg)(A) &= \sum_k d_kA^k \\
          &= \sum_k\sum_{i=0}^k a_ib_{k-i} A^k \\
          &= \sum_k\sum_{i=0}^k (a_i A^i)(b_{k-i} A^{k-i}) \\
          &= \sum_i\sum_j (a_iA^i)(b_jA^j) \\
          &= \sum_i a_iA^i \sum_j b_jA^j \\
          &= f(A) g(A).
 \end{align*}
\end{proof}

We are now ready to construct some modules over $K[x]$.
\begin{construction}\lbl{cons-MA}
 Let $A$ be an $n\tm n$ matrix over a field $K$; we will use this to
 define a module $M_A$ over $K[x]$.  The elements of $M_A$ are just
 the vectors $v=(v_1,\ldots,v_n)$ of length $n$ over $K$, so $M_A=K^n$
 as a set.  Addition and subtraction of vectors is defined in the
 usual way.  All that is left is to define the product $fv$ for
 $f\in K[x]$ and $v\in K^n$, which we do by the formula $fv=f(A)v$.
 Here $f(A)$ is an $n\tm n$ matrix, so the right hand side is defined
 by the ordinary multiplication of vectors by matrices.

 We need to check the module axioms.  Axioms~(a) to~(e) only involve
 addition and negation so they are clear.  Axiom~(f) is also clear
 because $fv$ is certainly a vector in $K^n$.  If $f(x)$ is constant
 polynomial $1$, then $f(A)$ is the identity matrix, so $fv=Iv=v$ for
 all $v$; this gives axiom~(g).  For axiom~(h) we recall that
 $(fg)(A)=f(A)g(A)$ so
 \begin{align*}
  (fg)v &= (fg)(A)v \\
        &= f(A)g(A)v \\
        &= f(A)(gv) \\
        &= f(gv).
 \end{align*}
 Similarly, axiom~(i) follows from the fact that
 $(f+g)(A)=f(A)+g(A)$.  Finally, axiom~(j) is clear, because
 $B(v+w)=Bv+Bw$ for any matrix $B$ and any vectors $v$ and $w$.
\end{construction}
\begin{remark}\lbl{rem-MA-MB}
 Let $A$ and $B$ be two different $n\tm n$ matrices.  Then $M_A$ and
 $M_B$ have the same elements but the multiplication rules in $M_A$
 and $M_B$ are different, so $M_A$ and $M_B$ are different modules.
\end{remark}
\begin{example}\lbl{eg-fA-diagonal}
 Let $A$ be the matrix $\bsm 2&0\\0&3\esm$ over $\Q$, so that
 $f(A)=\bsm f(2)&0\\0&f(3)\esm$ (by Example~\ref{eg-diag-poly}).  Then
 $M_A=\Q^2$, with the multiplication rule $f.(s,t)=(f(2)s,f(3)t)$.  For
 example, if $g(x)=x^2-6$ then $g(2)=-2$ and $g(3)=3$ so we have
 \[ (x^2-6)(10,11) = (-2\tm 10,3\tm 11) = (-20,33). \]
\end{example}
\begin{example}\lbl{eg-fA-jordan}
 Let $A$ be the matrix $\bsm 1&1\\0&1\esm$ over $\Q$, so that
 $f(A)=\bsm f(1)&f'(1)\\0&f(1)\esm$ (by Example~\ref{eg-jordan-poly}).
 Then $M_A$ is the set $\Q^2$ with the group operation
 $f.(s,t)=(f(1)s+f'(1)t,f(1)t)$.  For example, if $g(x)=x^2-6$ then
 $g(1)=-5$ and $g'(1)=2$ so we have
 \[ (x^2-6)(10,11)=(-5\tm 10+2\tm 11,-5\tm 11)=(-28,-55). \]
\end{example}
\begin{example}\lbl{eg-eigen-module}
 The simplest examples are where $A$ is just a $1\tm 1$ matrix, or in
 other words just an element $\lm\in K$.  The module $M_\lm$ is just a
 copy of $K$, with the multiplication rule $f.a=f(\lm)a$.  For
 example, the polynomial $f(x)=1+x+x^2$ satisfies $f(2)=7$, so in the
 module $M_2$ over $\Q[x]$ we have $f.6=7\tm 6=42$.
\end{example}

There is a well-known correspondence between matrices and
endomorphisms, and for many purposes it is more natural to use the
latter.  Let $V$ be a vector space over a field $K$, and let $\phi$ be
an endomorphism of $V$ (in other words, a linear map from $V$ to
itself).  Then we can define $\phi^2(v)=\phi(\phi(v))$ to get a new
endomorphism of $V$, and similarly we can define $\phi^k$ for all
$k\geq 0$.  More generally, for any polynomial
$f(x)=a_0+a_1x+\ldots+a_dx^d\in K[x]$ we can define an endomorphism
$f(\phi)$ by 
\[ f(\phi)(v) = a_0v+a_1\phi(v)+\ldots+a_d\phi^d(v). \]
We can then make $V$ into a module over $K[x]$ by defining
$fv=f(\phi)(v)$.  

\begin{example}\lbl{eg-diffop-endo}
 We can define a map $\partial\:\CRR\xra{}\CRR$ by $\partial(f)=f'$.
 As $(f+g)'=f'+g'$ and $(cf)'=cf'$ for constant $c$, we see that
 $\partial$ is an $\R$-linear endomorphism of $\CRR$, making $\CRR$
 into a module over $\R[x]$.  The multiplication rule is as follows:
 if $p(x)=\sum_ia_ix^i$ and $f(t)\in\CRR$ then
 \begin{align*}
  p.f &=
   a_0\partial^0(f) + a_1\partial^1(f) + a_2\partial^2(f) + \ldots \\
   &= a_0 f + a_1 f' + a_2 f'' + \ldots.
 \end{align*}
 Thus, this example is just the same as Example~\ref{eg-diffop}, with
 a slightly different viewpoint and less natural notation.
\end{example}

We explained above how a vector space $V$ over $K$ with an
endomorphism $\phi$ can be regarded as a $K[x]$-module.  We conclude
this section by showing that \emph{every} $K[x]$-module arises in this
way. 

Indeed, let $M$ be a module over $K[x]$.  As mentioned previously,
axioms~(a) to~(e) say that $M$ is an Abelian group under addition.
Also, if $a\in K$ then we can regard $a$ as a constant polynomial, so
$am$ is defined for all $m\in M$.  As $M$ is a module over $K[x]$,
axioms~(f) to~(j) are valid for all polynomials $a$ and $b$, so
certainly they are valid for the special case of constant
polynomials.  Thus, we can regard $M$ as a module over $K$.  A module
over a field is the same thing as a vector space, so $M$ is a vector
space over $K$.  

Next, if $m\in M$ then $xm$ is another element of $M$, so we can
define a function $\phi\:M\xra{}M$ by $\phi(m)=xm$.  I claim that this
is a $K$-linear endomorphism.  Indeed, for any $m,n\in M$ we have
$x(m+n)=xm+xn$ by the right distributivity law, which means that
$\phi(m+n)=\phi(m)+\phi(n)$.  Moreover, for $a\in K$ we have $ax=xa$,
so 
\[ a\phi(m) = a(xm) = (ax)m = (xa)m =x(am) = \phi(am) \]
(using axiom~(h) twice).  This shows that $\phi$ is linear, as
claimed.  Now consider a polynomial $f(x)=\sum_i a_ix^i\in K[x]$.  I
claim that $fm=\sum_i a_i\phi^i(m)=f(\phi)(m)$ for all $m\in M$.
Indeed, we have
\begin{align*}
 (x^2)m &= x(xm)=x\phi(m)=\phi(\phi(m)) = \phi^2(m) \\
 (x^3)m &= x(x^2m)=x\phi^2(m)=\phi(\phi^2(m))=\phi^3(m).
\end{align*}
Extending this by induction, we see that $x^km=\phi^k(m)$ for all
$k$.  Thus
\begin{align*}
 fm &= (\sum_i a_i x^i)m \\
    &= \sum_i a_i x^im \\
    &= \sum_i a_i\phi^i(m) \\
    &= f(\phi)(m). 
\end{align*}
Thus, the module structure is obtained from the endomorphism $\phi$ in
the way considered previously.

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{Jpow}
\begin{exercise}\exlabel{ex-jordan-powers}
 Let $A$ be the matrix $\bsm 1&1&0&0\\0&1&1&0\\0&0&1&1\\0&0&0&1\esm$.
 Find $A^2$, $A^3$ and $A^4$.  Can you give a general rule for $A^n$?
\end{exercise}

\begin{solution}
 The first few powers are
 \begin{align*}
  A^2 &= \bsm 1&2&1&0 \\
              0&1&2&1 \\
              0&0&1&2 \\
              0&0&0&1 \esm \\
  A^3 &= \bsm 1&3&3&1 \\
              0&1&3&3 \\
              0&0&1&3 \\
              0&0&0&1 \esm \\
  A^4 &= \bsm 1&4&6&4 \\
              0&1&4&6 \\
              0&0&1&4 \\
              0&0&0&1 \esm
 \end{align*}
 In general, in the matrix $A^n$ all the entries in the $k$'th band
 above and parallel to the diagonal are equal to the binomial
 coefficient $\bsm n\\k\esm$.  The entries below the diagonal are
 zero. 
\end{solution}

%\ip{MA}
\begin{exercise}\exlabel{ex-MA-calc}
 \begin{itemize}
  \item[(a)] Put $A=\bsm 1&0&1\\0&1&0\\0&0&1\esm$ and
   $m=(1,1,1)\in M_A$.  Calculate $(x^3-1)m$.
  \item[(b)] Put $A=\bsm 0&0&1\\0&1&0\\1&0&0\esm$ and
   $m=(1,2,3)\in M_A$.  Calculate $(x^3-1)m$.
  \item[(c)] Put $A=\bsm 1&1\\1&1\esm$ and $m=(1,-1)\in M_A$.
   Calculate $(14x^{12}+5x^{11}-36x^7-22x^4+13x-5)m$.  (You may wish
   to start by calculating $fm$ for some very simple polynomials $f$
   first.)
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] $A^3-I=\bsm 0&0&3\\0&0&0\\0&0&0\esm$ so
   \[ (x^3-1)m = \bsm 0&0&3\\0&0&0\\0&0&0\esm\bsm 1\\1\\1\esm
               = \bsm 3\\0\\0\esm.
   \]
  \item[(b)] $A^2=I$ so $A^3=A$ so $A^3-I=A-I$.  Moreover $Am=(3,2,1)$
   so $(A-I)m=Am-m=(3,2,1)-(1,2,3)=(2,0,-2)$.  Thus
   $(x^3-1)m=(2,0,-2)$.
  \item[(c)] $Am=(0,0)$ so $x^km=A^km=0$ for all $k>0$.  Thus when we
   expand out $(14x^{12}+5x^{11}-36x^7-22x^4+13x-5)m$, all the terms
   except the last one are zero, so we are left with $-5m=(-5,5)$.
 \end{itemize}
\end{solution}

%\ip{ch2}
\begin{exercise}\exlabel{ex-cayley-two}
 Let $A=\bsm a&b\\ c&d\esm$ be a $2\tm 2$ matrix, and put
 $f(x)=x^2-(a+d)x+(ad-bc)$.  Show that $f(A)=\bsm 0&0\\0&0\esm$.
 (This is the $2\tm 2$ case of the Cayley-Hamilton theorem.)
\end{exercise}
\begin{solution}
 We have
 \[ A^2 = \bbm a&b\\c&d\ebm\bbm a&b\\c&d\ebm
        = \bbm a^2 + bc & ab + bd \\ ac + cd & bc+d^2 \ebm
 \]
 and
 \[ (a+d)A = \bbm a^2+ad & ab+bd \\ ac + cd & ad + d^2 \ebm \]
 so
 \begin{align*}
  f(A) &= A^2 - (a+d)A + (ad-bc)I \\
       &= \bbm a^2 + bc & ab + bd \\ ac + cd & bc+d^2 \ebm
          - \bbm a^2+ad & ab+bd \\ ac + cd & ad + d^2 \ebm +
          \bbm ad-bc & 0 \\ 0 & ad-bc \ebm \\
       &= \bbm 0 & 0\\0 & 0\ebm.
 \end{align*}
\end{solution}


%\ip{fA}
\begin{exercise}\exlabel{ex-fA-block}
 Put $A=\bsm 1&1\\1&1\esm$ and $f(x)=x^4-3x$.  Calculate $f(A)$.
\end{exercise}
\begin{solution}
 $A^2=\bsm 2&2\\2&2\esm=2A$ so $A^4=(A^2)^2=(2A)^2=4A^2=8A$.  Thus
 $f(A)=A^4-3A=8A-3A=5A=\bsm 5&5\\5&5\esm$. 
\end{solution}

%\ip{fref}
\begin{exercise}\exlabel{ex-fA-swap}
 Consider the matrix 
 \[ A = \left(\bbm 0 & 1 \\ 1 & 0 \ebm\right), \]
 and a polynomial $f(x)=\sum_ia_ix^i$.
 \begin{itemize}
  \item[(a)] Calculate $A^i$ for some small numbers $i$, then give the
   general rule.
  \item[(b)] Write $b=a_0+a_2+a_4+\ldots=\sum_ja_{2j}$ and
   $c=a_1+a_3+\ldots=\sum_ja_{2j+1}$.  Express $f(1)$ and $f(-1)$ in
   terms of $b$ and $c$.
  \item[(c)] Show that
   \[ f(A) =
      \frac{f(1)}{2}\bbm 1&1\\1&1\ebm +
      \frac{f(-1)}{2}\bbm 1&-1\\-1&1\ebm.
   \]
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Clearly $A^0=I$ and $A^1=A$.  We observe that $A^2=I$,
   and it follows immediately that $A^i$ is $I$ whenever $i$ is even,
   and $A$ whenever $i$ is odd.
  \item[(b)] We have $f(1)=\sum_ia_i=b+c$, and
   $f(-1)=\sum_i(-1)^ia_i=b-c$.  It follows that $b=(f(1)+f(-1))/2$
   and $c=(f(1)-f(-1))/2$.
  \item[(c)] We have $f(A)=\sum_ia_iA^i$.  The term corresponding to
   an even number $i=2j$ is $a_{2j}I$, whereas the term corresponding
   to an odd number $i=2j+1$ is $a_{2j+1}A$.  We thus have 
   \begin{align*}
    f(A) &= \sum_j (a_{2j} I + a_{2j+1} A)  \\
         &= b I + c A \\
         &= ((f(1) + f(-1))/2) I + ((f(1) - f(-1))/2) A \\
         &= f(1) (I + A)/2 + f(-1) (I - A)/2 \\
         &= \frac{f(1)}{2}\bbm 1&1\\1&1\ebm +
            \frac{f(-1)}{2}\bbm 1&-1\\-1&1\ebm.
   \end{align*}
 \end{itemize}
\end{solution}

\section{General module theory}
\label{sec-general-modules}

Let $R$ be a ring.
\begin{definition}\lbl{defn-submodule}
 Let $M$ be an $R$-module.  A \emph{submodule} of $M$ is a subset
 $N\sse M$ such that
 \begin{itemize}
 \item[(a)] $0\in N$
 \item[(b)] If $n,m\in N$ then $n+m\in N$ (ie $N$ is closed under
  addition)
 \item[(c)] If $n\in N$ and $a\in R$ then $an\in N$ (ie $N$ is closed
  under multiplication by elements of $R$).
 \end{itemize}
\end{definition}
Note that if $N$ is a submodule and $n\in N$ then $-n=(-1)n\in N$, so
$N$ is closed under negation and thus is a subgroup of $M$ under
addition.  It is easy to see that $N$ can itself be considered as an
$R$-module. 

\begin{example}\lbl{eg-vector-subspace}
 If $R$ is a field, then modules are just the same as vector spaces,
 and submodules are just the same as vector subspaces.
\end{example}
\begin{example}\lbl{eg-subgroup}
 If $R=\Z$, then modules are just the same as Abelian groups, and
 submodules are just the same as subgroups.
\end{example}
\begin{example}\lbl{eg-trivial-submodule}
 If $M$ is a module over any ring $R$, it is clear that $\{0\}$ and
 $M$ itself are submodules of $M$.
\end{example}
\begin{example}\lbl{eg-stable-subspace}
 Let $V$ be a vector space over a field $K$, equipped with a
 $K$-linear endomorphism $\phi\:V\xra{}V$.  We regard $V$ as a
 $K[x]$-module in the usual way.  We say that a subset $W\sse V$ is
 \emph{stable under $\phi$} if $\phi(w)\in W$ for all $w\in W$ (or
 more briefly, if $\phi(W)\sse W$).  

 I claim that a subset $W\sse V$ is a $K[x]$-submodule if and only if
 it is a vector subspace and is stable under $\phi$.  Indeed, suppose
 that $W$ is a submodule.  Then it is certainly closed under addition
 and under multiplication by constant polynomials (ie elements of $K$)
 so it is a vector subspace.  Also, it is closed under multiplication
 by $x$, so for $w\in W$ we have $\phi(w)=xw\in W$; this shows that
 $W$ is stable under $\phi$, as claimed.

 Conversely, suppose that $W$ is a vector subspace and is stable under
 $\phi$.  Clearly $W$ is closed under addition.  For any $w\in W$ we
 have $\phi(w)\in W$.  Thus $\phi^2(w)=\phi(\phi(w))=\phi(\text{an
   element of $W$})= \text{ another element of $W$ }$, so
 $\phi^2(W)\sse W$.  Thus $\phi^3(w)=\phi(\phi^2(w))=\phi(\text{an
   element of $W$})= \text{ another element of $W$ }$, so
 $\phi^3(W)\sse W$, and so on, so $\phi^k(w)\in W$ for all $k\geq 0$.
 Now consider a polynomial $f(x)=a_0+\ldots+a_dx^d\in K[x]$.  We then
 have $fw=\sum_ia_i\phi^i(w)$.  The vectors $\phi^i(w)$ lie in $W$,
 the coefficients $a_i$ lie in $K$, and $W$ is a vector subspace of
 $V$, so we see that $\sum_ia_i\phi^i(w)\in W$.  Thus $fw\in W$ for
 all $w\in W$ and $f\in K[x]$, so $W$ is a submodule of $V$.
\end{example}
\begin{example}
 Let $A$ be the matrix $\bbm 0&-6\\1&5\ebm$ over $\Q$, and use this to
 make $\Q^2$ into a module over $\Q[x]$.  Put
 $W_0=\{(u,v)\in\Q^2\st u=-3v\}$ and $W_1=\{(u,v)\in\Q^2\st u=-4v\}$.
 A typical element of $W_0$ has the form $(-3v,v)$ and we have
 \[ \bbm 0&-6\\1&5 \ebm\bbm -3v\\v\ebm = \bbm -6v\\2v\ebm, \]
 which also lies in $W_0$.  Thus $W_0$ is stable under $A$ and thus is
 a submodule of $\Q^2$.

 However, $W_1$ is not a submodule.  Indeed, the vector $(-4,1)$ lies
 in $W_1$ but 
 \[ \bbm 0&-6\\1&5 \ebm\bbm -4\\1\ebm = \bbm -6\\1\ebm, \]
 which does not lie in $W_1$.
\end{example}
\begin{example}\lbl{eg-eigenspaces}
 Suppose that $\lm,\mu\in K$ and $\lm\neq\mu$.  Define
 $\phi\:K^2\xra{}K^2$ by $\phi(u,v)=(\lm u,\mu v)$, and use this to
 make $K^2$ into a module over $K[x]$.  Define
 \begin{align*}
  L &= \{ (u,0) \st u\in K \} \subset K^2 \\
  M &= \{ (0,v) \st v\in K \} \subset K^2.
 \end{align*}
 I claim that $L$ and $M$ are $K[x]$-submodules of $K^2$, and moreover
 that the only submodules are $\{0\}$, $L$, $M$ and $K^2$ itself.

 It is clear that $L$ and $M$ are vector subspaces of $K^2$.  Moreover
 we have $\phi(u,0)=(\lm u,0)\in L$, so $L$ is stable under $\phi$ and
 thus is a submodule.  Similarly $\phi(0,v)=(0,\mu v)\in M$, so $M$ is
 a submodule.  It is trivial to check that $\{0\}$ and $K^2$ are
 subspaces of $K^2$.

 Now let $W$ be any submodule of $K^2$.  Then $W$ is also a vector
 subspace, with $0\leq\dim(W)\leq\dim(K^2)=2$.  If $\dim(W)=0$ then
 clearly $W=0$, and if $\dim(W)=2$ then clearly $W=K^2$.  We can thus
 assume that $\dim(W)=1$, so $W=K.(u,v)$ for some vector
 $(u,v)\neq(0,0)$.  Because $W$ is a $K[x]$-submodule, we know that
 $(\lm u,\mu v)\in W$, but $W=K.(u,v)$ so $(\lm u,\mu v)=\nu.(u,v)$
 for some $\nu\in K$, so $(\lm-\nu)u=(\mu-\nu)v=0$.  If $u\neq 0$ then
 we deduce that $\nu=\lm$ so $(\mu-\lm)v=0$, but $\mu\neq\lm$ so
 $v=0$.  This means that $W=K.(u,0)= L$.  Similarly, if $v\neq 0$ we
 deduce that $W=M$.
\end{example}
\begin{example}\lbl{eg-poly-diff}
 The set $\R[t]$ of polynomial functions is a vector subspace of the
 space $\CRR$ of all smooth functions from $\R$ to $\R$.  Moreover if
 $f\in\R[t]$ then the derivative of $f$ is again a polynomial, in
 other words $\partial(f)=f'\in\R[t]$.  This means that the subspace
 $\R[t]$ is stable under the endomorphism $\partial$, so it is an
 $\R[D]$-submodule of $\CRR$.
\end{example}
\begin{example}\lbl{eg-sin-cos}
 Let $W$ be the space of functions of the form
 $f(t)=a\cos(t)+b\sin(t)$ (with $a,b\in\R$).  Because
 $\partial(a\cos(t)+b\sin(t))=b\cos(t)-a\sin(t)$, we see that $W$ is
 stable under $\partial$.  It is thus an $\R[D]$-submodule of $\CRR$.
\end{example}

\begin{remark}\lbl{rem-intersection}
 Suppose that $N_0$ and $N_1$ are two submodules of an $R$-module $M$.
 I claim that $N_0\cap N_1$ is again a submodule.  Indeed, as $0\in
 N_0$ and $0\in N_1$ we have $0\in N_0\cap N_1$.  Suppose that $n,m\in
 N_0\cap N_1$.  As $n,m\in N_0$ and $N_0$ is a submodule we have
 $n+m\in N_0$.  As $n,m\in N_1$ and $N_1$ is a submodule we have
 $n+m\in N_1$.  Thus $n+m\in N_0\cap N_1$.  Now suppose that
 $a\in R$.  As $N_0$ is a submodule and $n\in N_0$ we have
 $an\in N_0$.  As $N_1$ is a submodule and $n\in N_1$ we also have
 $an\in N_1$, so $an\in N_0\cap N_1$.  This shows that $N_0\cap N_1$
 is a submodule, as claimed.
\end{remark}

\begin{definition}\lbl{defn-sum}
 Suppose that $N_0$ and $N_1$ are two submodules of an $R$-module $M$.
 We define $N_0+N_1$ to be the set of elements $x\in M$ that can be
 written in the form $x=n_0+n_1$ for some $n_0\in N_0$ and
 $n_1\in N_1$.  I claim that this is a submodule of $M$.  Indeed,
 suppose that $x,y\in N_0+N_1$, so we can write $x=n_0+n_1$ and
 $y=m_0+m_1$ with $n_0,m_0\in N_0$ and $n_1,m_1\in N_1$.  Then
 $x+y$ can be written as $(n_0+m_0)+(n_1+m_1)$ , with $n_0+m_0\in N_0$
 and $n_1+m_1\in N_1$, so $x+y\in N_0+N_1$.  Similarly, if $a\in R$
 then $an_0\in N_0$ and $an_1\in N_1$ so $ax=an_0+an_1\in N_0+N_1$.
 This shows that $N_0+N_1$ is closed under addition and under
 multiplication by $R$, so it is a submodule as claimed.
\end{definition}

\begin{definition}\lbl{defn-direct-ext}
 Let $N_0$ and $N_1$ be $R$-modules.  We define $N_0\op N_1$ to be the
 set of pairs $(n_0,n_1)$ with $n_0\in N_0$ and $n_1\in N_1$.  We make
 this set into an $R$-module by defining
 \begin{align*}
  (n_0,n_1) + (m_0,m_1) &= (n_0+m_0,n_1+m_1) \\
  a (n_0,n_1) &= (an_0,an_1).
 \end{align*}
 (It is a longish but straightforward exercise to check that the
 axioms are satisfied.)  This $R$-module is called the \emph{external
 direct sum} of $N_0$ and $N_1$.
\end{definition}
\begin{example}\lbl{eg-Z-six}
 The group $\Z_2$ has elements $\ov{0}$ and $\ov{1}$, and the group
 $\Z_3$ has elements $\ov{0}$, $\ov{1}$ and $\ov{2}$.  Thus, the group
 $\Z_2\op\Z_3$ has elements $(\ov{0},\ov{0})$, $(\ov{0},\ov{1})$,
 $(\ov{0},\ov{2})$, $(\ov{1},\ov{0})$, $(\ov{1},\ov{1})$ and
 $(\ov{1},\ov{2})$.  To illustrate the addition law, we have
 $(\ov{1},\ov{2})+(\ov{1},\ov{2})=(\ov{2},\ov{4})$.  The first
 component is to be interpreted as an element of $\Z_2$, so
 $\ov{2}=\ov{0}$.  The second component is to be interpreted as an
 element of $\Z_3$, so $\ov{4}=\ov{1}$.  Thus
 $(\ov{1},\ov{2})+(\ov{1},\ov{2})=(\ov{0},\ov{1})$.
\end{example}
\begin{example}\lbl{eg-Rn-Rm}
 An element of $R^n\op R^m$ is a pair $(u,v)$ with $u\in R^n$ and
 $v\in R^m$, or in other words a list
 $(u_1,\ldots,u_n,v_1,\ldots,v_m)$ where each $u_i$ and $v_j$ is an
 element of $R$.  Thus, $R^n\op R^m=R^{n+m}$.
\end{example}
The next example relies on the following definition:
\begin{definition}\lbl{defn-block-sum}
 Let $A$ and $B$ be matrices over a field $K$, of sizes $p\tm q$ and
 $n\tm m$.  The \emph{block sum} of $A$ and $B$ is the matrix
 $\blockmat{A}{0_{n\tm q}}{0_{p\tm m}}{B}$, of size $(p+n)\tm(q+m)$.
 This is denoted by $A\op B$.  For example, if $A=\bsm 1&2\\3&4\esm$
 and $B=\bsm 5&6\\7&8\esm$ then the block sum of $A$ and $B$ is 
 \[ A\op B = \bbm 1&2\\3&4\ebm\op\bbm 5&6\\7&8\ebm = 
     \bbm 1&2&0&0\\3&4&0&0\\0&0&5&6\\0&0&7&8 \ebm.
 \]
 Note that an element $w\in R^{p+n}$ can be written as $w=(u,v)$ with
 $u\in R^p$ and $v\in R^n$, and we have
 \[ (A\op B)w = \blockmat{A}{0}{0}{B} \blockvec{u}{v} = 
     \blockvec{Au}{Bv}.
 \]
\end{definition}
\begin{example}\lbl{eg-block-sum}
 Let $A$ and $B$ be square matrices over a field $K$, of sizes $n$ and
 $m$ say.  We then have modules $M_A$ and $M_B$ over $K[x]$.  The
 elements of $M_A\op M_B$ are pairs $w=(u,v)$ with $u\in K^n$ and
 $v\in K^m$, or equivalently they are elements of $K^{n+m}$.  The
 module structure is given by the rule $x(u,v)=(xu,xv)=(Au,Bv)$, or in
 other words $xw=(A\op B)w$.  Thus $M_A\op M_B=M_{A\op B}$.
\end{example}

\begin{definition}\lbl{defn-direct-int}
 Let $M$ be an $R$-module, and let $N_0$ and $N_1$ be submodules.  We
 say that $M$ is the \emph{internal direct sum} of $N_0$ and $N_1$ if
 $N_0+N_1=M$ and $N_0\cap N_1=\{0\}$. 
\end{definition}
\begin{remark}\lbl{rem-int-ext}
 We can define a function $\sg\:N_0\op N_1\xra{}M$ by
 $\sg(n_0,n_1)=n_0+n_1$.  When we have defined homomorphisms and
 isomorphisms of modules, we will see that $\sg$ is always a
 homomorphism, and that $\sg$ is an isomorphism if and only if $M$ is
 the internal direct sum of $N_0$ and $N_1$.  This is the precise
 sense in which internal direct sums are ``the same'' as external
 ones.
\end{remark}
\begin{example}\lbl{eg-eigen-splitting}
 In example~\ref{eg-eigenspaces} we see that $K^2$ is the internal
 direct sum of $L$ and $M$.
\end{example}
\begin{example}
 Consider the Abelian group $M=\Z_{12}$ as a module over $\Z$.  Put
 $N_0=\{\ov{0},\ov{3},\ov{6},\ov{9}\}$ and
 $N_1=\{\ov{0},\ov{4},\ov{8}\}$.  It is easy to see that $N_0$ and
 $N_1$ are subgroups, and obviously $N_0\cap N_1=\{\ov{0}\}$.  I claim
 that we also have $N_0+N_1=M$.  Indeed, we have
 $\ov{1}=\ov{9}+\ov{4}\in N_0+N_1$ and $N_0+N_1$ is a submodule so for
 any $a\in\Z$ we have $\ov{a}=a.\ov{1}\in N_0+N_1$, as required.  Thus
 $M$ is the internal direct sum of $N_0$ and $N_1$.
\end{example}
\begin{example}\lbl{eg-trig-splitting}
 Let $V$ be the space of functions $f\in\CRR$ that satisfy $f''=f$.
 This is a vector space closed under differentiation, so it is an
 $\R[D]$-submodule of $\CRR$.  Put $W_0=\{f\st f'=f\}$ and
 $W_1=\{f\st f'=-f\}$.  These are also vector spaces closed under
 differentiation, so they are $\R[D]$-submodules of $\CRR$.  If
 $f\in W_1$ then $f''=(-f)'=-(-f)=f$, so $f\in V$.  This shows that
 $W_1\sse V$ and similarly $W_0\sse V$, so $W_0$ and $W_1$ are
 submodules of $V$.  

 I claim that $V$ is the direct sum of $W_0$ and $W_1$.  One way to
 see this is just to solve the differential equations.  We find that
 $V$ consists of all functions of the form $ae^t+be^{-t}$, that $W_0$
 consists of all functions of the form $ae^t$, and that $W_1$
 consists of all functions of the form $be^{-t}$, and the claim is
 clear from this.

 We can also prove the claim without solving the differential
 equations explicitly.  Indeed, if $f\in W_0\cap W_1$ then $f=f'$
 (because $f\in W_0$) and $f'=-f$ (because $f\in W_1$) so $f=-f$, so
 $f=0$.  This shows that $W_0\cap W_1=\{0\}$.  Next, suppose that
 $g\in V$, so $g''=g$.  Put $g_0=(g+g')/2$ and $g_1=(g-g')/2$.  We
 find that $g'_0=(g'+g'')/2=(g'+g)/2=g_0$, so $g_0\in W_0$.
 Similarly, we have $g'_1=(g'-g'')/2=(g'-g)/2=-g_1$, so $g_1\in W_1$.
 As $g=g_0+g_1$ it follows that $g\in W_0+W_1$, and we conclude that
 $V=W_0+W_1$ as required.
\end{example}

\begin{definition}\lbl{defn-submod-generated}
 Let $M$ be a module over a ring $R$, and let $m_1,\ldots,m_r$ be
 elements of $M$.  Let $N$ be the set of elements $x\in M$ that can be
 written in the form $x=a_1m_1+\ldots+a_rm_r$ for some
 $a_1,\ldots,a_r\in R$.  I claim that this is a submodule of $M$.
 Indeed, if $x,y\in N$ then we have $x=\sum_ia_im_i$ and
 $y=\sum_ib_im_i$ for some $a_1,\ldots,a_r,b_1,\ldots,b_r\in R$.  We
 then have $x+y=\sum_i(a_i+b_i)m_i$ so $x+y\in N$; this shows that $N$
 is closed under addition.  Similarly, if $c\in R$ we have
 $cx=\sum_i(ca_i)m_i\in N$, so $N$ is closed under multiplication by
 $R$, so it is a submodule as claimed.

 We call $N$ \emph{the submodule generated by $\{m_1,\ldots,m_r\}$}.
 In particular, we say that \emph{$M$ is generated by
   $\{m_1,\ldots,m_r\}$ } if $N=M$, or equivalently if every element
 $x\in M$ can be written in the form $a_1m_1+\ldots+a_rm_r$.  We say
 that $M$ is \emph{finitely generated} if there is some finite list of
 elements that generates $M$.  
\end{definition}

\begin{definition}\lbl{defn-cyclic}
 We say that an $R$-module $M$ is \emph{cyclic} if there is a single
 element $m\in M$ that generates $M$, which means that every element
 $x\in M$ can be written in the form $x=am$ for some $a\in R$.
\end{definition}

\begin{example}\lbl{eg-standard-basis}
 The module $R^d$ is clearly generated by the standard basis elements
 $e_1=(1,0,\ldots,0)$, $e_2=(0,1,0,\ldots,0)$ and so on.  In
 particular it is finitely generated.  It is not cyclic unless $d=1$.
\end{example}
\begin{example}\lbl{eg-finite-fg}
 Let $M$ be a finite Abelian group, considered as a $\Z$-module.  Let
 the elements of $M$ be $m_1,\ldots,m_d$.  Then any element $m\in M$
 is equal to $m_i$ for some $i$, so certainly it can be expressed in
 the form $\sum_i a_im_i$ (for example,
 $m_2=0.m_1+1.m_2+0.m_3+\ldots+0.m_d$).  Thus, $M$ is finitely
 generated as a $\Z$-module.
\end{example}
\begin{example}\lbl{eg-poly-cyclic}
 Let $W_2$ be the space of functions of the form $f(t)=a+bt+ct^2$,
 considered as a module over $\R[D]$ in the usual way.  In particular,
 the function $g(t)=t^2$ gives an element of $W_2$.  I claim that
 $W_2$ is generated by $g$, and thus is cyclic.  Indeed, we have
 $g'(t)/2=t$ and $g''(t)/2=1$.  It follows that for any function
 $f(t)=a+bt+ct^2$, we have
 $(c+(b/2)D+(a/2)D^2)g=cg+(b/2)g'+(a/2)g''=f$, so $f\in\R[D]g$.  This
 proves that $\R[D]g=W_2$ as required.

 It is not hard to extend this method to show that the space $W_d$ of
 polynomials of degree at most $d$ is also a cyclic module over
 $\R[D]$ generated by the function $g(t)=t^d$.
\end{example}
\begin{example}\lbl{eg-inf-gen}
 Consider $\R[x]$ as a module over $\R$; I claim it is not finitely
 generated.  Indeed, suppose we have a finite list $f_1,\ldots,f_n$ of
 elements of $\R[x]$.  Let $d_i$ be the degree of the polynomial
 $f_i$, and put $d=\max(d_1,\ldots,d_n)$.  Then each of the
 polynomials $f_i$ only involves the powers $1,x,x^2,\ldots,x^d$, so
 any polynomial of the form $a_1f_1+\ldots+a_nf_n$ (with
 $a_1,\ldots,a_n\in\R$) also involves only these powers.  This means
 that $x^{d+1}$ cannot be written in the form $a_1f_1+\ldots+a_nf_n$,
 so the elements $f_1,\ldots,f_n$ do not generate $\R[x]$ as a module
 over $\R$.
\end{example}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{imann}
\begin{exercise}\exlabel{ex-image-annihilator}
 Let $\al\:M\xra{}N$ be a homomorphism of modules over $\C[x]$.
 Suppose that $(x^3-x)M=\{0\}$ and $x^5N=\{0\}$.  Prove that for
 $n\in\img(\al)$ we have $xn=0$.  Can you formulate a general theorem
 of which this is a special case?
\end{exercise}
\begin{solution}
 Let $\al\:M\xra{}N$ be a homomorphism of modules over a Euclidean
 domain $R$.  Suppose that $aM=\{0\}$ and $bN=\{0\}$ and let $c$ be
 the gcd of $a$ and $b$.  I claim that $cn=0$ for all
 $n\in\img(\al)$.  Indeed, we can write $c=au+bv$ for some
 $a,b\in R$.  If $n\in\img(\al)$ then $n=\al(m)$ for some $m\in M$.
 We have $am=0$ (because $aM=\{0\}$) so
 $an=a\al(m)=\al(am)=\al(0)=0$.  We also have $n\in N$ and $bN=\{0\}$
 so $bn=0$.  Thus $cn=uan+vbn=0+0=0$ as claimed.

 In the case considered we have $R=\C[x]$ and $a=x^3-x=(x-1)(x+1)x$
 and $b=x^5$ so it is clear that $c=x$.  Thus $xn=0$ for all
 $n\in\img(\al)$. 
\end{solution}

%\ip{retract}
\begin{exercise}\exlabel{ex-cyclic-retract}
 Let $R$ be a ring, and let $M$ and $N$ be $R$-modules.  Show that if
 $M\op N$ is cyclic, then so are $M$ and $N$.
\end{exercise}
\begin{solution}
 Suppose that $M\op N$ is cyclic, so there is an element
 $(x,y)\in M\op N$ as an $R$-module.  This means that for any element
 $(m,n)\in M\op N$, there exists $a\in R$ such that $a(x,y)=(m,n)$.
 In particular, for any element $m\in M$ we have $(m,0)\in M\op N$, so
 there exists $a\in R$ such that $a(x,y)=(m,0)$, which means that
 $m=ax$.  This shows that $M$ is generated by the single element $x$,
 so $M$ is cyclic.  Similarly, $N$ is generated by $y$ and so is
 cyclic.
\end{solution}

%\ip{submod}
\begin{exercise}\exlabel{ex-which-submodule}
 \begin{itemize}
  \item[(a)] Put $N_0=\{(n,m)\in\Z^2\st n-m\text{ is even }\}$ and
   $N_1=\{(n,m)\in\Z^2\st n-m\text{ is odd }\}$.  Are these
   $\Z$-submodules of $\Z^2$?
  \item[(b)] Put $A=\bsm 1&1\\1&1\esm$ and
   $N_0=\{(u,v)\in\R^2\st u-v=0\}$ and $N_1=\{(u,v)\st u+v=0\}$.  Are
   these $\R[x]$-submodules of $M_A$?
  \item[(c)] Put $N_0=\{f\in\CRR\st f(1)=0\}$ and
   $N_1=\{f\in\CRR\st\int_0^2 f=0\}$.  Are these $\R[D]$-submodules
   of $\CRR$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The set $N_1$ is not a submodule, because $(1,0)\in N_1$
   but $2(1,0)=(2,0)\not\in N_1$.  However, the set $N_0$ is a
   submodule.  To see this, suppose that $(n,m)$ and $(n',m')$ lie in
   $N_0$, so $n-m$ and $n'-m'$ are even.  Then
   $(n,m)+(n',m')=(n+n',m+m')$ and the integer
   $(n+n')-(m+m')=(n-m)+(n'-m')$ is even so $(n,m)+(n',m')\in N_0$.
   Similarly, for any $a\in\Z$ we have $a(n,m)=(an,am)$ and
   $an-am=a(n-m)$ is even, so $a(n,m)\in N_0$.  It is clear that
   $(0,0)\in N_0$ and it follows that $N_0$ is a submodule as
   claimed.
  \item[(b)] I claim that both $N_0$ and $N_1$ are submodules of
   $M_A$.  It is clear that they are both vector subspaces of $\R^2$,
   so it is enough to check that they are both stable under $A$.  An
   element $w\in N_0$ has the form $w=\bsm x\\x\esm$ so
   $Aw=\bsm 1&1\\1&1\esm\bsm x\\x\esm=\bsm 2x\\2x\esm$, so
   $Aw\in N_0$.  This shows that $N_0$ is stable under $A$ and thus is
   a submodule.  Similarly, an element $w\in N_1$ has the form
   $w=\bsm x\\-x\esm$ so $Aw=\bsm 0\\0\esm$.  As the zero vector
   certainly lies in $N_1$ we have $Aw\in N_1$ and so $N_1$ is also
   stable under $A$.
  \item[(c)] I claim that neither $N_0$ nor $N_1$ is an
   $\R[D]$-submodule of $\CRR$.  Indeed, put $f(t)=t-1$, so
   $f\in\CRR$.  Then $f(0)=0$, so $f\in N_0$.  However,
   $f'(0)=1\neq 0$, so $f'\not\in N_0$, so $N_0$ is not closed under
   differentiation, so it is not an $\R[D]$-submodule of $\CRR$.  
   To prove that $N_1$ is not a submodule, we can use the same
   function $f$.  We have $\int_0^2f=[t^2/2-t]_0^2=0$, so $f\in N_1$.
   However, $\int_0^2f'=\int_0^21=2$, so $f'\not\in N_1$, so $N_1$ is
   not a submodule.
 \end{itemize}
\end{solution}

%\ip{freecount}
\begin{exercise}\exlabel{ex-ten-elements}
 Let $R$ be a ring with exactly $10$ elements, and let $M$ be an
 $R$-module with exactly $20$ elements.  Prove that $M$ is not a free
 module.  
\end{exercise}
\begin{solution}
 If $M$ were free, it would be isomorphic to $R^d$ for some $d$, so we
 would have $20=|M|=|R^d|=|R|^d=10^d$.  As $20$ is not a power of
 $10$, this is impossible, so $M$ cannot be free.
\end{solution}

%\ip{latt24}
\begin{exercise}\exlabel{ex-twenty-four}
 For any integer $d$, let $N_d$ be the submodule of $\Z_{24}$
 generated by $\ov{d}$.  The group $N_6$ has precisely $4$ elements;
 list them.  Find integers $d$ and $e$ such that
 $N_6\cap N_4=N_d$ and $N_6+N_4=N_e$.
\end{exercise}
\begin{solution}
 The elements of $N_6$ are the multiples of $\ov{6}$, which are
 $\ov{0}$, $\ov{6}$, $\ov{12}$ and $\ov{18}$.  We can stop at this
 point because $\ov{24}=\ov{0}$, $\ov{30}=\ov{6}$ and so on.  Thus
 $N_6=\{\ov{0},\ov{6},\ov{12},\ov{18}\}$, and similarly we have
 $N_4=\{\ov{0},\ov{4},\ov{8},\ov{12},\ov{16},\ov{20}\}$.  From this we
 see that $N_4\cap N_6=\{\ov{0},\ov{12}\}=N_{12}$, so we can take
 $d=12$.  

 As $\ov{8}\in N_4$ and $\ov{18}\in N_6$, the group $N_4+N_6$ contains
 $\ov{8}+\ov{18}=\ov{26}=\ov{2}$.  As $N_4+N_6$ is a subgroup of
 $\Z_{24}$ we deduce that all multiples of $\ov{2}$ lie in $N_4+N_6$,
 so $N_2\sse N_4+N_6$.  On the other hand, as $4$, $6$ and $24$ are
 all even we see that all elements of $N_4+N_6$ have the form $\ov{a}$
 for some even integer $a$ and thus they lie in $N_2$.  This shows
 that $N_4+N_6=N_2$, so we can take $e=2$.
\end{solution}

%\ip{latt900}
\begin{exercise}\exlabel{ex-nine-hundred}
 For any natural number $d$ dividing $900$, let $N_d$ be the submodule
 of $\Z_{900}$ generated by $\ov{d}$.
 \begin{itemize}
  \item[(a)] What is the order of $N_{10}$?
  \item[(b)] Which standard group is isomorphic to $\Z_{900}/N_{10}$?
  \item[(c)] Find $d$ such that the submodule generated by $\ov{70}$ is
   $N_d$.
  \item[(d)] Find $d$ such that $N_{12}+N_{30}+N_{100}=N_d$.
  \item[(e)] Find $d$ such that $N_{30}\cap N_{50}=N_d$. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The order is $900/10=90$.
  \item[(b)] The factor group $\Z_{900}/N_{10}$ is isomorphic to
   $\Z_{10}$.
  \item[(c)] Here $d$ is the greatest common divisor of $70$ and
   $900$, which is $10$.
  \item[(d)] Here $d$ is the greatest common divisor of $12=2^2\tm 3$,
   $30=2\tm 3\tm 5$ and $100=2^2\tm 5^2$, so $d=2$.
  \item[(e)] Here $d$ is the least common multiple of $30=2\tm 3\tm 5$
   and $50=2\tm 5^2$, so $d=2\tm 3\tm 5^2=150$.
 \end{itemize}
\end{solution}

%\ip{MAcyc}
\begin{exercise}\exlabel{ex-MA-cyclic}
 Consider the following matrix over $\Q$.
 \[ A = \bbm 0&0&1\\0&1&1\\1&1&1 \ebm \]
 Show that the module $M_A$ over $\Q[x]$ is cyclic, and give a
 polynomial $f(x)$ such that $M_A\simeq\Q[x]/f(x)$.
\end{exercise}
\begin{solution}
 Put $u=(1,0,0)$.  Then $xu=(0,0,1)$ and $x^2u=(1,1,1)$.  These three
 vectors are clearly linear independent, so they form a basis of
 $\Q^3$, so they span $\Q^3$.  Thus any vector $v\in\Q^3$ can be
 written in the form $au+bxu+cx^2u$ for some $a,b,c\in\Q$.  In other
 words, $v=(a+bx+cx^2)u\in\Q[x]u$, so we see that $u$ generates $M_A$
 as a $\Q[x]$-module.  This means that $M_A\simeq\Q[x]/f(x)$ for some
 polynomial $f(x)$, which we can assume is monic.  From the general
 theory we know that the degree of $f$ is the size of $A$, which is
 $3$.  We also know that $f(x)$ is the only monic polynomial of degree
 $3$ such that $f(x).u=0$.

 The polynomial $f$ is in fact the characteristic polynomial of $A$,
 which can be calculated directly:
 \[ \det(xI-A) = \left| \begin{array}{ccc} 
     x & 0 & -1 \\ 0 & x-1 & -1 \\ -1 & -1 & x-1 
    \end{array} \right| =
    x^3 - 2 x^2 - x + 1.
 \]
 
 For another approach, consider the vector $x^3u=x(1,1,1)=(1,2,3)$.
 We would like to write this in the form $au+bxu+cx^2u$, so we want
 \[ (1,2,3)=a(1,0,0) + b(0,0,1) + c(1,1,1)=(a+c,c,b+c). \]
 The solution is $a=-1,b=1,c=2$, so $x^3u=-u+xu+2x^2u$, so
 $(x^3-2x^2-x+1)u=0$, so $f(x)=x^3- 2 x^2 - x + 1$.
\end{solution}

%\ip{Wdcyclic}
\begin{exercise}\exlabel{ex-Wd-cyclic}
 Let $W_d$ be the set of polynomials $f(t)$ of degree at most $d$.
 Prove that $W_d$ is a cyclic module over $\R[D]$.  What is the ideal
 $I\sse\R[D]$ such that $W_d\simeq\R[D]/I$?
\end{exercise}
\begin{solution}
 Define $\al\:\R[D]\xra{}W_d$ by $\al(p(D))=p(D).t^d$.  Note that
 \begin{align*}
  D  .t^d &= d t^{d-1} \\
  D^2.t^d &= d(d-1) t^{d-2} \\
  D^3.t^d &= d(d-1)(d-2) t^{d-3}
 \end{align*}
 and so on.  In general, we have $D^k.t^d=m_kt^{d-k}$, where
 $m_k=d(d-1)\ldots(d-k+1)=\prod_{i=0}^{k-1}(d-i)$, as one can easily
 check by induction.  Note also that when $k\leq d$ all the factors
 $d-i$ for $0\leq i\leq k-1$ are nonzero, so $m_k\neq 0$.  However, we
 have $m_k=0$ for $k>d$.  Any element $f(t)\in W_d$ has the form
 $f(t)=a_0+a_1t+\ldots+a_dt^d$ for some $a_0,\ldots,a_d\in\R$.  If we
 define 
 \[ p(D)= \sum_{i=0}^d a_{d-i} D^i/m_i = 
          a_d+a_{d-1}D/m_1+\ldots+a_0D^d/m_d
 \]
 we find that 
 \begin{align*}
  p(D).t^d &=
   a_d t^d + a_{d-1} m_1^{-1} D.t^d + \ldots + a_0 m_d^{-1} D^d.t^d \\
  &= a_d t^d + a_{d-1} t^{d-1} + \ldots + a_0 t^0 \\
  &= f(t).
 \end{align*}
 Thus every element $f\in W_d$ has the form $f=p(D).t^d$ for some
 $p(D)\in\R[D]$, so $W_d$ is generated by $t^d$ as a module over
 $\R[D]$.  

 Now put $I=\{p\in\R[D]\st p(D).t^d=0\}$, so $W_d\simeq\R[D]/I$.
 Suppose we have an element $p(D)=\sum_ib_iD^i\in\R[D]$.  Then
 $p(D).t^d=b_0t^d+b_1m_1t^{d-1}+\ldots+b_dm_dt^0$, and this is zero
 iff $b_0=\ldots=b_d=0$.  This means that $p(D)$ has the form
 $b_{d+1}D^{d+1}+b_{d+2}D^{d+2}+\ldots$, so it is divisible by
 $D^{d+1}$.  Thus $I$ is the principal ideal $\R[D].D^{d+1}$, and
 $W_d\simeq\R[D]/D^{d+1}$.
\end{solution}

%\ip{crossprod}
\begin{exercise}\exlabel{ex-crossprod}
 Fix a nonzero vector $u\in\R^3$, and write $r=\|u\|$.  Define an
 endomorphism $\phi\:\R^3\xra{}\R^3$ by $\phi(v)=u\tm v$.  Write $M$
 for $\R^3$, considered as a module over $\R[x]$ using $\phi$.  Let
 $L$ be the line through $u$ and $0$, and let $K$ be the plane
 perpendicular to $L$.
 \begin{itemize}
  \item[(a)] Show that $L$ is a submodule of $M$, and that $xL=0$.
  \item[(b)] Show that $K$ is a submodule of $M$, and that $(x^2+r^2)K=0$
   and $xM\leq K$.
  \item[(c)] Show that $(x^3 + r^2 x)M=0$.
 \end{itemize}
 (You will need a number of standard facts about dot and cross
 products of vectors.)
\end{exercise}
\begin{solution}
 \begin{itemize}
 \item[(a)] If $v\in L$ then $v=tu$ for some $t\in\R$, so
  $xv=\phi(v)=t(u\tm u)=0$ (using the fact that $a\tm a=0$ for any
  vector $a$).  This shows that $xL=\phi(L)=0$, so certainly
  $\phi(L)\leq L$, so $L$ is a submodule.
 \item[(b)] For any $v\in M$ we have $xv=\phi(v)=u\tm v$, which is
  always perpendicular to $u$, so it lies in $K$.  This says that
  $xM=\phi(M)\leq K$, so certainly $\phi(K)\leq K$, so $K$ is a
  submodule.  Next, for any $v\in K$ we have $u.v=0$ and so 
  \[ x^2v = \phi^2(v) = u\tm(u\tm v) = (u.v)u - (u.u)v =
      0u - r^2v = -r^2 v,
  \]
  so $(x^2 + r^2)v=0$.  This shows that $(x^2+r^2)K=0$ as claimed.
 \item[(c)] We now know that $xM\leq K$ so 
  \[ (x^3+r^2x)M = (x^2+r^2)xM \leq (x^2 + r^2) K = 0. \]
 \end{itemize}
\end{solution}


\section{Homomorphisms}
\label{sec-homomorphisms}

\begin{definition}\lbl{defn-homomorphism}
 Let $M$ and $N$ be modules over a ring $R$.  An \emph{$R$-module
 homomorphism} (or just \emph{homomorphism}) from $M$ to $N$ is a
 function $\al\:M\xra{}N$ such that
 \begin{itemize}
 \item[(a)] $\al(m_0+m_1)=\al(m_0)+\al(m_1)$ for all $m_0,m_1\in M$.
 \item[(b)] $\al(am)=a\al(m)$ for all $a\in R$ and $m\in M$.
 \end{itemize}
 Note that this implies that $\al(0)=\al(0.0)=0\al(0)=0$ and
 $\al(-m)=\al((-1).m)=(-1).\al(m)=-\al(m)$.

 An \emph{isomorphism} is a homomorphism which is also a bijection.  
\end{definition}
\begin{remark}\lbl{rem-inv-homo}
 Let $\al\:M\xra{}N$ be an isomorphism.  As $\al$ is a bijection,
 there is an inverse function $\al^{-1}\:N\xra{}M$ such that
 $\al(\al^{-1}(n))=n$ for all $n\in N$ and $\al^{-1}(\al(m))=m$ for
 all $m\in M$.  I claim that $\al^{-1}$ is also a homomorphism.  To
 see this, suppose that $n_0,n_1\in N$.  We then have elements
 $\al^{-1}(n_0)$ and $\al^{-1}(n_1)$ in $M$.  As $\al$ is a
 homomorphism, we have $\al(\al^{-1}(n_0)+\al^{-1}(n_1))=
 \al(\al^{-1}(n_0))+\al(\al^{-1}(n_1))=n_0+n_1$.  We can apply
 $\al^{-1}$ to this equation to get
 $\al^{-1}(\al(\al^{-1}(n_0))+\al(\al^{-1}(n_1)))=\al^{-1}(n_0+n_1)$.
 Because $\al^{-1}(\al(m))=m$ for all $m$, the left hand side is just
 $\al^{-1}(n_0)+\al^{-1}(n_1)$.  We thus have
 $\al^{-1}(n_0)+\al^{-1}(n_1)=\al^{-1}(n_0+n_1)$, showing that
 $\al^{-1}$ respects addition.

 Similarly, suppose that $n\in N$ and $a\in R$.  As $\al$ respects
 multiplication by $R$, we have
 $\al(a\al^{-1}(n))=a\al(\al^{-1}(n))=an$.  By applying $\al^{-1}$ to
 this equation we get $\al^{-1}(\al(a\al^{-1}(n)))=\al^{-1}(an)$.  The
 left hand side is just $a\al^{-1}(n)$, so we have
 $a\al^{-1}(n)=\al^{-1}(an)$, completing the proof that $\al^{-1}$ is
 a homomorphism.
\end{remark}

\begin{example}\lbl{eg-tau-sg-dl}
 Let $R$ be any ring.  Define $\tau\:R^2\xra{}R^2$, $\sg\:R^3\xra{}R$
 and $\dl\:R^2\xra{}R^3$ by
 \begin{align*}
  \tau(u,v) &= (v,u) \\
  \sg(x,y,z) &= x+y+z \\
  \dl(u,v) &= (u,v-u,-v).
 \end{align*}
 It is easy to check that these are all homomorphisms.  For example,
 we have
 \begin{align*}
  \dl(u_0,v_0) + \dl(u_1,v_1) &= 
   (u_0,v_0-u_0,-v_0) + (u_1,v_1-u_1,-v_1) \\
   &= (u_0+u_1,v_0+v_1 - u_0 - u_1, -v_0-v_1) \\
   &= \dl(u_0+u_1,v_0+v_1) \\
   &= \dl((u_0,v_0) + (u_1,v_1))
 \end{align*}
 and
 \begin{align*}
  a \dl(u,v) &= a.(u,v-u,-v) \\
             &= (au,av-au,-av) \\
             &= \dl(au,av) \\
             &= \dl(a.(u,v)),
 \end{align*}
 so $\dl$ is a homomorphism.
\end{example}
\begin{example}\lbl{eg-bad-defn}
 I would like to define two homomorphisms $\al,\bt\:\Z_3\xra{}\Z_{12}$
 by $\al(\ov{m})=\ov{4m}$ and $\bt(\ov{m})=\ov{5m}$.  There is a
 potential problem with this kind of definition, which means that the
 definition of $\bt$ is actually invalid, although it turns out that
 $\al$ is OK.  Consider the element $x=\ov{1}\in\Z_3$, which can also
 be described as $x=\ov{4}$.  Using the description $x=\ov{1}$ we get
 $\bt(x)=\ov{5}\in\Z_{12}$.  Using the description $x=\ov{4}$ we get
 $\bt(x)=\ov{20}\in\Z_{12}$.  As $20\neq 5\pmod{12}$, the elements
 $\ov{5}$ and $\ov{20}$ in $\Z_{12}$ are not the same, so our
 definition of $\bt$ is not self-consistent.

 However, this problem does not occur with $\al$.  To see why, suppose
 we describe an element $y\in\Z_3$ in two different ways, say
 $y=\ov{n}=\ov{m}$.  As $\ov{n}=\ov{m}$ in $\Z_3$, we have
 $n=m\pmod{3}$, so $n=m+3k$ for some integer $k$.  This means that
 $4n=4m+12k$, so $\ov{4n}=\ov{4m}$ in $\Z_{12}$.  This means that we
 get the same answer for $\al(y)$ no matter which description we use,
 so $\al$ is a well-defined function from $\Z_3$ to $\Z_{12}$.  

 We also have
 $\al(\ov{n})+\al(\ov{m})=\ov{4n}+\ov{4m}=\ov{4(n+m)}=\al(\ov{n}+\ov{m})$,
 so $\al$ is a homomorphism of groups.  It follows easily that it is
 also a homomorphism of $\Z$-modules.
\end{example}
The above example generalizes as follows:
\begin{proposition}\lbl{prop-cyclic-hom}
 Let $p$, $q$ and $r$ be integers such that $p,q>0$ and $pr$ is
 divisible by $q$.  Then there is a unique homomorphism
 $\al\:\Z_p\xra{}\Z_q$ such that $\al(\ov{m})=\ov{rm}$ for all
 $m\in\Z$.  
\end{proposition}
\begin{proof}
 By assumption we have $pr=qs$ for some integer $s$.  If
 $\ov{n}=\ov{m}$ in $\Z_p$ then $m=n+pk$ for some $k$, so
 $rm=rn+rpk=rn+qsk$, so $rm=rn\pmod{q}$, so $\ov{rm}=\ov{rn}$ in
 $\Z_q$.  This shows that $\al$ is well-defined.  We also have 
 $\al(\ov{n})+\al(\ov{m})=\ov{rn}+\ov{rm}=
  \ov{r(n+m)}=\al(\ov{n}+\ov{m})$, so $\al$ is a homomorphism.
\end{proof}
\begin{example}\lbl{eg-cyclic-hom}
 Because $6\tm 5$ is divisible by $15$, there is a unique homomorphism
 $\al\:\Z_6\xra{}\Z_{15}$ such that $\al(\ov{m})=\ov{5m}$ for all $m$.
\end{example}

\begin{example}\lbl{eg-free-hom}
 Let $N$ be a module over a ring $R$, and let $n_1,\ldots,n_d$ be a
 list of elements of $N$.  We define a function $\al\:R^d\xra{}N$ by 
 \[ \al(u_1,\ldots,u_d) = u_1n_1 + \ldots+u_d n_d. \]
 I claim that this is a homomorphism.  Indeed, we have
 \begin{align*}
  \al((u_1,\ldots,u_d)+(v_1,\ldots,v_d)) &= 
   \al(u_1+v_1,\ldots,u_d+v_d) \\
   &= (u_1+v_1)n_1 + \ldots + (u_d+v_d)n_d \\
   &= (u_1n_1+\ldots+u_dn_d) + (v_1n_1+\ldots+v_dn_d) \\
   &= \al(u_1,\ldots,u_d) + \al(v_1,\ldots,v_d), 
 \end{align*}
 and
 \begin{align*}
  \al(a(u_1,\ldots,u_d)) &= \al(au_1,\ldots,au_d) \\
   &= au_1n_1 + \ldots + au_dn_d \\
   &= a(u_1n_1+\ldots+u_dn_d) \\
   &= a\al(u_1,\ldots,u_d)
 \end{align*}
 as required.

 Next, I claim that every homomorphism $\bt\:R^d\xra{}N$ is of the
 form just described.  To see this, let $e_k$ be the element of $R^d$
 given by $e_k=(0,\ldots,0,1,0,\ldots,0)$, with the $1$ in the $k$'th
 place.  Put $n_k=\bt(e_k)\in N$.  Any element $u=(u_1,\ldots,u_d)$ in
 $R^d$ can be written as $u=u_1e_1+\ldots+u_de_d$.  For example, in
 the case $d=3$ we have 
 \[ (u_1,u_2,u_3) = u_1(1,0,0) + u_2(0,1,0) + u_3(0,0,1) = 
      u_1e_1 + u_2e_2 + u_3e_3.
 \]
 As $\bt$ is a homomorphism, we have
 \begin{align*}
  \bt(u_1,\ldots,u_d) &= \bt(u_1e_1+\ldots+u_de_d) \\
                      &= u_1\bt(e_1) + \ldots + u_d\bt(e_d) \\
                      &= u_1 n_1 + \ldots + u_d n_d,
 \end{align*}
 so $\bt$ has the form described previously.
\end{example}
We summarize the above discussion as follows:
\begin{proposition}\lbl{prop-free-hom}
 For any list $(n_1,\ldots,n_d)$ of elements of $N$ we have a
 homomorphism $\al\:R^d\xra{}N$ given by $\al(u)=\sum_iu_in_i$.
 Conversely, every homomorphism $\al\:R^d\xra{}N$ arises in this way
 from a unique list $(n_1,\ldots,n_d)$. \qed
\end{proposition}

\begin{example}\lbl{eg-matrix}
 We now consider homomorphisms $\al\:R^d\xra{}R^e$.  By the previous
 example, $\al$ corresponds to a list $u_1,\ldots,u_d$, where each
 $u_i$ is an element of $R^e$, or in other words a vector of length
 $e$.  We can construct a matrix $A$ (with $d$ columns and $e$ rows) 
 whose columns are the vectors $u_1,\ldots,u_d$, and we find that
 $\al(x)=Ax$ for all $x\in R^d$.

 Consider for example the homomorphism $\dl\:R^2\xra{}R^3$ given by
 $\dl(s,t)=(s,t-s,-t)$.  We have
 \begin{align*}
  u_1 &= \dl(e_1) = \dl(1,0) = (1,-1,0) \\
  u_2 &= \dl(e_2) = \dl(0,1) = (0,1,-1) \\
  A &= \left(\begin{array}{cc} 1&0\\-1&1\\0&-1 \end{array}\right)
 \end{align*}
 so
 \[ A\bbm s \\ t\ebm = 
     \bbm 1&0\\-1&1\\0&-1 \ebm \bbm s\\ t\ebm =
     \bbm s \\ t-s \\ -t \ebm = \al(s,t) 
 \]
 as claimed.
\end{example}
We summarize the above discussion as follows:
\begin{proposition}\lbl{prop-matrix}
 Homomorphisms from $R^d$ to $R^e$ are essentially the same as
 $d\tm e$ matrices over $R$. \qed
\end{proposition}

We next consider homomorphisms of modules over polynomial rings.
\begin{proposition}\lbl{prop-Kx-hom}
 Let $V$ and $W$ be vector spaces over a field $K$, and let
 $\phi\:V\xra{}V$ and $\psi\:W\xra{}W$ be $K$-linear maps.  We use
 these to make $V$ and $W$ into modules over $K[x]$ in the usual way,
 so that $xv=\phi(v)$ for $v\in V$ and $xw=\psi(w)$ for $w\in W$.
 Then the $K[x]$-module homomorphisms from $V$ to $W$ are precisely
 the $K$-linear maps $\gm\:V\xra{}W$ such that $\psi\gm=\gm\phi$, or
 in other words $\psi(\gm(v))=\gm(\phi(v))$ for all $v\in V$.
\end{proposition}
\begin{proof}
 Let $\gm\:V\xra{}W$ be a $K[x]$-module homomorphism, so
 $\gm(v_0+v_1)=\gm(v_0)+\gm(v_1)$ for all $v_0,v_1\in V$, and
 $\gm(av)=a\gm(v)$ for all $a\in K[x]$ and $v\in V$.  By taking $a$ to
 be a constant polynomial, we see that $\gm(av)=a\gm(v)$ for all
 $a\in K$, so $\gm$ is a $K$-linear map.  Now take $a=x$ instead.  As
 $v\in V$ we have $xv=\phi(v)$, and as $\gm(v)\in W$ we have
 $x\gm(v)=\psi(\gm(v))$.  Thus, the equation $\gm(xv)=x\gm(v)$ becomes
 $\gm(\phi(v))=\psi(\gm(v))$, as required.

 Conversely, suppose we have a $K$-linear map $\gm\:V\xra{}W$
 satisfying $\gm\phi=\psi\gm$.  It follows that 
 \[ \gm\phi^2=(\gm\phi)\phi=(\psi\gm)\phi=\psi(\gm\phi)=\psi^2\gm. \]
 This can be extended by induction to show that $\gm\phi^k=\psi^k\gm$
 for all $k\geq 0$.  Thus, for any polynomial $p(x)=\sum_ia_ix^i$ we
 have 
 \[ \gm p(\phi)=\sum_i a_i \gm\phi^i=\sum_ia_i\psi^i\gm=p(\psi)\gm, \]
 so $\gm(p(x)v)=p(x)\gm(v)$, so $\gm$ is a $K[x]$-module homomorphism.
\end{proof}
\begin{remark}\lbl{rem-diagram}
 We can indicate where the maps $\gm$, $\phi$ and $\psi$ go by a
 diagram as follows:
 \[ \xymatrix{
  V \dto_{\phi} \arrow{e,t}\rto^{\gm} &
  W \dto^{\psi} \\
  V \rto_{\gm} & W.
 } \]
 The condition $\psi\gm=\gm\phi$ means that the two way round the
 square are the same.  This is usually expressed by saying that the
 diagram \emph{commutes}.
\end{remark}
\begin{remark}\lbl{rem-Hom-MA-MB}
 The proposition can be restated as follows in terms of matrices: If
 $A$ and $B$ are square matrices of size $n$ and $m$ over a field $K$,
 then the $K[x]$-module homomorphisms from $M_A$ to $M_B$ are the
 $n\tm m$ matrices $C$ over $K$ such that $CA=BC$.  Here the matrices
 $A$, $B$ and $C$ correspond to the linear maps $\phi$, $\psi$ and
 $\gm$ respectively.
\end{remark}
\begin{remark}\lbl{rem-usually-zero}
 If matrices $A$ and $B$ are chosen randomly, then usually the only
 matrix $C$ satisfying $CA=BC$ will be the zero matrix.  Of course,
 most of the examples in these notes are chosen specially so that
 there are some nonzero solutions.
\end{remark}

\begin{example}\lbl{eg-Hom-MA-MB}
 Put $A=\bsm 1&1\\1&1\esm$ and $B=\bsm 0&2\\2&0\esm$.  The
 homomorphisms from $M_A$ to $M_B$ are then the matrices
 $C=\bsm a&b\\c&d\esm$ for which $CA=BC$, or in other words 
 \[ \bbm a+b & a+b \\ c+d & c+d\ebm = 
    \bbm 2c & 2d \\ 2a & 2b \ebm,
 \]
 or equivalently
 \begin{align*}
  a+b &= 2c \\
  a+b &= 2d \\
  c+d &= 2a \\
  c+d &= 2b.
 \end{align*}
 Solving these equations gives $a=b=c=d$.  Thus, the homomorphisms
 from $M_A$ to $M_B$ are precisely the matrices of the form
 $\bsm a&a\\a&a\esm$ for some $a\in K$.
\end{example}
\begin{example}
 Define $\phi\:\R^2\xra{}\R^2$ by $\phi(u,v)=(v,u)$, and define
 $\psi\:\R^3\xra{}\R^3$ by $\psi(u,v,w)=(v,w,u)$.  We use these to
 make $\R^2$ and $\R^3$ into modules over $\R[x]$ as usual.  Define
 $\gm\:\R^2\xra{}\R^3$ by $\gm(u,v)=(u+v,u+v,u+v)/2$.  I claim that
 this is an $\R[x]$-module homomorphism.  It is clearly $\R$-linear,
 so it is enough to check that $\gm\phi=\psi\gm$.  We have
 $\gm\phi(u,v)=\gm(v,u)=(v+u,v+u,v+u)/2$.  We also have
 $\gm(u,v)=(u+v,u+v,u+v)/2$ and $\psi(w,w,w)=(w,w,w)$ for any $w$, so
 $\psi\gm(u,v)=(u+v,u+v,u+v)/2$, which is the same as $\gm\phi(u,v)$,
 as claimed.

 Next, I claim that any other $\R[x]$-module homomorphism
 $\bt\:\R^2\xra{}\R^3$ is actually a multiple of $\gm$.  To see this,
 note that $\bt(1,1)\in\R^3$, so $\bt(1,1)=(\lm,\mu,\nu)$ for some
 $\lm,\mu,\nu\in\R$.  As $\bt$ is a homomorphism we must have
 $\bt\phi(1,1)=\psi\bt(1,1)=\psi(\lm,\mu,\nu)=(\mu,\nu,\lm)$.  On the
 other hand, we have $\phi(1,1)=(1,1)$ so
 $\bt\phi(1,1)=\bt(1,1)=(\lm,\mu,\nu)$.  Thus
 $(\lm,\mu,\nu)=(\mu,\nu,\lm)$, so $\nu=\mu=\lm$.  This means that
 $\bt(1,1)=(\lm,\lm,\lm)=\lm.(1,1,1)$. 
 
 We next claim that $\bt(1,-1)=(0,0,0)$.  To see this, note that
 $\bt\phi^3=\psi^3\bt$.  Moreover $\psi^3(u,v,w)=(u,v,w)$ for all
 $(u,v,w)\in\R^3$, and $\phi^3(1,-1)=(-1,1)=-(1,-1)$, so the equation
 $\psi^3\bt(1,-1)=\bt\phi^3(1,-1)$ becomes
 $\bt(1,-1)=\bt(-1,1)=-\bt(1,-1)$.  By rearranging we see that
 $\bt(1,-1)=0$ as claimed.

 Now consider an arbitrary element $(u,v)\in\R^2$.  We can write this
 as 
 \[ (u,v) = \frac{u+v}{2} (1,1) + \frac{u-v}{2} (1,-1), \]
 so 
 \begin{align*}
  \bt(u,v) &= \frac{u+v}{2} \bt(1,1) + \frac{u-v}{2} \bt(1,-1) \\
           &= \frac{u+v}{2} (\lm,\lm,\lm) + \frac{u-v}{2}(0,0,0) \\
           &= \lm\gm(u,v).
 \end{align*}
 This proves that $\bt=\lm\gm$, as claimed.
\end{example}
\begin{example}\lbl{eg-shift}
 Define $\tau\:\CRR\xra{}\CRR$ by $\tau(f)(t)=f(t+1)$.  Thus if
 $g(t)=3t$ and $h(t)=\sin(2\pi t)$ and $k(t)=2^t$ then $\tau(g)=g+3$
 and $\tau(h)=h$ and $\tau(k)=2k$.  It is clear that $\tau$ is an
 $\R$-linear map.  By the chain rule we have
 \[ \frac{d}{dt} f(t+1) = f'(t+1) \frac{d}{dt}(t+1) = f'(t+1), \]
 so $\partial(\tau(f))=\tau(\partial(f))$, so $\tau$ is a homomorphism
 of $\R[D]$-modules.
\end{example}

\begin{proposition}\lbl{prop-conj-iso}
 Let $A$ and $B$ be $n\tm n$ matrices over a field $K$.  Then $M_A$ is
 isomorphic to $M_B$ if and only if $A$ is conjugate to $B$, in other
 words there is an invertible $n\tm n$ matrix $P$ such that
 $PAP^{-1}=B$.
\end{proposition}
\begin{proof}
 A homomorphism from $M_A$ to $M_B$ is an $n\tm n$ matrix $P$ such
 that $PA=BP$.  Such a homomorphism is an isomorphism if and only if
 $P$ is invertible, and if so, the condition $PA=BP$ is equivalent to
 the condition $PAP^{-1}=B$.
\end{proof}
\begin{corollary}\lbl{cor-semisimple}
 Suppose that $A$ is a diagonalizable $n\tm n$ matrix over $K$.  Then
 there exists an invertible matrix $P$ such that $D:=PAP^{-1}$ is a
 diagonal matrix, with diagonal entries $\lm_1,\ldots,\lm_n$ say.
 Then $M_A$ is isomorphic to $M_D$ and thus to
 $M_{\lm_1}\op\ldots\op M_{\lm_n}$. \qed
\end{corollary}

\begin{definition}\lbl{defn-ker-img}
 Let $\al\:M\xra{}N$ be a homomorphism of modules over a ring $R$.  We
 define the kernel and image of $\al$ by 
 \begin{align*}
  \ker(\al) &= \{ m\in M \st \al(m) = 0 \} \\
  \img(\al) &= \{ n\in N \st n=\al(m) \text{ for some } m\in M\}.
 \end{align*}
\end{definition}
\begin{proposition}\lbl{prop-img-submodule}
 $\ker(\al)$ is a submodule of $M$ and $\img(\al)$ is a submodule of
 $N$. 
\end{proposition}
\begin{proof}
 Suppose that $m_0,m_1\in\ker(\al)$ and that $a\in R$.  We then have
 $\al(m_0)=\al(m_1)=0$ and so
 \begin{align*}
  \al(m_0+m_1) &= \al(m_0) + \al(m_1) = 0+0 = 0 \\
  \al(am_0) &= a\al(m_0) = a.0 = 0,
 \end{align*}
 so $m_0+m_1\in\ker(\al)$ and $am_0\in\ker(\al)$.  This shows that
 $\ker(\al)$ is a submodule of $M$.

 Now suppose that $n_0,n_1\in\img(\al)$ and $a\in R$.  We then have
 $n_0=\al(m_0)$ for some $m_0\in M$ and $n_1=\al(m_1)$ for some
 $m_1\in M$.  It follows that $\al(m_0+m_1)=n_0+n_1$, so $n_0+n_1$ is
 $\al(\text{something})$, so $n_0+n_1\in\img(\al)$.  Similarly, we
 have $an_0=\al(am_0)$, so $an_0\in\img(\al)$.  This shows that
 $\img(\al)$ is a submodule of $N$.
\end{proof}
\begin{example}\lbl{eg-times-four}
 Define $\al\:\Z\xra{}\Z_{12}$ by $\al(n)=\ov{4n}$.  We then have 
 \[ \begin{array}{rlcrlcrl}
  \al(0) &= \ov{0} & \hspace{5em} &
  \al(1) &= \ov{4} & \hspace{5em} &
  \al(2) &= \ov{8} \\
  \al(3) &= \ov{12} = \ov{0} & \hspace{5em} &
  \al(4) &= \ov{16} = \ov{4} & \hspace{5em} &
  \al(5) &= \ov{20} = \ov{8} \\
  \al(6) &= \ov{24} = \ov{0} & \hspace{5em} &
  \al(7) &= \ov{28} = \ov{4} & \hspace{5em} &
  \al(8) &= \ov{32} = \ov{8}
 \end{array} \]
 and everything repeats with period three.  It follows that the only
 elements of $\Z_{12}$ that can be written in the form $\al(n)$ are
 $\ov{0}$, $\ov{4}$ and $\ov{8}$, so
 $\img(\al)=\{\ov{0},\ov{4},\ov{8}\}$.  It also follows that
 $\al(n)=\ov{0}$ if and only if $n$ is divisible by $3$, so
 $\ker(\al)=\{n\in\Z\st n=0\pmod{3}\}=3\Z$. 
\end{example}
\begin{example}\lbl{eg-sum-hom}
 Define $\sg\:R^3\xra{}R$ by $\sg(x,y,z)=x+y+z$ and
 $\dl\:R^2\xra{}R^3$ by $\dl(u,v)=(u,v-u,-v)$.  I claim that
 $\img(\dl)=\ker(\sg)$.  

 To see this, first suppose that $(x,y,z)\in\img(\dl)$.  This means
 that $(x,y,z)=\dl(u,v)=(u,v-u,-v)$ for some $u,v$.  It follows that
 $\sg(x,y,z)=x+y+z=u+(v-u)+(-v)=0$, so $(x,y,z)\in\ker(\sg)$.  This
 proves that $\img(\dl)\sse\ker(\sg)$.

 Conversely, suppose that $(x,y,z)\in\ker(\sg)$.  This means that
 $x+y+z=0$, so $-(x+y)=z$.  From this it follows that
 $(x,y,z)=(x,(x+y)-x,-(x+y))=\dl(x,x+y)$, so $(x,y,z)$ is
 $\dl(\text{something})$, so $(x,y,z)\in\img(\dl)$.  This proves that
 $\ker(\sg)\sse\img(\dl)$ and thus that $\ker(\sg)=\img(\dl)$, as
 claimed. 
\end{example}
\begin{remark}\lbl{rem-exact-seq}
 Suppose we have modules $L,M,N$ and homomorphisms $\al\:L\xra{}M$ and
 $\bt\:M\xra{}N$ such that $\img(\al)=\ker(\bt)$.  We then say that
 the sequence $L\xra{\al}M\xra{\bt}N$ is \emph{exact}; for example,
 the sequence $R^2\xra{\dl}R^3\xra{\sg}R$ in the above example is
 exact.  This is a very important concept elsewhere in the theory of
 modules, although we will make little use of it in this course.
\end{remark}

\begin{proposition}\lbl{prop-jective}
 Let $\al\:M\xra{}N$ be a homomorphism of $R$-modules.  Then
 \begin{itemize}
 \item[(a)] $\al$ is injective if and only if $\ker(\al)=\{0\}$.
 \item[(b)] $\al$ is surjective if and only if $\img(\al)=N$.
 \item[(c)] $\al$ is an isomorphism if and only if $\ker(\al)=\{0\}$
  and $\img(\al)=N$.
 \end{itemize}
\end{proposition}
\begin{proof}
 First, suppose that $\ker(\al)=\{0\}$.  If $\al(m_0)=\al(m_1)$ then
 $\al(m_0-m_1)=\al(m_0)-\al(m_1)=0$, so $m_0-m_1\in\ker(\al)=\{0\}$,
 so $m_0-m_1=0$, so $m_0=m_1$.  This proves that $\al$ is injective.

 Conversely, suppose that $\al$ is injective.  If $m\in\ker(\al)$ then
 $\al(m)=0$, so $\al(m)=\al(0)$, and as $\al$ is injective this means
 that $m=0$.  Thus $\ker(\al)=\{0\}$.  This completes the proof
 of~(a). 

 For~(b), note that $\img(\al)$ is the set of things in $N$ that can
 be written in the form $\al(m)$ for some $m$.  Thus $\img(\al)=M$ if
 and only if \emph{every} element in $N$ can be written as $\al(m)$
 for some $m$, and this is precisely the definition of surjectivity.

 Finally, an isomorphism is just a bijective homomorphism.  It is
 standard that a function is a bijection if and only if it is both
 injective and surjective, so~(c) follows immediately from~(a)
 and~(b).  
\end{proof}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{HomKx0}
\begin{exercise}\exlabel{ex-Hom-MA-MB-i}
 In each of the following situations, find all the $\Q[x]$-module
 homomorphisms from $M_A$ to $M_B$.
 \begin{itemize}
  \item[(a)] $A=\bsm 1&1\\0&1\esm$ and $B=\bsm 0&1\\1&0\esm$.
  \item[(b)] $A=\bsm \lm&0\\0&\mu\esm$ and $B=\bsm\mu&0\\0&\lm\esm$,
   where $\lm\neq\mu$.
  \item[(c)] $A=I_2$ (the $2\tm 2$ identity matrix) and $B=I_3$.
 \end{itemize} 
\end{exercise}
\begin{solution}
 Such homomorphisms correspond to matrices $P$ of the appropriate
 shape (the same number of columns as $A$, and the same number of rows
 as $B$) such that $PA=BP$.  
 \begin{itemize}
  \item[(a)] Here $P$ is a $2\tm 2$ matrix, say $P=\bsm a&b\\c&d\esm$.
   We have $PA=\bsm a&a+b\\c&c+d\esm$ and $BP=\bsm c&d\\a&b\esm$.  Thus
   $PA=BP$ if and only if $a=c$, $a+b=d$, $c=a$ and $c+d=b$.  By
   solving these equations we find that $c=a=0$ and $d=b$.  Thus, the
   homomorphisms from $M_A$ to $M_B$ are precisely the matrices of the
   form $P=\bsm 0&b\\0&b\esm$ over $\Q$.
  \item[(b)] Here again we have $P=\bsm a&b\\c&d\esm$ for some
   $a,b,c,d$.  Thus $PA=\bsm \lm a&\mu b\\ \lm c&\mu d\esm$ and
   $BP=\bsm\mu a&\mu b\\ \lm c&\lm d\esm$, so $PA=BP$ if and only if
   $\lm a=\mu a$ and $\mu d=\lm d$, or in other words
   $(\lm-\mu)a=(\lm-\mu)d=0$.  As $\lm-\mu\neq 0$ this means that
   $a=d=0$.  Thus, the homomorphisms from $M_A$ to $M_B$ are precisely
   the matrices of the form $P=\bsm 0&b\\ c&0\esm$.
  \item[(c)] Here $P$ is a $2\tm 3$ matrix over $\Q$.  We have
   $PA=PI_2=P$ and $BP=I_3P=P$ so the condition $PA=BP$ is
   automatically satisfied.  Thus the homomorphisms from $M_A$ to
   $M_B$ are all the $2\tm 3$ matrices over $\Q$.  
 \end{itemize}
\end{solution}

%\ip{HomKx1}
\begin{exercise}\exlabel{ex-Hom-MA-MB-ii}
 \begin{itemize}
  \item[(a)] Put $A=\bsm 1&2\\0&2\esm$ and $B=\bsm 1&1\\1&1\esm$.
   Find all the $\Q[x]$-module homomorphisms from $M_A$ to $M_B$.
  \item[(b)] Put $A=\bsm 1&0&0\\1&1&0\\0&1&1\esm$ and
   $B=\bsm 1&0&0\\0&1&0\\0&0&1\esm$.  Find all the $\Q[x]$-module
   homomorphisms from $M_A$ to $M_B$. 
  \item[(c)] Suppose that $\lm_1,\lm_2,\lm_3,\mu_1,\mu_2\in\C$, and
   put $A=\bsm\lm_1&0&0\\0&\lm_2&0\\0&0&\lm_3\esm$ and
   $B=\bsm\mu_1&0\\0&\mu_2\esm$.  Show that for most values of the
   $\lm$'s and $\mu$'s, the only $\C[x]$-module homomorphism from
   $M_A$ to $M_B$ is zero.  What can you say about the exceptional
   cases?
 \end{itemize} 
\end{exercise}
\begin{solution}
 \begin{itemize}
 \item[(a)] The homomorphisms correspond to matrices
  $C=\bsm a&b\\c&d\esm$ with $CA=BC$, or equivalently
  $\bsm a&2a+2b\\c&2c+2d\esm=\bsm a+c&b+d\\a+c&b+d\esm$, so
  \begin{align*}
   a &= a+c \\
   2a+2b &= b+d \\
   c &= a+c \\
   2c+2d &= b+d.
  \end{align*}
  The first and third equations give $a=c=0$, and the remaining
  equations then give $d=b$, so $C$ must have the form
  $\bsm 0&b\\0&b\esm$.
 \item[(b)] We need to find the $3\tm 3$ matrices with $CA=BC$.  Note
  that $BC=C$.  Put $D=A-I=\bsm 0&0&0\\1&0&0\\0&1&0\esm$.  The
  condition $CA=BC$ now becomes $C(I+D)=C$, or equivalently $CD=0$.
  If the columns of $C$ are $u$, $v$ and $w$, then the columns of $CD$
  are easily seen to be $v$, $w$ and $0$.  Thus $CD=0$ iff $v=w=0$, so
  all the nonzero entries of $C$ must be in the first column.  Thus
  the homomorphisms from $M_A$ to $M_B$ are the matrices of the form
  $\bsm a&0&0\\b&0&0\\c&0&0\esm$.
 \item[(c)] Here we need the matrices
  $C=\bbm c_{11}&c_{21}&c_{31}\\c_{12}&c_{22}&c_{32}\ebm$ such that
  \[ \bbm c_{11}&c_{21}&c_{31}\\c_{12}&c_{22}&c_{32}\ebm
     \bbm \lm_1&0&0\\0&\lm_2&0\\0&0&\lm_3\ebm =
     \bbm \mu_1&0 \\ 0&\mu_2 \ebm
     \bbm c_{11}&c_{21}&c_{31}\\c_{12}&c_{22}&c_{32}\ebm,
  \]
  or equivalently
  \[ \bbm (\lm_1-\mu_1)c_{11} &
          (\lm_2-\mu_1)c_{21} &
          (\lm_3-\mu_1)c_{31} \\
          (\lm_1-\mu_2)c_{11} &
          (\lm_2-\mu_2)c_{21} &
          (\lm_3-\mu_2)c_{31} \ebm = 
     \bbm 0&0&0 \\ 0&0&0 \ebm.
  \]
  For most choices of numbers $\lm_1,\lm_2,\lm_3,\mu_1,\mu_2$, the
  $\lm$'s will all be different from the $\mu$'s, so all the numbers
  $\lm_i-\mu_j$ will be nonzero.  In this case, the only possible
  matrix $C$ is the zero matrix.  In general, if we have $\lm_i=\mu_j$
  for some pairs $(i,j)$, then the corresponding entries $c_{ij}$ can
  be nonzero.  
 \end{itemize}
\end{solution}

%\ip{HomRI}
\begin{exercise}\exlabel{ex-annihilator}
 Let $a$ be an element of a ring $R$.  For any $R$-module $M$, put
 \[ \ann(a,M)=\{m\in M\st am=0\}. \]
 We will also write $R/a$ for the factor module $R/Ra$.

 \begin{itemize}
  \item[(a)] Find $\ann(4,\Z_{12})$.
  \item[(b)] Find $\ann(x-1,M_A)$, where $A$ is the matrix
   $\bsm 1&0\\1&1\esm$.
  \item[(c)] Let $\al\:R/a\xra{}M$ be a homomorphism.  Show that
   $\al(\ov{1})\in\ann(a,M)$.
  \item[(d)] Conversely, given $m\in\ann(a,M)$, show that there is a
   unique homomorphism $\al\:R/a\xra{}M$ such that $\al(\ov{1})=m$.
  \item[(e)] Describe all the $\R[D]$-module homomorphisms from
   $\R[D]/(D^2-1)$ to $\CRR$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)]
   $\ann(4,\Z_{12})=\{\ov{0},\ov{3},\ov{6},\ov{9}\}\simeq\Z_4$.
  \item[(b)]
   We have 
   \[ (x-1)(u,v) = (A-I)(u,v) = \bbm 0&0\\1&0 \ebm\bbm u\\ v\ebm = 
       \bbm 0 \\ u \ebm, 
   \] 
   so $(x-1)(u,v)=(0,0)$ iff $u=0$.  Thus
   $\ann(x-1,M_A)=\{(0,v)\st v\in\R\}$.
  \item[(c)] $a\al(\ov{1})=\al(a.\ov{1})=\al(\ov{a})=\al(\ov{0})=0$,
   because $\ov{a}=\ov{0}$ in $R/a$.
  \item[(d)] Suppose that $m\in\ann(a,M)$.  We can certainly define a
   homomorphism $\bt\:R\xra{}M$ by $\bt(x)=xm$.  As $am=0$ this satisfies
   $\bt(ya)=yam=0$, so $\bt(x)=0$ for $x\in Ra$.  By the first
   isomorphism theorem we get an induced map
   $\al=\ov{\bt}\:R/aR\xra{}M$ defined by $\al(\ov{x})=\bt(x)=xm$, and
   in particular $\al(\ov{1})=m$.
  \item[(e)] Note that $\ann(D^2-1,\CRR)$ is the space of solutions of
   the differential equation $f''=f$, or equivalently the space of
   functions of the form $f(t)=ue^t+ve^{-t}$ with $u,v\in\R$.  For
   each such function we get a homomorphism
   $\al\:\R[D]/(D^2-1)\xra{}\CRR$ given by
   \[ \al(\ov{a+bD}) = (a+bD)(ue^t+ve^{-t}) = 
       (a+b)ue^t + (a-b)ve^{-t}.
   \]
 \end{itemize}
\end{solution}

%\ip{Z9ext}
\begin{exercise}\exlabel{ex-Z-nine-SES}
 Show that there are homomorphisms $\al\:\Z_3\xra{}\Z_9$ and
 $\bt\:\Z_9\xra{}\Z_3$ given by $\al(\ov{n})=\ov{3n}$ and
 $\bt(\ov{m})=\ov{m}$.  Show that the sequence
 $\Z_3\xra{\al}\Z_9\xra{\bt}\Z_3$ is exact.
\end{exercise}
\begin{solution}
 Recall that there is a map $\phi\:\Z_p\xra{}\Z_q$ with
 $\phi(\ov{m})=\ov{rm}$ iff $rp$ is divisible by $q$.  This is
 satisfied when $r=p=3$ and $q=9$, so $\al$ exists.  It is also
 satisfied when $p=9$ and $r=1$ and $q=3$, so $\bt$ exists.

 The elements of $\Z_3$ are $\ov{0}$, $\ov{1}$ and $\ov{2}$.  We have
 $\al(\ov{0})=\ov{0}$, $\al(\ov{1})=\ov{3}$ $\al(\ov{2})=\ov{6}$, so
 the image of $\al$ is $\{\ov{0},\ov{3},\ov{6}\}$.  

 Next, the elements of $\Z_9$ are $\ov{0},\ldots,\ov{8}$.  We have
 $\bt(\ov{3})=\ov{3}$.  The $\ov{3}$ on the left hand side is
 interpreted as an element of $\Z_9$ and thus is nonzero, but the
 $\ov{3}$ on the right hand side is interpreted as an element of
 $\Z_3$ and thus is zero.  In other words, we have
 $\bt(\ov{3})=\ov{0}$.  Similarly, we have
 \begin{align*}
  \bt(\ov{0}) &= \bt(\ov{3}) = \bt(\ov{6}) = \ov{0} \\
  \bt(\ov{1}) &= \bt(\ov{4}) = \bt(\ov{7}) = \ov{1} \\
  \bt(\ov{2}) &= \bt(\ov{5}) = \bt(\ov{8}) = \ov{2}.
 \end{align*}
 Thus
 $\ker(\bt)=\{a\in\Z_9\st\bt(a)=\ov{0}\}=\{\ov{0},\ov{3},\ov{6}\}$.
 We have shown that $\ker(\bt)=\img(\al)$, so the sequence
 $\Z_3\xra{\al}\Z_9\xra{\bt}\Z_3$ is exact.
\end{solution}


%\ip{nohom}
\begin{exercise}\exlabel{ex-no-homs}
 \begin{itemize}
  \item[(a)] Let $\al$ be a $\C[x]$-module homomorphism from
   $\C[x]/(x^2-1)$ to $M_A$, where $A=\bsm 1&1&1\\1&1&1\\1&1&1\esm$.
   Prove that $\al=0$.
  \item[(b)] Let $V$ be the space of functions of the form
   $a\sin(t)+b\cos(t)$, and let $W$ be the space of functions of the
   form $a\sinh(t)+b\cosh(t)$.  Let $\bt\:V\xra{}W$ be an
   $\R[D]$-module homomorphism.  Show that $\bt=0$.
  \item[(c)] Let $\gm\:\Z_4\xra{}\Z^4$ be a homomorphism of
   $\Z$-modules.  Prove that $\gm=0$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] It is easy to see that $A^2=3A$, so for all $v\in M_A$ we
   have $(x^2-3x)v=0$.  In particular, for $u\in\C[x]/(x^2-1)$ we have
   $(x^2-3x)\al(u)=0$.  However, we also have $(x^2-1)u=0$ and so
   $(x^2-1)\al(u)=0$.  The polynomials $x^2-3x=x(x-3)$ and
   $x^2-1=(x-1)(x+1)$ are coprime, so there exist polynomials $p(x)$
   and $q(x)$ with $p(x)(x^2-3x)+q(x)(x^2-1)=1$.  It follows that
   \[ \al(u)=p(x)(x^2-3x)\al(u) + q(x)(x^2-1)\al(x)=0+0=0. \]
   As this holds for all $u\in\C[x]/(x^2-1)$, we have $\al=0$ as
   claimed.
  \item[(b)] As $\sin''=-\sin$ and $\cos''=-\cos$ we have
   $(D^2+1)V=0$.  As $\sinh''=\sinh$ and $\cosh''=\cosh$ we have
   $(D^2-1)W=0$.  The elements $D^2+1$ and $D^2-1$ are coprime in
   $\R[D]$, so $\bt=0$ by the same argument as in part~(a).  
  \item[(c)] We have $\gm(\ov{1})=(w,x,y,z)\in\Z^4$ for some
   $w,x,w,z\in\Z$.  As $4.\ov{1}=\ov{0}$ we see that
   $(4w,4x,4y,4z)=\gm(\ov{0})=(0,0,0,0)$, so $4w=4x=4y=4z=0$.  As $w$,
   $x$, $y$ and $z$ are just integers, this implies that $w=x=y=z=0$,
   so $\gm(\ov{1})=0$.  This in turn implies that
   $\gm(\ov{n})=n.\gm(\ov{1})=n.0=0$ for all $n$.
 \end{itemize} 
\end{solution}

%\ip{homcount}
\begin{exercise}\exlabel{ex-count-Hom}
 Let $R$ be a ring, and let $M$ be an $R$-module with only finitely
 many elements, say $|M|=m$.  How many homomorphisms are there from
 $R^d$ to $M$?
\end{exercise}
\begin{solution}
 A homomorphism from $R^d$ to $M$ corresponds to a list
 $(m_1,\ldots,m_d)$ of elements of $M$.  There are $m$ possible
 choices for each entry in the list, so there are $m^d$ possible
 lists, and thus $m^d$ different homomorphisms from $R^d$ to $M$.
\end{solution}

\section{Factor modules}
\label{sec-factor-modules}

Let $M$ be an $R$-module for some ring $R$, and let $N$ be a
submodule.  We next define the factor module $M/N$.

For any element $m\in M$ we define the set $m+N=\{m+n\st n\in N\}$,
which is a subset of $M$.  A \emph{coset} of $N$ in $M$ is a subset
$C\sse M$ that can be written in the form $C=m+N$ for some $m$.  We
write $M/N$ for the set of all such cosets.

\begin{example}\lbl{eg-three-cosets}
 Take $R=\Z$ and $M=\Z$ and 
 \begin{align*}
  N &= 3\Z = \{n\in\Z\st n=0\pmod{3}\} \\
    &= \{\ldots,-9,-6,-3,0,3,6,9,\ldots\}.
 \end{align*}
 Consider the following three sets:
 \begin{align*}
  A &= \{\ldots,-10,-7,-4,-1,2,5,8,\ldots\} \\
  B &= \{\ldots,-9,-6,-3,0,3,6,9,\ldots\} \\
  C &= \{\ldots,-8,-5,-2,1,4,7,10,\ldots\}.
 \end{align*}
 The set $A$ can be described as $-7+N$ or as $-1+N$ or as $26+N$, so
 it is a coset.  Similarly, $B$ can be described as $0+N$ or $999+N$
 and $C$ can be described as $-8+N$ or $1+N$, so $A$, $B$ and $C$ are
 all cosets.  In fact, they are the only cosets, so $\Z/N=\{A,B,C\}$.
\end{example}

\begin{proposition}\lbl{prop-cosets}
 Let $R$, $M$ and $N$ be as above, and suppose that $m_0,m_1\in M$.
 Then the following are equivalent
 \begin{itemize}
 \item[(a)] $m_0+N=m_1+N$
 \item[(b)] $m_0-m_1\in N$ 
 \item[(c)] $m_0\in m_1+N$
 \item[(d)] $m_1\in m_0+N$
 \item[(e)] $(m_0+N)\cap(m_1+N)$ is nonempty.
 \end{itemize}
\end{proposition}
\begin{proof}
 If $m_0\in m_1+N$ then $m_0=m_1+n$ for some $n\in N$ so
 $m_0-m_1=n\in N$.  Conversely, if $m_0-m_1\in N$ then the equation
 $m_0=m_1+(m_0-m_1)$ shows that $m_0\in m_1+N$, so statements~(b)
 and~(c) are equivalent.  Similarly, (b) and~(d) are equivalent.  

 If~(c) holds then $m_0$ lies in both $m_0+N$ and $m_1+N$, so
 $(m_0+N)\cap(m_1+N)\neq\emptyset$, so~(e) holds.  

 Conversely suppose that~(e) holds, so there is an element $x$ lying
 in both $m_0+N$ and $m_1+N$.  This means that $x=m_0+n_0$ for some
 $n_0\in N$ and $x=m_1+n_1$ for some $n_1\in N$, so
 $m_0=x-n_0=m_1+(n_1-n_0)$.  This shows that $m_0\in m_1+N$, so~(c)
 holds.  We now see that~(b), (c), (d) and (e) are all equivalent to
 each other.

 If (a) holds then it is clear that (c) holds and thus that (b),(d)
 and~(e) also hold.  Conversely, suppose that~(b) holds.  If
 $x\in m_0+N$ then $x=m_0+n$ for some $n\in N$, so
 $x=m_1+((m_0-m_1)+n)$ and $(m_0-m_1)+n\in N$ so $x\in m_1+N$.  This
 shows that $m_0+N\sse m_1+N$, and a similar argument shows that
 $m_1+N\sse m_0+N$, so~(a) holds.  This now shows that all five
 statements are equivalent to each other.
\end{proof}

We next want to define addition of cosets.  Given two cosets $C_0$ and
$C_1$ we choose $m_0,m_1\in M$ such that $C_0=m_0+N$ and $C_1=m_1+N$,
and then we would like to define $C_0+C_1=(m_0+m_1)+N$.  Similarly, if
$a\in R$ we would like to define $aC_0=(am_0)+N$.  There is a
potential problem here: suppose we chose a different description of
the same coset $C_0$ (say as $m'_0+N$) and a different description of
$C_1$ (say as $m'_1+N$).  This gives an apparently different answer
for $C_0+C_1$: before we had $(m_0+m_1)+N$, now we have
$(m'_0+m'_1)+N$.  If these were genuinely different cosets then our
definition of addition would be ambiguous and thus invalid.  However,
we will show that these are simply different descriptions of the same
coset, so our definition is unambiguous after all.
\begin{lemma}\lbl{lem-cosets}
 If $m_0+N=m'_0+N$ and $m_1+N=m'_1+N$ then $(m_0+m_1)+N=(m'_0+m'_1)+N$
 and $(am_0)+N=(am'_0)+N$.
\end{lemma}
\begin{proof}
 By Proposition~\ref{prop-cosets} we have $m_0-m'_0\in N$ and
 $m_1-m'_1\in N$.  As $N$ is closed under addition this means that
 $m_0-m'_0+m_1-m'_1\in N$, or in other words
 $(m_0+m_1)-(m'_0+m'_1)\in N$, so $(m_0+m_1)+N=(m'_0+m'_1)+N$ as
 claimed.  Similarly, as $m_0-m'_0\in N$ and $N$ is a submodule, we
 have $am_0-am'_0=a(m_0-m'_0)\in N$, so $(am_0)+N=(am'_0)+N$ as
 claimed.
\end{proof}
\begin{corollary}\lbl{cor-cosets}
 We can unambiguously define addition of cosets and multiplication of
 cosets by elements of $R$, using the formulae
 \begin{align*}
  (m_0+N) + (m_1+N) &= (m_0+m_1) + N \\
  a(m+N) &= (am) + N.
 \end{align*}
\end{corollary}
\begin{notation}\lbl{ntn-cosets}
 If there is no ambiguity about which submodule $N$ is intended, we
 will write $\ov{m}$ for $m+N$.
\end{notation}
\begin{proposition}\lbl{prop-factor-module}
 The definitions in Corollary~\ref{cor-cosets} make the set $M/N$ into
 an $R$-module.  Moreover, the function $\pi\:M\xra{}M/N$ defined by
 $\pi(m)=\ov{m}=m+N$ is an $R$-module homomorphism.
\end{proposition}
\begin{proof}
 We need to check all the axioms in Definition~\ref{defn-module}.  All
 the proofs follow the same pattern, so we will do only two of them.
 We first consider axiom~(d), which says that addition is
 associative.  Let $A,B,C$ be any three cosets; we must show that
 $A+(B+C)=(A+B)+C$.  We can choose $a,b,c\in M$ such that $A=a+N$,
 $B=b+N$ and $C=c+N$.  From the definition of addition in $M/N$ we
 have $B+C=(b+c)+N$ and thus $A+(B+C)=(a+(b+c))+N$.  Similarly, we
 have $(A+B)+C=((a+b)+c)+N$.  As addition is associative in the module
 $M$ that we started with, we have $(a+b)+c=a+(b+c)$ so
 $((a+b)+c)+N=(a+(b+c))+N$, so $(A+B)+C=A+(B+C)$ as required.

 We next check axiom~(j) (which says that multiplication is
 right-distributive).  Let $A$ and $B$ be elements of $M/N$, and let
 $r$ be an element of $R$; we must show that $r(A+B)=rA+rB$.  Choose
 $a$ and $b$ such that $A=a+N$ and $B=b+N$.  We then have
 $A+B=(a+b)+N$ so
 $r(A+B)=r((a+b)+N)=(r(a+b))+N=(ra+rb)+N=
  (ra+N)+(rb+N)=r(a+N)+r(b+N)=rA+rB$, as required.

 The very definition of addition and multiplication in $M/N$ says that
 $\pi(m_0)+\pi(m_1)=\pi(m_0+m_1)$ and $a\pi(m)=\pi(am)$, so $\pi$ is
 an $R$-module homomorphism.
\end{proof}

\begin{theorem}[The first isomorphism theorem]
 Let $\al\:M\xra{}N$ be a homomorphism of $R$-modules.  Put
 $K=\ker(\al)\sse M$ and $L=\img(\al)\sse N$.  Then there is an
 isomorphism $\alb\:M/K\xra{}L$ such that $\alb(m+K)=\al(m)$
 for all $m\in M$.
\end{theorem}
\begin{proof}
 Let $C$ be a coset in $M/K$.  We can choose an element $m\in M$ such
 that $C=m+K$, and clearly $\al(m)\in\img(\al)=L$.  We would like to
 define $\alb(C)=\al(m)$, but we need to check that this is
 well-defined.  Suppose we describe the same coset $C$ in a different
 way, say as $C=m'+K$.  This gives an apparently different answer for
 $\alb(C)$: before we had $\al(m)$, now we have $\al(m')$.  As
 $m'+K=m+K$ we have $m'-m\in K=\ker(\al)$, which means that
 $\al(m')-\al(m)=\al(m'-m)=0$, so $\al(m')=\al(m)$, so our two answers
 are actually the same.  Thus, we have a well-defined function
 $\alb\:M/K\xra{}L$ satisfying $\alb(m+K)=\al(m)$ for all $m$.

 Next, we have
 \begin{align*}
  \alb((m_0+K)+(m_1+K)) &= \alb((m_0+m_1)+K) \\
                        &= \al(m_0+m_1) \\
                        &= \al(m_0) + \al(m_1) \\
                        &= \alb(m_0+K) + \alb(m_1+K) 
 \end{align*}
 and
 \begin{align*}
  \alb(a(m+K)) &= \alb((am)+K) \\
               &= \al(am) \\
               &= a\al(m) \\
               &= a \alb(m+K),
 \end{align*}
 so $\alb$ is a homomorphism.

 We next show that $\alb\:M/K\xra{}L$ is surjective.  As $L$ was
 defined as the image of $\al$, any element $n\in L$ has the form
 $n=\al(m)$ for some $m\in M$.  This means that $n=\alb(m+K)$, so $n$
 is in the image of $\alb$.  As this is true for every element of $L$,
 the homomorphism $\alb$ is surjective.

 Finally, we show that $\alb$ is injective.  Suppose we have cosets
 $C_0,C_1$ with $\alb(C_0)=\alb(C_1)$.  Choose $m_0,m_1\in M$ such
 that $C_0=m_0+K$ and $C_1=m_1+K$.  Then the equation
 $\alb(C_0)=\alb(C_1)$ means that $\al(m_0)=\al(m_1)$, so
 $\al(m_0-m_1)=0$, so $m_0-m_1\in\ker(\al)=K$.  As $m_0-m_1\in K$ we
 have $m_0+K=m_1+K$ or in other words $C_0=C_1$.  This proves that
 $\alb$ is injective as well as surjective, so it is an isomorphism.
\end{proof}

\begin{example}\lbl{eg-FIT}
 Define $\al\:\Z^2\xra{}\Z^2$ by $\al(u,v)=(u+v,u+v)$, and put
 $K=\ker(\al)$ and $L=\img(\al)$.  Clearly $\al(u,v)=(0,0)$ if and
 only if $v=-u$, so 
 \[ K = \{(u,v)\in \Z^2\st v=-u\} = \{(t,-t)\st t\in\Z\}. \]
 Next, note that that $\al(u,v)$ is always of the form $(r,r)$ for
 some $r$.  Conversely, any vector of the form $(r,r)$ can be written
 as $\al(r,0)$, so it lies in the image of $\al$.  Thus
 \[ L = \{(x,y)\in\Z^2\st x=y\} = \{(r,r)\st r\in \Z\}. \]
 Consider the set 
 \[ C = 
  \{ \ldots,(-3,5),(-2,4),(-1,3),(0,2),(1,1),(2,0),(3,-1),\ldots\}.
 \]
 This can be described as $C=(1,1)+K$ or $C=(-3,5)+K$ or
 $C=(-999,1001)+K$, so it is a coset of $K$, or in other words an
 element of the group $\Z^2/K$.  We have 
 \[ \alb(C) = \al(1,1) = \al(-3,5) = \al(-999,1001) = (2,2). \]
\end{example}

\begin{proposition}\lbl{prop-cyclic}
 Let $M$ be a cyclic $R$-module.  Then $M\simeq R/I$ for some
 submodule $I$ of $R$.
\end{proposition}
\begin{proof}
 Choose an element $m$ that generates $M$.  Define a homomorphism
 $\al\:R\xra{}M$ by $\al(a)=am$.  As $m$ generates $M$, this
 homomorphism is surjective.  Put $I=\ker(\al)$, which is a submodule
 of $R$.  The First Isomorphism Theorem now tells us that
 $R/I\simeq M$. 
\end{proof}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{2IT}
\begin{exercise}\exlabel{ex-SIT}
 Let $M$ be a module over a ring $R$, and let $L$ and $N$ be
 submodules of $M$.  Prove that $L/(L\cap N)$ is isomorphic to
 $(L+N)/N$.  [You may wish to consider the homomorphism
 $\pi\:L\xra{}(L+N)/N$ given by $\pi(x)=x+N$.]
\end{exercise}
\begin{solution}
 Let $\pi$ be as described.  Every element of $(L+N)/N$ has the form
 $z+N$ for some $z\in L+N$.  We can write $z$ as $x+y$ for some
 $x\in L$ and $y\in N$, so $z+N=x+y+N=x+N$ (because $y+N=N$).  Thus
 every element of $(L+N)/N$ has the form $\pi(x)$ for some $x\in L$,
 which means that $\pi$ is surjective.

 Next, $\ker(\pi)$ is the set of those $x\in L$ for which $x+N=N$, or
 in other words those $x\in L$ for which we also have $x\in N$, so
 $\ker(\pi)=L\cap N$.  

 The First Isomorphism Theorem now tells us that 
 \[ L/(L\cap N)=L/\ker(\pi)\simeq \img(\pi) = (L+N)/N. \]
\end{solution}

%\ip{intext}
\begin{exercise}\exlabel{ex-int-ext}
 Let $M$ be a module over a ring $R$, and let $N_0$ and $N_1$ be
 submodules of $M$.  Define a homomorphism $\sg\:N_0\op N_1\xra{}M$ by
 $\sg(n_0,n_1)=n_0+n_1$.  Prove that $\sg$ is an isomorphism if and
 only if $M$ is the internal direct sum of $N_0$ and $N_1$.
\end{exercise}
\begin{solution}
 The homomorphism $\sg$ is an isomorphism iff it is both injective and
 surjective, or equivalently $\ker(\sg)=\{0\}$ and $\img(\sg)=M$.  We
 have $\img(\sg)=M$ iff every element of $M$ can be written in the form
 $n_0+n_1$ for some $n_0\in N_0$ and $n_1\in N_1$, or equivalently
 $M=N_0+N_1$.  Next, $\ker(\sg)$ is the set of pairs $(n,-n)$ where
 $n\in N_0$ and $-n\in N_1$.  However, we have $-n\in N_1$ iff
 $n\in N_1$, so $\ker(\sg)=\{(n,-n)\st n\in N_0\cap N_1\}$.  It
 follows that $\ker(\sg)=\{0\}$ iff $N_0\cap N_1=\{0\}$.  Thus $\sg$
 is an isomorphism iff $M=N_0+N_1$ and $N_0\cap N_1=\{0\}$, which
 means precisely that $M$ is the internal direct sum of $N_0$ and
 $N_1$. 
\end{solution}


\section{Ideals and factor rings}
\label{sec-ideals}

\begin{definition}\lbl{defn-ideal}
 An \emph{ideal} in a ring $R$ is a subset $I\sse R$ such that
 \begin{itemize}
 \item[(a)] $0\in I$
 \item[(b)] If $b,c\in I$ then $b+c\in I$
 \item[(c)] If $a\in R$ and $b\in I$ then $ab\in I$.
 \end{itemize}
\end{definition}
\begin{remark}\lbl{rem-ideal-submod}
 We remarked earlier that $R$ can be regarded as a module over
 itself.  By comparing the above definition with
 Definition~\ref{defn-submodule} we see that ideals are just the same
 as submodules of $R$.
\end{remark}

\begin{example}\lbl{eg-even}
 Let $I$ be the set of even integers; then $I$ is an ideal in $\Z$.
 Indeed, $0$ is even so~(a) holds; the sum of two even integers is
 even so~(b) holds; and the product of an even integer with any other
 integer is still even so~(c) holds.
\end{example}
\begin{example}\lbl{eg-ker-eval}
 Put $R=\Z[x]$ and $I=\{f\in\Z[x]\st f(1)=0\}$.  For example
 $x^{10}-x\in I$ and $(x-3)(x-2)(x-1)\in I$ but $x+7\not\in I$
 (because $1+7\neq 0$) and $\half x^2-x+\half\not\in I$ (because the
 coefficients are not integers, so $\half x^2-x+\half\not\in\Z[x]$).
 Clearly the zero polynomial is an element of $I$, so~(a) holds.  If
 $f,g\in I$ then $f(1)=g(1)=0$ so $(f+g)(1)=f(1)+g(1)=0+0=0$, so
 $f+g\in I$; thus~(b) holds.  If $f\in I$ and $g$ is any polynomial
 then $(gf)(1)=g(1)f(1)=g(1).0=0$ so $gf\in I$; thus~(c) holds.  This
 shows that $I$ is an ideal in $R$.
\end{example}
\begin{example}\lbl{eg-const}
 Put $R=\Z[x]$ and $I=\{\text{ constant polynomials }\}\sse R$.
 Then $I$ is \emph{not} an ideal.  Axioms~(a) and~(b) certainly hold.
 Moreover, if $a$ and $b$ are elements of $I$ then $ab\in I$ also.
 However, axiom~(c) says more than this: it says that if $b\in I$ and
 $a$ is \emph{any} element of $R$, not necessarily in $I$, then $ab$
 must be in $I$.  However, $x\in R$ and $1\in I$ but $x.1\not\in I$ so
 axiom~(c) is violated.
\end{example}
\begin{example}\lbl{eg-field-ideal}
 Let $K$ be a field.  It is easy to see that $\{0\}$ and $K$ itself
 are ideals in $K$; I claim that these are the only ideals.  Indeed,
 let $I$ be an ideal in $K$.  If $I\neq\{0\}$ then we have some
 nonzero element $b\in I$.  For any element $c\in K$ we have
 $cb^{-1}\in K$ and $b\in I$ so axiom~(c) tells us that
 $(cb^{-1})b\in I$, or in other words $c\in I$.  This means that
 $I=K$, as required.
\end{example}
\begin{example}\lbl{eg-principal}
 Let $R$ be any ring, and let $x$ be any element of $R$.  Define
 $Rx=\{ux\st u\in R\}$.  I claim that this is an ideal (called the
 \emph{principal ideal} generated by $x$).  First, we have $0=0.x\in
 Rx$, so axiom~(a) holds.  Second, if $a,b\in Rx$ then there exist
 $u,v$ such that $a=ux$ and $b=vx$ so $a+b=(u+v)x$, so $a+b\in Rx$.
 This shows that~(b) holds.  Finally, if $a\in R$ and $b\in Rx$ then
 $b=vx$ for some $v\in R$ so $ab=(av)x\in Rx$, so~(c) holds. 

 In example~\ref{eg-even}, the ideal $I$ is just $\Z.2$.  In
 example~\ref{eg-ker-eval}, the ideal $I$ is just $\Z[x].(x-1)$.
\end{example}

Now let $I$ be an ideal in a ring $R$.  Recall that $I$ is an
$R$-submodule of $R$, so we can define the $R$-module
$R/I=\{x+I\st x\in R\}$ as before.  In the case where $I$ is the
principal ideal $Ra$ for some $a\in R$, we will generally write $R/a$
rather than $R/Ra$.

We next show that $R/I$ can itself be regarded as a ring.
\begin{lemma}\lbl{lem-coset-product}
 If $a+I=a'+I$ and $b+I=b'+I$ then $ab+I=a'b'+I$.
\end{lemma}
\begin{proof}
 As $a+I=a'+I$, the element $u:=a-a'$ lies in $I$.  Similarly, the
 element $v:=b-b'$ lies in $I$.  We have $a=a'+u$ and $b=b'+v$ so
 \[ ab-a'b'=(a'+u)(b'+v)-a'b'=a'v+b'u+uv=(a'+u)v+b'u. \]
 As $a'+u\in R$ and $v\in I$, axiom~(c) says that $(a'+u)v\in I$.  As
 $b'\in R$ and $u\in I$, axiom~(c) also says that $b'u\in I$.  As $I$
 is closed under addition, this means that $(a'+u)v+b'u\in I$, so
 $ab-a'b'\in I$, so $ab+I=a'b'+I$ as required.
\end{proof}
It follows that we can define multiplication of cosets unambiguously
by $(a+I)(b+I)=ab+I$.  By the method of
Proposition~\ref{prop-factor-module} we see that this makes $R/I$ into
a ring.  
\begin{definition}\lbl{defn-ring-homomorphism}
 Let $R_0$ and $R_1$ be rings.  A \emph{ring homomorphism} from $R_0$
 to $R_1$ is a function $\al\:R_0\xra{}R_1$ such that
 \begin{itemize}
 \item[(a)] $\al(a+b)=\al(a)+\al(b)$ for all $a,b\in R_0$
 \item[(b)] $\al(1)=1$ 
 \item[(c)] $\al(ab)=\al(a)\al(b)$ for all $a,b\in R_0$.
 \end{itemize}
 One can check that a ring homomorphism automatically satisfies
 $\al(0)=0$ and $\al(-a)=-\al(a)$.  Moreover, if $a$ is invertible in
 $R_0$ then $\al(a)$ is invertible in $R_1$ with
 $\al(a)^{-1}=\al(a^{-1})$.  We say that $\al$ is an
 \emph{isomorphism} if it is a bijection as well as a homomorphism.
 If so, one can check that the inverse function
 $\al^{-1}\:R_1\xra{}R_0$ is also a ring homomorphism.  
\end{definition}

If we define $\pi\:R\xra{}R/I$ by $\pi(a)=a+I$, we find that
$\pi(a+b)=\pi(a)+\pi(b)$ and also $\pi(1)=1$ and
$\pi(ab)=\pi(a)\pi(b)$, in other words $\pi$ is a homomorphism of
rings.

The following result is the \emph{First Isomorphism Theorem for Rings}.
\begin{theorem}\lbl{thm-FIT-rings}
 Let $\al\:R_0\xra{}R_1$ be a homomorphism of rings.  Then $\ker(\al)$
 is an ideal in $R_0$ and $\img(\al)$ is a subring of $R_1$.
 Moreover, there is a ring isomorphism
 $\alb\:R_0/\ker(\al)\simeq\img(\al)$ given by
 $\alb(a+\ker(\al))=\al(a)$.  In particular, if $\al$ is surjective
 then $R_0/\ker(\al)\simeq R_1$.
\end{theorem}
\begin{proof}
 Put $K=\ker(\al)$ and $R_2=\img(\al)$.  If $a,b\in K$ then
 $\al(a)=\al(b)=0$ so $\al(a+b)=\al(a)+\al(b)=0$ so $a+b\in K$.  Also,
 if $c$ is any element of $R_0$ then
 $\al(ca)=\al(c)\al(a)=\al(c).0=0$, so $ca\in K$.  This shows that $K$
 is an ideal in $R_0$.

 As $\al(0)=0$ and $\al(1)=1$ we see that $0,1\in\img(\al)=R_2$.  If
 $u,v\in R_2$ then we have $u=\al(a)$ and $v=\al(b)$ for some
 $a,b\in R_0$.  Thus $u+v=\al(a+b)$ and $-u=\al(-a)$ and $uv=\al(ab)$,
 so $u+v,-u,uv\in R_2$.  This shows that $R_2$ is a subring of $R_1$.

 Just as in the proof of the first isomorphism theorem for modules, we
 have a well-defined bijection $\alb\:R_0/K\xra{}R_2$ given by
 $\alb(a+K)=\al(a)$.  We then have
 \[ \alb((a+K)(b+K))=\alb(ab+K)=\al(ab)=
      \al(a)\al(b)=\alb(a+K)\alb(b+K),
 \]
 and similarly $\alb(1+K)=1$ and
 $\alb((a+K)+(b+K))=\alb(a+K)+\alb(b+K)$, so $\alb$ is a ring
 homomorphism.
\end{proof}

\begin{example}\lbl{eg-FIT-rings}
 For any $n>0$ we put
 $n\Z=\{nk\st k\in\Z\}=\{m\in\Z\st m=0\pmod{n}\}$.  This is an ideal
 in $\Z$, so we have a factor ring $\Z/n\Z$.  Note that $a+n\Z=b+n\Z$
 iff $a-b\in n\Z$ iff $a=b\pmod{n}$.  Using this, we see that $\Z/n\Z$
 is just the usual ring $\Z_n$ of residue classes modulo $n$.
\end{example}
\begin{example}
 Let $K$ be a field, and $\lm$ an element of $K$.  Put
 $I=\{f\in K[x]\st f(\lm)=0\}$, which is an 
 ideal in the ring $R:=K[x]$.  To see this, define a function
 $\al\:K[x]\xra{}K$ by $\al(f)=f(\lm)$.  Clearly
 \begin{align*}
  \al(f+g) &= (f+g)(\lm)=f(\lm) + g(\lm) = \al(f) + \al(g) \\
  \al(fg) &= (fg)(\lm)=f(\lm) g(\lm) = \al(f) \al(g) \\
  \al(1) &= 1
 \end{align*}
 so $\al$ is a ring homomorphism.  If $c\in K$ then we can regard $c$
 as a constant polynomial and we find that $\al(c)=c$; this shows that
 $\al$ is surjective.  It is clear that $\ker(\al)=I$, so
 $K[x]/I\simeq K$ by the First Isomorphism Theorem.

 It is also a standard fact that $f(\lm)=0$ iff $f$ is divisible by
 $x-\lm$, so $I$ is just the principal ideal $K[x].(x-\lm)$, so we
 have shown that $K[x]/(x-\lm)\simeq K$.
\end{example}
\begin{example}\lbl{eg-C-as-quotient}
 Let $I$ be the principal ideal $\R[x](x^2+1)$ in $\R[x]$.  I claim
 that $\R[x]/(x^2+1)=\R[x]/I$ is isomorphic to $\C$.  The basic point
 is just that $\ov{x}^2+1=\ov{x^2+1}=\ov{0}$, so $\ov{x}^2=-\ov{1}$, so
 $\ov{x}$ is a square root of $-1$.

 To give a formal proof, we consider the function $\al\:\R[x]\xra{}\C$
 defined by $\al(f)=f(i)$.  Just as in the last example we find that
 $\al$ is a ring homomorphism.  Any complex number $z$ can be written
 in the form $a+bi$ for some $a,b\in\R$ and we find that
 $\al(a+bx)=a+bi=z$, so $\al$ is surjective.  Thus, if we put
 $I=\ker(\al)=\{f\in\R[x]\st f(i)=0\}$ we have $\R[x]/I\simeq\C$.

 As $i^2+1=0$ we have $x^2+1\in I$ and so $\R[x](x^2+1)\sse I$.
 Conversely, suppose that $f\in I$, so $f(i)=0$.  We can divide $f(x)$
 by $x^2+1$ to get $f(x)=(x^2+1)q(x)+a+bx$ for some polynomial
 $q(x)\in\R[x]$ and $a,b\in\R$.  We then have
 $0=f(i)=q(i)(i^2+1)+a+bi=a+bi$, and by comparing real and imaginary
 parts we see that $a=b=0$.  This means that $f(x)=q(x)(x^2+1)$, so
 $f(x)\in\R[x].(x^2+1)$.  This shows that $I=\R[x](x^2+1)$, and so
 $\R[x]/(x^2+1)=\R[x]/I\simeq\C$.
\end{example}

The next example relies on the following result.
\begin{lemma}\lbl{lem-Zpk-units}
 Let $p$ be a prime number and $k\geq 0$.  If $a$ is not divisible by
 $p$ then $\ov{a}$ is invertible in $\Z_{p^k}$.
\end{lemma}
\begin{proof}
 Let $d$ be the greatest common divisor of $a$ and $p^k$.  This is in
 particular a divisor of $p^k$, so it must be of the form $p^j$ for
 some $j\leq k$.  It must also be a divisor of $a$, which is
 impossible if $j>0$, because $a$ is not divisible by $p$.  We must
 therefore have $j=0$ or in other words $d=1$.  As $(a,p^k)=1$ we have
 $ab+p^kc=1$ for some integers $b,c$.  This means that
 $ab=1\pmod{p^k}$ or in other words $\ov{a}\ov{b}=\ov{1}$.  This shows
 that $\ov{a}$ is invertible (with inverse $\ov{b}$) as required.  
\end{proof}
\begin{example}\lbl{eg-p-local-units}
 Let $p$ be a prime, and consider the ring $\Zpl$ as in
 example~\ref{eg-Zplocal}.  Let $I$ be the principal ideal $\Zpl.p^k$ for
 some $k\geq 0$.  I claim that $\Zpl/p^k$ is isomorphic to
 $\Z_{p^k}=\Z/p^k$.  To see this, we must define a homomorphism
 $\rho\:\Zpl\xra{}\Z/p^k$.  Any element of $x\in\Zpl$ can be written as
 $x=a/b$ where $a,b\in\Z$ and $b\neq 0\pmod{p}$.  This means that
 $\ov{b}$ is invertible in $\Z/{p^k}$, so we have an element
 $\ov{a}\ov{b}^{-1}\in\Z/{p^k}$.  Now suppose we describe $x$ in a
 different way, say as $x=c/d$ with $c,d\in\Z$ and $d\neq 0\pmod{p}$.
 Then $a/b=c/d$ so $ad=bc$ so $\ov{a}\ov{d}=\ov{b}\ov{c}$.  As
 $\ov{b}$ and $\ov{d}$ are invertible we can divide through by them to
 get $\ov{a}\ov{b}^{-1}=\ov{c}\ov{d}^{-1}$.  Thus, we can
 unambiguously define a function $\rho\:\Zpl\xra{}\Z/{p^k}$ by
 $\rho(a/b)=\ov{a}\ov{b}^{-1}$.  It is easy to check that this is a
 ring homomorphism.  

 For any element $y\in\Z/{p^k}$ we can write $y=\ov{a}$ for some
 $a\in\{0,1,\ldots,p^k-1\}$.  Any of these numbers $a$ can be regarded
 as an element of $\Zpl$ and then we have $\rho(a)=\ov{a}=y$.  This
 shows that $\img(\rho)=\Z/{p^k}$.  

 Next, suppose that $x\in\ker(\rho)$.  Then $x=a/b$ for some
 $a,b\in\Z$ with $b\neq 0\pmod{p}$ and $\ov{a}\ov{b}^{-1}=\rho(a/b)=0$
 in $\Z/{p^k}$.  We can multiply this equation by $\ov{b}$ to see that
 $\ov{a}=0$ in $\Z/{p^k}$, so $a=0\pmod{p^k}$, so $a=p^kc$ for some
 integer $c$.  If we define $y=c/b$ we find that $y\in\Zpl$ and
 $x=p^ky$ so $x\in p^k\Zpl$.  Conversely, if $x\in p^k\Zpl$ then
 $x=p^ky$ for some $y\in\Zpl$ so $\rho(y)\in\Z/{p^k}$ and
 $\rho(x)=p^k\rho(y)$.  However, it is easy to see that $p^kz=0$ for
 all $z\in\Z/{p^k}$, so $\rho(x)=0$ so $x\in\ker(\rho)$.

 The first isomorphism theorem now gives us an isomorphism
 $\ov{\rho}\:\Zpl/\ker(\rho)\xra{}\img(\rho)$, or equivalently
 $\ov{\rho}\:\Zpl/p^k\xra{}\Z/{p^k}$.
\end{example}

\begin{proposition}\lbl{prop-RI-mod}
 Modules over $R/I$ are the same thing as modules over $R$ with the
 property that $am=0$ for all $a\in I$ and $m\in M$. 
\end{proposition}
\begin{proof}
 Let $M$ be a module over $R/I$.  To make $M$ into an $R$-module, we
 need to define $am$ for each $a\in R$ and $m\in M$.  Note that
 $\pi(a)=(a+I)\in R/I$, and $M$ is a module over $R/I$, so $\pi(a)m$
 is already defined.  We can thus define $am$ to be $\pi(a)m$.  It is
 easy to check that the axioms are satisfied; for example, we have
 \[ (a+b)m=\pi(a+b)m=(\pi(a)+\pi(b))m=\pi(a)m + \pi(b)m = am+bm, \]
 so multiplication is left distributive.  Thus $M$ is a module over
 $R/I$.  If $a\in I$ then $\pi(a)=0$ so $am=\pi(a)m=0$ as required.

 Conversely, suppose that $M$ is a module over $R$ with the property
 that $am=0$ for all $a\in I$ and $m\in M$.  To make $M$ into an
 $R/I$-module, we must define $Am$ for each coset $A\in R/I$ and each
 $m\in M$.  We would like to do this by writing $A$ in the form $a+I$
 for some $a\in R$ and defining $Am$ to be the same as $am$.  This
 raises the usual problem of ambiguity, but if $a+I=a'+I$ then
 $a-a'\in I$, so $(a-a')m=0$ (by our assumption on $M$) so $am=a'm$.
 Thus, we have an unambiguous definition of $Am$ for $A\in R/I$ and
 $m\in M$.  It is again straightforward to check that the module
 axioms are satisfied.  For example, if $A,B\in R/I$ we can choose
 $a,b\in R$ such that $A=a+I$ and $b=b+I$.  We then have $A+B=(a+b)+I$
 so 
 \[ (A+B)m=((a+b)+I)m=(a+b)m=am+bm=(a+I)m+(b+I)m=Am+Bm, \]
 so multiplication is left distributive.  Thus $M$ is a module over
 $R/I$, as required.
\end{proof}
\begin{example}\lbl{eg-vier}
 Consider the Abelian group $V=\{0,a,b,c\}$ with addition table as
 follows:
 \begin{align*}
  a+a &= b+b = c+c = 0 \\
  a+b &= c \\
  b+c &= a \\
  c+a &= b.
 \end{align*}
 Like any Abelian group, this can be regarded as a $\Z$-module.  As
 $a+a=b+b=c+c=0$, we see that $2v=0$ for all $v\in V$, and thus that
 $nv=0$ for all $n\in 2\Z$.  It follows that $V$ can be regarded as a
 module over the ring $\Z_2$.
\end{example}
\begin{example}\lbl{eg-lagrange}
 Let $M$ be an Abelian group of order $d$, considered as a module over
 $\Z$ as usual.  By Lagrange's theorem, if $m\in M$ then the order of
 $m$ divides $d$.  As we are using additive notation, this just means
 that $dm=0$.  It follows that $am=0$ for all $a\in d\Z$, so $M$ can
 be regarded as a module over $\Z_d$.
\end{example}
\begin{example}\lbl{eg-trig-module}
 Let $W$ be the space of functions of the form
 $f(t)=a\cos(t)+b\sin(t)$ (with $a,b\in\R$).  As in
 Example~\ref{eg-sin-cos}, we can regard this as a module over
 $\R[D]$.  If $f$ is as above then $f'(t)=-a\sin(t)+b\cos(t)$ and so
 $f''(t)=-a\cos(t)-b\sin(t)=-f(t)$, so $(D^2+1)f=f''+f=0$.  If we let
 $I$ be the principal ideal $(D^2+1)\R[D]$ we find that $p(D)f=0$ for
 all $p(D)\in I$ and $f\in W$, so $W$ can be regarded as a module over
 the ring $\R[D]/I=\R[D]/(D^2+1)$.  
\end{example}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{noringhom}
\begin{exercise}\exlabel{ex-no-ring-homs}
 \begin{itemize}
  \item[(a)] Show that there are no ring homomorphisms
   from $\Z_3$ to $\Z$.  [Consider the equation
   $\ov{1}+\ov{1}+\ov{1}=\ov{0}$]. 
  \item[(b)] Show that there are no ring homomorphisms from $\Q$ to
   $\Z$.  [Consider the equation $\half.(1+1)=1$.]
  \item[(c)] Show that there are no ring homomorphisms from $\C$ to
   $\R$.
  \item[(d)] Find a ring homomorphism from $\C$ to $\C$ that is not
   the identity (there is only one reasonable example).
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Let $\al$ be a ring homomorphism from $\Z_3$ to $\Z$.  By
   applying $\al$ to the equation $\ov{1}+\ov{1}+\ov{1}=\ov{0}$ we
   obtain $\al(\ov{1})+\al(\ov{1})+\al(\ov{1})=\al(\ov{0})$ but
   $\al(\ov{1})=1$ and $\al(\ov{0})=0$ so $1+1+1=0$ in $\Z$.  This is
   clearly false, so no such $\al$ can exist. 
  \item[(b)] Let $\al$ be a ring homomorphism from $\Q$ to $\Z$.  By
   applying $\al$ to the equation $\half.(1+1)=1$ we get
   $\al(\half).(\al(1)+\al(1))=\al(1)$.  We also have $\al(1)=1$ so
   $\al(\half).2=1$.  However, there is no element $x\in\Z$ with
   $x.2=1$ so this is impossible, so no such $\al$ can exist.
  \item[(c)] Let $\al$ be a ring homomorphism from $\C$ to $\R$.  By
   applying $\al$ to the equation $i^2+1=0$ we obtain
   $\al(i)^2+\al(1)=\al(0)$ or in other words $\al(i)^2+1=0$.  There
   is no element $x\in\R$ with $x^2+1=0$, so this is impossible, so no
   such $\al$ can exist.
  \item[(d)] The only reasonable example is given by $\al(z)=\ov{z}$
   (the complex conjugate of $z$).  This is a ring homomorphism
   because $\ov{z+w}=\ov{z}+\ov{w}$ and $\ov{zw}=\ov{z}\;\ov{w}$ and
   $\ov{1}=1$.  (There are some other examples defined by a bizarre
   procedure involving heavy set theory.  The above example is the
   only one that is a continuous function from $\C$ to itself.)
 \end{itemize}
\end{solution}

%\ip{FITR}
\begin{exercise}\exlabel{ex-FIT-rings}
 \begin{itemize}
  \item[(a)] Prove that $\Q[x]/(x^2-2)$ is isomorphic to a subring of
   $\R$.
  \item[(b)] Let $I$ be the ideal $\Z[i].(2+3i)$ in $\Z[i]$, and
   define a homomorphism $\al\:\Z\xra{}\Z[i]/I$ by $\al(n)=n+I$.
   \begin{itemize}
   \item[(i)] Show that $\al(-5)=i+I$, and deduce that $\al$ is
    surjective.
   \item[(ii)] Suppose that $n\in\Z$ and that $n$ is divisible by
    $2+3i$ in $\Z[i]$.  Show that $n^2$ is divisible by $13$ in $\Z$.
   \item[(iii)] Show that $\Z[i]/I\simeq \Z_{13}$.
   \end{itemize}
%  \item[(c)] Define two different homomorphisms from $\R[x]/(x^2-1)$
%   to $\R$, and then show that $\R[x]/(x^2-1)\simeq\R\tm\R$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Define $\al\:\Q[x]\xra{}\R$ by $\al(f)=f(\sqrt{2})$; this
   is clearly a ring homomorphism.  As $(\sqrt{2})^2-2=0$ we have
   $x^2-2\in\ker(\al)$, so $\Q[x].(x^2-2)\sse\ker(\al)$.  Conversely,
   suppose that $f\in\ker(\al)$, so $f(\sqrt{2})=0$.  We can divide
   $f(x)$ by $x^2-2$ to get $f(x)=(x^2-2)q(x)+a+bx$ for some
   $a,b\in\Q$.  We then have
   \[ 0=f(\sqrt{2})=(\sqrt{2}^2-2)q(\sqrt{2})+a+b\sqrt{2}=a+b\sqrt{2}. \]
   If $b\neq 0$ we can deduce that $\sqrt{2}=-a/b$ which is impossible
   as $\sqrt{2}$ is irrational.  Thus, we must have $b=0$, in which
   case the equation $0=a+b\sqrt{2}$ tells us that $a=0$ also.  Thus
   $f(x)=(x^2-2)q(x)$, so $f(x)\in\Q[x].(x^2-2)$.  Thus
   $\ker(\al)=\Q[x].(x^2-2)$, so $\Q[x]/(x^2-2)\simeq\img(\al)$ (by
   the First Isomorphism Theorem for rings), and $\img(\al)$ is a
   subring of $\R$ as required.
  \item[(b)] 
   \begin{itemize}
    \item[(i)] We have $\al(-5)=-5+I$, and we want to show that this
     is the same as $i+I$, or in other words that $-5-i\in I$.  By
     direct calculation we have $(-5-i)/(2+3i)=1+i$ which lies in
     $\Z[i]$, so $-5-i=(1+i)(2+3i)\in \Z[i].(2+3i)=I$ as required.

     Now suppose we have an element $a+ib+I\in\Z[i]/I$ (so
     $a,b\in\Z$).  We find that
     $\al(a-5b)=\al(a)+\al(b)\al(-5)=a+bi+I$, and it follows that
     $\al$ is surjective.
    \item[(ii)] Suppose that $n=(2+3i)(u+iv)$.  By taking norms we
     find that $n^2=N(n)=N(2+3i)N(u+iv)=13(u^2+v^2)$, so $n^2$ is
     divisible by $13$ in $\Z$.
    \item[(iii)] As $13=(2+3i)(2-3i)\in I$ we have $\al(13)=0$ so
     $13\Z\sse\ker(\al)$.  Conversely, suppose that $\al(n)=0$, so $n$
     is divisible by $2+3i$ in $\Z[i]$.  By~(ii) we see that $n^2$ is
     divisible by $13$, but $13$ is prime so $n$ itself must be
     divisible by $13$, so $n\in 13\Z$.  Thus $\ker(\al)=13\Z$ and
     $\Z_{13}=\Z/13\Z\simeq\img(\al)=\Z[i]/I$.
   \end{itemize}
 \end{itemize}
\end{solution}

\begin{exercise}\exlabel{ex-C-as-quotient}
 \begin{itemize}
  \item[(a)] Prove that the ring $\R[x]/(x^2+4)$ is isomorphic to
   $\C$.
  \item[(b)] Prove that $\R[x]/(x^2-4)$ is not a field (and thus
   cannot be isomorphic to $\C$).
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Define $\al\:\R[x]\xra{}\C$ by $\al(f)=f(2i)$.  This is
   clearly a ring homomorphism.  Any complex number $a+ib$ can be
   written as $\al(a+bx/2)$, so $\al$ is surjective.  Put
   \[ I=\ker(\al)= \{f\in\R[x]\st f(2i)=0\}. \]
   The First Isomorphism Theorem for rings now tells us that
   $\R[x]/I\simeq\C$, so it will be enough to show that
   $I=\R[x].(x^2+4)$.  It is clear that the polynomial $f(x)=x^2+4$
   satisfies $f(2i)=0$, so $x^2+4\in I$, so $\R[x].(x^2+4)\sse I$.

   Conversely, suppose that $g(x)\in I$, so $g(2i)=0$.  By the
   division algorithm we have $g(x)=q(x)(x^2+4)+ax+b$ for some
   polynomial $q(x)\in\R[x]$ and some $a,b\in\R$.  If we substitute
   $x=2i$ in this relation and use the fact that $g(2i)=0$ we find
   that $2ai+b=0$.  As $a$ and $b$ are real, we can conclude that
   $a=b=0$, so $g(x)=q(x)(x^2+4)$, so $g(x)\in\R[x].(x^2+4)$.  Thus
   $I=\R[x].(x^2+4)$, as required.
  \item[(b)] In $\R[x]/(x^2-4)$ the elements $\ov{x-2}$ and $\ov{x+2}$
   are nonzero, but their product is $\ov{x^2-4}=\ov{0}$.  Thus
   $\R[x]/(x^2-4)$ is not an integral domain, and thus not a field.
 \end{itemize}
\end{solution}

%\ip{F9field}
\begin{exercise}\exlabel{ex-F-nine}
 Let $R$ be the ring $\Z[i]/3$, and put $u=1+i+3\Z[i]\in R$.
 \begin{itemize}
  \item[(a)] List the elements of $R$.
  \item[(b)] Calculate $u^k$ for $0\leq k\leq 8$.
  \item[(c)] Compare your list in~(a) with your list in~(b), and show
   that $R$ is a field.
  \item[(d)] Do you know another proof that $R$ is a field?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] I claim that 
   \[ R=\{\ov{0},\ov{1},\ov{2},
          \ov{i},\ov{1+i},\ov{2+i},
          \ov{2i},\ov{1+2i},\ov{2+2i}\},
   \]
   or equivalently $R=\{\ov{a+bi}\st a,b\in\{0,1,2\}\}$.  Indeed, the
   listed elements are certainly contained in $R$, and it is easy to
   see that they are all different.  Conversely, any element $x\in R$
   can be written as $x=\ov{a+ib}$ for some $a,b\in\Z$.  We can then
   write $a=3c+a'$ for some $c\in\Z$ and $a'\in\{0,1,2\}$ and
   $c\in\Z$.  Similarly we have $b=3d+b'$ for some $d\in\Z$ and
   $b'\in\{0,1,2\}$.  It follows that $(a+bi)=(a'+b'i)+3(c+di)$, so
   $x=\ov{a'+b'i}$, so $x$ is in our list.
  \item[(b)]
   Here we will just write $a+bi$ for $\ov{a+bi}$.  We have
   \begin{align*}
    u^0 &= 1 \\
    u^1 &= 1+i \\
    u^2 &= (1+i)^2 = 2i \\
    u^3 &= u^2.u = 2i(1+i) = 2i-2 = 2i+1 \\
    u^4 &= (u^2)^2 = -4 = 2 \\
    u^5 &= u^4.u = 2+2i \\
    u^6 &= u^4.u^2 = 4i = i \\
    u^7 &= u^4.u^3 = 4i+2 = i+2 \\
    u^8 &= (u^4)^2 = 4 = 1.
   \end{align*}
  \item[(c)] On comparing~(a) with~(b) we see that every nonzero
   element of $R$ is a power of $u$.  We also have $u^8=1$, so
   $u^{8-k}$ is an inverse for $u^k$, so every nonzero element of the
   ring $R$ is invertible.  This means that $R$ is a field.
  \item[(d)] If we can prove that $3$ is irreducible in $\Z[i]$, it
   will follow that $\Z[i]/3$ is a field (Proposition 10.6 in the
   notes).  It is a general fact that prime numbers of the form $4k-1$
   are irreducible in $\Z[i]$, and this obviously covers the case of
   the prime $3$.  More explicitly, if $3$ were reducible we would
   have $3=rs$ for some nonunits $r$ and $s$.  We would then have
   $9=N(3)=N(r)N(s)$, and $N(r),N(s)\neq 1$ as $r$ and $s$ are not
   units.  This means we must have $N(r)=N(s)=3$.  However, $3$ cannot
   be written as $a^2+b^2$ for any integers $a$ and $b$, so we cannot
   have $N(r)=3$, so $3$ must be irreducible after all.
 \end{itemize}
\end{solution}


\section{Euclidean domains}
\label{sec-euclidean}

We next consider Euclidean domains, which are a particular kind of
commutative ring.  The idea is to generalize the following two facts:
\begin{enumerate}
\item If $n$ and $m$ are integers with $m\neq 0$ then we can divide
 $n$ by $m$ to get a quotient $q$ with remainder $r$.  We then have
 $n=mq+r$ and $|r|<|m|$.
\item If $f$ and $g$ are polynomials over $\C$ with $g\neq 0$ then we
 can divide $f$ by $g$ to get a quotient $q$ with remainder $r$.  We
 then have $f=gq+r$ and the degree of $r$ is less than the degree of
 $g$. 
\end{enumerate}
Given a ring $R$ and elements $a,b\in R$ with $b\neq 0$, we would like
to do a similar kind of division to get an equation $a=bq+r$ where the
``size'' of $r$ is less than the ``size'' of $b$.  If $R=\Z$ then
``size'' means absolute value, and if $R=\C[x]$ then ``size'' means
degree.  For a general ring there may not be a suitable notion of
size, so there may not be a useful division algorithm.  A suitable
notion of size is called a \emph{Euclidean valuation}; the formal
definition is as follows.

\begin{definition}\lbl{defn-valn}
 A \emph{Euclidean valuation} on a ring $R$ is a function $\nu(a)$
 defined for all nonzero elements $a$ of $R$ such that
 \begin{itemize}
 \item[(a)] $\nu(a)$ is a nonnegative integer whenever $a\neq 0$.
 \item[(b)] If $a,b\in R$ and $b\neq 0$ then there are elements
  $q,r\in R$ such that $a=bq+r$ and either $r=0$ or $\nu(r)<\nu(b)$.
 \item[(c)] If $a,b\in R$ and $a\neq 0$ and $b\neq 0$ then $ab\neq 0$
  and $\nu(a)\leq\nu(ab)$.  
 \end{itemize}
 A \emph{Euclidean domain} is a ring $R$ for which there exists a
 Euclidean valuation.
\end{definition}
\begin{remark}
 The first part of condition~(c) says that the product of two nonzero
 elements is nonzero, or in other words that $R$ is an integral
 domain.
\end{remark}

\begin{example}\lbl{eg-Z-euclidean}
 The function $\nu(a)=|a|$ is a Euclidean valuation on $\Z$.  Indeed,
 conditions~(a) and~(c) are clear, and~(b) is just the ordinary
 division algorithm for integers.
\end{example}
\begin{example}\lbl{eg-poly-euclidean}
 If $K$ is a field then we can define a Euclidean valuation $\nu$ on
 $K[x]$ by $\nu(f)=\deg(f)=\text{ the degree of } f$.  Indeed,
 conditions~(a) and~(c) are clear, and~(b) is just the ordinary
 division algorithm for polynomials.
\end{example}
\begin{example}\lbl{eg-Zi-euclidean}
 The function $\nu(x+iy)=|x+iy|^2=x^2+y^2$ defines a Euclidean
 valuation on $\Z[i]$.  Indeed, condition~(a) is clear.  For
 condition~(b), suppose that $a=x+iy$ and $b=u+iv$ for some integers
 $u,v,x,y$.  As $b\neq 0$ we can consider the complex number
 $a/b=s+it$ say, where $s,t\in\R$.  Let $s_0$ be the closest integer
 to $s$, so $|s-s_0|\leq 1/2$.  (If $s$ has the form $m+1/2$ for some
 integer $m$ then we could take $s_0=m$ or $s_0=m+1$; it doesn't
 matter which.)  Similarly, we let $t_0$ be the closest integer to
 $t$, so $|t-t_0|\leq 1/2$.  We put $q=s_0+it_0$ and $r=a-qb$ so that
 $a=qb+r$.  We find that
 \[ |a/b-q|^2 = |(s-s_0)+(t-t_0)i|^2 = 
     (s-s_0)^2 + (t-t_0)^2 \leq 1/4 + 1/4 < 1, 
 \]
 so $|r|^2=|a-qb|^2=|b|^2|a/b-q|^2<|b|^2$, or in other words
 $\nu(r)<\nu(b)$ as required.

 Finally, for condition~(c), we have $\nu(ab)=|ab|^2=|a|^2|b|^2$.  It
 is clear that $|b|^2$ is a nonnegative integer, and as $b\neq 0$ we
 have $|b|^2\neq 0$ so $|b|^2\geq 1$ so $\nu(ab)\geq\nu(a)$.
\end{example}
\begin{example}\lbl{eg-p-local-euclidean}
 We next define a Euclidean valuation on $\Zpl$.  Any nonzero element
 $a\in\Zpl$ has the form $u/w$, where $u$ and $w$ are integers and $w$
 is not divisible by $p$.  If we divide $u$ by $p$ as many times as
 possible we end up with an equation $u=p^tv$ where $v$ is not
 divisible by $p$.  We then define $\nu(a)=\nu(p^tv/w)=t$.  For
 example, if $p=3$ we have $\nu(567/13)=\nu(3^4\tm 7/13)=4$.

 Now suppose we have some other element $b\in\Zpl$ with $b\neq 0$.
 We can write this in the form $b=p^sx/y$ where $p$ does not
 divide $x$ or $y$, so $\nu(b)=s$.  We then have
 $ab=p^{t+s}(vx)/(wy)$, from which it is not hard to see that
 $\nu(ab)=t+s=\nu(a)+\nu(b)$.  Given this, condition~(c) is clear.

 Condition~(b) is also satisfied, in a rather trivial way.  If $t<s$
 then we just put $q=0$ and $r=a$ and we have $a=qb+r$ with
 $\nu(r)<\nu(b)$.  On the other hand, if $t\geq s$ then the number
 $q:=a/b=p^{t-s}(vy)/(wx)$ lies in $\Zpl$ and $a=bq$ so we can take
 $r=0$. 
\end{example}

For the rest of this section, we let $R$ denote a Euclidean domain,
with Euclidean valuation $\nu$ say.

\begin{theorem}\lbl{thm-pid}
 Every ideal $I$ in $R$ is principal.
\end{theorem}
\begin{proof}
 Let $I$ be an ideal; we must find an element $b\in R$ such that
 $I=Rb$.  First, if $I=\{0\}$ then $I=R0$ as required; so we may
 assume that $I\neq\{0\}$.  Each nonzero element $b\in I$ has a
 valuation $\nu(b)\in\Z$ with $\nu(b)\geq 0$.  Choose such an element
 for which $\nu(b)$ is as small as possible.  By assumption $b\in I$
 and $I$ is an ideal so $Rb\sse I$.  

 Conversely, suppose that $a\in I$.  By axiom~(b), then there are
 elements $q,r\in R$ such that $a=bq+r$ and either $r=0$ or
 $\nu(r)<\nu(b)$.  Note that $r=a-bq$ and $a,b\in I$ so $r\in I$.  If
 $r$ were a nonzero element of $I$ with $\nu(r)<\nu(b)$, this would
 contradict our choice of $b$.  We must therefore have $r=0$ instead,
 so $a=qb$, so $a\in Rb$.  This proves that $I\sse Rb$ and thus that
 $I=Rb$.
\end{proof}

Now suppose we have two elements $a,b\in R$.  Then $Ra+Rb$ is an ideal
in $R$, so by the theorem there must be an element $d$ such that
$Ra+Rb=Rd$.  This raises the question of how to find $d$ explicitly.

The first thing to note is that $Ra+Rb$ is \emph{not} the same as
$R(a+b)$.  For example, take $R=\Z$ and $a=3$ and $b=2$.  We can write
any number $n$ as $n=n\tm 3+(-n)\tm 2$ so $n\in\Z.3+\Z.2$, which shows
that $\Z.3+\Z.2=\Z$.  However, $\Z.(2+3)$ consists only of the
multiples of $5$, so it is not the same.  

\begin{definition}\lbl{defn-divisible}
 Let $a$ and $b$ be elements of $R$.  We say that $a$ is
 \emph{divisible} by $b$ if and only if there is an element $c\in R$
 such that $a=bc$, or equivalently if $a\in Rb$, or equivalently if
 $Ra\sse Rb$.  We also write $b|a$ if $a$ is divisible by $b$.
\end{definition}

\begin{definition}\lbl{defn-gcd}
 Let $a$ and $b$ be elements of $R$.  A \emph{common divisor} of $a$
 and $b$ is an element $d\in R$ such that $a$ and $b$ are both
 divisible by $d$.  A \emph{greatest common divisor} (or \emph{gcd})
 of $a$ and $b$ is an element $d$ such that
 \begin{itemize}
 \item[(i)] $d$ is a common divisor of $a$ and $b$; and
 \item[(ii)] if $d'$ is any other common divisor then $d$ is divisible
  by $d'$.
 \end{itemize}
\end{definition}

\begin{proposition}\lbl{prop-gcd}
 We have $Ra+Rb=Rd$ if and only if $d$ is a gcd of $a$ and $b$.  If
 so, then $d$ can be written as $xa+yb$ for some $x,y\in R$.
\end{proposition}
\begin{proof}
 First suppose that $Ra+Rb=Rd$.  We can write $a$ as $1a+0b$, so
 $a\in Ra+Rb=Rd$, so $a$ is divisible by $d$.  Similarly $b$ is
 divisible by $d$, so $d$ is a common divisor of $a$ and $b$.

 Next, it is clear that $d\in Rd$, so $d\in Ra+Rb$, so $d=xa+yb$ for
 some $x,y\in R$.

 Finally, suppose that $d'$ is another common divisor of $a$ and $b$.
 Then $a=u'd'$ and $b=v'd'$ for some $u',v'\in R$.  This means that
 $d=xa+yb=xu'd'+yv'd'=(xu'+yv')d'$, so $d$ is divisible by $d'$.  This
 proves that $d$ is a gcd of $a$ and $b$.

 Conversely, suppose that $d$ is a gcd of $a$ and $b$.  We know from
 Theorem~\ref{thm-pid} that $Ra+Rb=Rc$ for some $c\in R$, and we know
 from the first half of this proof that $c$ must also be a gcd of $a$
 and $b$.  As $d$ is a greatest common divisor and $c$ is a common
 divisor, we see that $d$ is divisible by $c$, so $Rd\sse Rc$.  As $c$
 is a greatest common divisor and $d$ is a common divisor, we see that
 $c$ is divisible by $d$, so $Rc\sse Rd$, so $Rd=Rc=Ra+Rb$ as
 required.
\end{proof}

We now explain how to actually find the gcd of two elements $a,b\in
R$.  The answer is to use the Euclidean algorithm, just as in $\Z$ or
$\R[x]$.  We may assume that $a,b\neq 0$ (otherwise the problem is
trivial).  We then define $a_0=a$ and $b_0=b$.  By the definition of a
Euclidean valuation, we can find $p_0,a_1\in R$ such that
$a_0=p_0b_0+a_1$ and either $a_1=0$ or $\nu(a_1)<\nu(b_0)$.
(Informally, $a_1$ is the remainder when we divide $a_0$ by $b_0$.)
Assuming that $a_1\neq 0$, we can then find $q_1,b_1$ such that
$b_0=q_1a_1+b_1$ and either $b_1=0$ or $\nu(b_1)<\nu(a_1)$.  Assuming
that $b_1\neq 0$ we can find $p_1,a_2$ such that $a_1=p_1b_1+a_2$ and
either $a_2=0$ or $\nu(a_2)<\nu(b_1)$.  Assuming that $a_2\neq 0$, we
can then find $q_2,b_2$ such that $b_1=q_2a_2+b_2$ and either $b_2=0$
or $\nu(b_2)<\nu(a_2)$.  Continuing in this way, we get a sequence
\[ \nu(b_0)>\nu(a_1)>\nu(b_1)>\nu(a_2)>\nu(b_2)> \ldots.  \]
Now, all these valuations are nonnegative integers so they cannot keep
decreasing forever.  Thus, after a finite number of steps we must end
up with either $a_k=0$ or $b_k=0$, forcing the process to stop.

Suppose that the first term to be zero is $a_k$, so the elements
$a_0,\ldots,a_{k-1}$ and $b_0,\ldots,b_{k-1}$ are all nonzero.  I
claim that $Ra+Rb=Rb_{k-1}$, so that $b_{k-1}$ is a gcd of $a$ and
$b$.  

To see this, put $I=Ra+Rb$ and $J=Rb_{k-1}$; we must show that $I=J$.
Certainly $a_0,b_0\in I$.  Using the equation $a_1=a_0-p_0b_0$ we
deduce that $a_1\in I$.  Using the equation $b_1=b_0-q_1a_1$ we deduce
that $b_1\in I$.  Using the equation $a_2=a_1-p_1b_1$ we deduce that
$a_2\in I$.  Continuing in this way, we see that $a_i,b_i\in I$ for
all $i$ and in particular that $b_{k-1}\in I$, so $J\sse I$.

We now use a similar argument in the opposite direction.  Clearly
$b_{k-1}\in J$.  We have $a_{k-1}=p_{k-1}b_{k-1}+a_k$ but $a_k=0$ so
$a_{k-1}=p_{k-1}b_{k-1}\in J$.  We can now use the equation
$b_{k-2}=q_{k-1}a_{k-1}+b_{k-1}$ to show that $b_{k-2}\in J$, and then
use the equation $a_{k-2}=p_{k-2}b_{k-2}+a_{k-1}$ to show that
$a_{k-2}\in J$.  Working backwards in this way, we eventually find
that $a_0,b_0\in J$ or in other words $a,b\in J$.  This implies that
$ua+vb\in J$ for all $u,v\in R$, or in other words that 
$I=Ra+Rb\sse J$.  We have already seen that $J\sse I$, so $I=J$ as
required.  

All this assumed that the first term to be zero was $a_k$.  It could
instead happen that the first term to be zero was $b_k$, in which case
a very similar argument would show that $Ra+Rb=Ra_k$, so $a_k$ is a
gcd of $a$ and $b$.

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{rootsofone}
\begin{exercise}\exlabel{ex-roots-of-unity}
 Let $n$ and $m$ be coprime positive integers, and put $f(x)=x^n-1$ and
 $g(x)=x^m-1$.  By considering the roots of $f$ and $g$, show that the
 gcd of $f$ and $g$ in $\C[x]$ is $x-1$.
\end{exercise}
\begin{solution}
 Let $h$ be the gcd of $f$ and $g$, which we can take to be monic.
 Note that $f(1)=g(1)=0$, so both $f$ and $g$ are divisible by $x-1$,
 so $h$ is divisible by $x-1$, or equivalently $h(1)=0$.  We claim
 that this is the only root of $h$.  Indeed, suppose that $h(\zt)=0$.
 As $h$ divides both $f$ and $g$ and $h(\zt)=0$, we see that
 $f(\zt)=g(\zt)=0$, so $\zt^n=\zt^m=1$.  We are also given that $n$
 and $m$ are coprime, so $nu+mv=1$ for some integers $u$ and $v$.  We
 deduce that
 \[ \zt = \zt^1 = \zt^{nu+mv} = (\zt^n)^u (\zt^m)^v = 1^u 1^v = 1, \]
 so $\zt=1$ as claimed.  As this is the only root of $h$, we see that
 $h(x)=(x-1)^k$ for some $k>0$.  To show that $k=1$, it will suffice
 to check that $f$ and $g$ are not divisible by $(x-1)^2$, or
 equivalently that $f'(1)\neq 0\neq g'(1)$.  This is clear from the
 formulae: we have $f'(x)=nx^{n-1}$, so $f'(1)=n>0$, and similarly
 $g'(1)=m>0$. 
\end{solution}


%\ip{Zpgcd}
\begin{exercise}\exlabel{eg-p-local-gcd}
 Let $p$ be a prime, and let $a$ and $b$ be nonzero elements of
 $\Z_{(p)}$.  Show that either $a$ is a gcd of $a$ and $b$ in
 $\Z_{(p)}$, or $b$ is a gcd of $a$ and $b$ in $\Z_{(p)}$.
\end{exercise}
\begin{solution}
 We can write $a=p^nr/s$ for some integer $n\geq 0$ and some integers
 $r,s$ that are not divisible by $p$.  Similarly, we can write
 $b=p^mt/u$ for some integer $m\geq 0$ and some integers
 $t,u$ that are not divisible by $p$.  If $n\leq m$ then the number
 $x=b/a=p^{m-n}ts/ru$ lies in $\Z_{(p)}$ and $b=ax$.  This says that
 $b$ is divisible by $a$, and it follows easily that $a$ is a gcd of
 $a$ and $b$.  Similarly, if $n\geq m$ then $b$ is a gcd of $a$ and
 $b$. 
\end{solution}


\section{Factorisation in Euclidean domains}
\label{sec-factorisation}

Let $R$ be a Euclidean domain.  We say that an element $a\in R$ is a
\emph{nonunit} if it is not invertible in $R$.  We say that an element
is \emph{reducible} if it can be written as a product of two nonunits.
We say that it is \emph{irreducible} if it is a nonunit but is not
reducible. 

Note that $0$ is a nonunit and $0=0.0$ so $0$ is a product of two
nonunits and thus is reducible.  For the rest of this section, we
exclude the element $0$ from consideration.
\begin{example}\lbl{eg-prime-numbers}
 Let $p$ be a prime number.  Then the only ways to factor $p$ in $\Z$
 are $p=1.p=(-1).(-p)=p.1=(-p).(-1)$.  In each case, one of the
 factors is either $1$ or $-1$ and thus is invertible in $\Z$.  Thus,
 $p$ is irreducible.  Similarly, $-p$ is irreducible, but if $n>0$ and
 $n$ is composite then $n$ and $-n$ are reducible.
\end{example}
\begin{example}\lbl{eg-irreducible-poly}
 Consider the ring $\C[x]$.  I claim that the irreducible elements are
 precisely the polynomials of the form $ax+b$ with $a\neq 0$.
 Firstly, polynomials of degree $0$ are invertible (because we have
 excluded the zero polynomial from consideration).  Thus, any nonunit
 has degree at least $1$, so any reducible polynomial has degree at
 least $2$.  Thus when $a\neq 0$ the polynomial $ax+b$ is a nonunit
 but not reducible, so it is irreducible.

 Next, let $f(x)$ be any polynomial of degree $d>1$.  By the
 Fundamental Theorem of Algebra, $f$ has a root, in other words there
 is a complex number $a$ such that $f(a)=0$.  This means that $f$ is
 divisible by $x-a$, say $f(x)=(x-a)g(x)$ for some polynomial $g$.
 Note that $g$ has degree $d-1>0$, so both $g(x)$ and $x-a$ are
 nonunits, so $f$ is reducible.  This shows that the irreducibles are
 precisely the polynomials of degree exactly one, as claimed.
\end{example}
\begin{example}\lbl{eg-p-local-irreducible}
 Any element in $x\in\Zpl$ can be written as $x=p^va/b$ where $a$ and
 $b$ are not divisible by $p$ and so $a/b$ is a unit in $\Zpl$.  In
 other words, every element is a unit multiple of $p^v$ for some
 $v\geq 0$.  Using this we find that $x$ is a unit iff $v=0$, that $x$
 is irreducible iff $v=1$, and that $x$ is reducible iff $v>1$.
\end{example}

\begin{definition}\lbl{defn-associate}
 We write $a\sim b$ if and only if there is an invertible element
 $u\in R$ such that $au=b$.  If so, we say that $a$ is a \emph{unit
 multiple} or \emph{associate} of $b$.  It is not hard to see that
 $a\sim b$ if and only if $Ra=Rb$, and that the relation $\sim$ is an
 equivalence relation.
\end{definition}

Note that in $\Z$, both $7$ and $-7$ are irreducible, but they are
unit multiples of each other so for many purposes it makes no sense to
use both of them.  It is thus traditional to ignore $-7$.  Similarly,
in $\C[x]$ both $x-2$ and $2x-4$ are irreducible but they are unit
multiples of each other, so we usually ignore $2x-4$.  This leads us
to make the following definitions.
\begin{definition}\lbl{defn-comp-irr}
 A \emph{complete set of irreducibles} in a Euclidean domain $R$ is a
 set $\CP$ of irreducibles such that for every irreducible $p$, there
 is a unique irreducible $p'\in\CP$ such that $p'\sim p$.
\end{definition}
Note that for any Euclidean domain, we can always choose a complete
set of irreducibles.  We simply divide the set of all irreducibles up
into equivalence classes under the relation $\sim$, and we pick one
irreducible from each equivalence class.
\begin{example}\lbl{eg-complete-set}
 The set of positive prime numbers is a complete set of irreducibles
 in $\Z$.  The set $\{x-a\st a\in\C\}$ is a complete set of
 irreducibles in $\C[x]$.  The set $\{p\}$ (with just one element) is
 a complete set of irreducibles in $\Zpl$.
\end{example}

\begin{proposition}\lbl{prop-Rp-field}
 If $p$ is irreducible in $R$ then $R/p$ is a field.
\end{proposition}
\begin{proof}
 As $p$ is not a unit we see that $1$ is not divisible by $p$, so
 $\ov{1}\neq\ov{0}$ in $R/p$.

 Every nonzero element of $R/p$ has the form $\ov{a}=a+Rp$, where
 $a\in R$ but $a\not\in Rp$.  We then have $Ra+Rp=Rd$, where $d$ is a
 gcd of $a$ and $p$.  Thus $d$ divides $a$ and $p$, say $a=ud$ and
 $p=vd$.  As $p$ is irreducible, one of $v$ and $d$ must be a unit.
 If $v$ is a unit we have $a=ud=uv^{-1}p$ so $a\in Rp$, contrary to
 assumption.  Thus $d$ must be a unit instead, so $1\in Rd=Ra+Rp$, so
 $1=xa+yp$ for some $x,y\in R$.  This means that $\ov{x}\ov{a}=\ov{1}$
 in $R/p$, so $\ov{a}$ is invertible as required.
\end{proof}
\begin{corollary}\lbl{cor-irr-prime}
 If $a$ and $b$ are not divisible by $p$, then $ab$ is not divisible
 by $p$.
\end{corollary}
\begin{proof}
 $a+Rp$ and $b+Rp$ are nonzero elements of the field $R/p$, so
 $ab+Rp=(a+Rp)(b+Rp)$ is nonzero, so $p$ does not divide $ab$.
\end{proof}

\begin{lemma}\lbl{lem-nu-greater}
 If $a$ and $b$ are nonzero and $b$ is a nonunit then
 $\nu(ab)>\nu(a)$. 
\end{lemma}
\begin{proof}
 Axiom~(c) for Euclidean valuations says that $\nu(ab)\geq\nu(a)$.  It
 is thus enough to suppose that $\nu(ab)=\nu(a)$ and deduce a
 contradiction.  We can divide $a$ by $ab$ with remainder to get
 $a=abq+r$ for some $q,r$ with either $r=0$ or
 $\nu(r)<\nu(ab)=\nu(a)$.  Note that $a(1-bq)=r$.  As $b$ is not a
 unit we cannot have $bq=1$, so $1-bq\neq 0$.  We also have $a\neq 0$
 so $r\neq 0$, so $\nu(r)<\nu(a)$.  However, Axiom~(c) also says that
 $\nu(a(1-bq))\geq\nu(a)$, giving the required contradiction.
\end{proof}

\begin{lemma}\lbl{lem-nu-zero}
 If $\nu(a)=0$ then $a$ is invertible.
\end{lemma}
\begin{proof}
 Divide $1$ by $a$ to get $1=qa+r$ with $r=0$ or $\nu(r)<\nu(a)$.  As
 valuations are always nonnegative we cannot have $\nu(r)<\nu(a)$ so
 we must have $r=0$, so $1=qa$, so $q$ is an inverse for $a$.
\end{proof}

\begin{theorem}\lbl{thm-ufd}
 Let $R$ be a Euclidean domain, and let $\CP$ be a complete set of
 irreducibles in $R$.  Then any nonzero element $a\in R$ can be
 written in the form $a=up_1^{n_1}\ldots p_r^{n_r}$, where
 $p_1,\ldots,p_r$ are distinct irreducibles in $\CP$ and
 $n_1,\ldots,n_r\in\N$ and $u$ is invertible.  Moreover, this
 factorization is unique (except that it could be written in a
 different order, for example $3^2\tm 5^3=5^3\tm 3^2$).
\end{theorem}
\begin{proof}
 We first show that any nonzero element $a\in R$ can be written as a
 unit times a product of standard irreducibles, by induction on
 $\nu(a)$.  If $\nu(a)=0$ then $a$ is a unit, which we think of as a
 unit times the product of the empty list of standard irreducibles.
 If $\nu(a)>0$ then $a$ is a nonunit.  If $a$ is irreducible then it
 has the form $up$, where $u$ is a unit and $p$ is a standard
 irreducible.  Otherwise, $a$ can be written as a product of two
 nonunits, say $a=bc$.  By Lemma~\ref{lem-nu-greater} we see that
 $\nu(b)$ and $\nu(c)$ are strictly less than $\nu(a)$.  By induction
 we may assume that $b$ and $c$ can be written as units times products
 of standard irreducibles, and it follows that the same is true of
 $a$.  By collecting factors together, we can write
 $a=up_1^{n_1}\ldots p_r^{n_r}$, where $p_1,\ldots,p_r$ are distinct
 irreducibles in $\CP$ and $n_1,\ldots,n_r\in\N$ and $u$ is
 invertible.  

 Suppose we have another such factorization
 $a=vq_1^{m_1}\ldots q_s^{m_s}$, where $q_1,\ldots,q_s$ are distinct
 irreducibles in $\CP$ and $m_1,\ldots,m_s\in\N$ and $v$ is
 invertible.  I claim that $p_1=q_j$ for some $j$.  If not, then all
 the elements $q_j$ would be indivisible by $p_1$, as would $v$, and
 Corollary~\ref{cor-irr-prime} would tell us that $v\prod_jq_j^{m_j}$
 is indivisible by $p_1$, so $a$ is indivisible by $p_1$, which is
 clearly false.  Thus $p_1=q_j$ for some $j$, and the same argument
 shows that each $p_i$ is a $q_j$, and similarly that each $q_j$ is a
 $p_i$.  Thus, after renumbering the $q$'s if necessary, we may assume
 that $r=s$ and $p_i=q_i$ for all $i$.

 Now suppose that $n_1\leq m_1$.  Put $b=up_2^{n_2}\ldots p_r^{n_r}$
 and $c=vp_1^{m_1-n_1}p_2^{m_2}\ldots p_r^{m_r}$.  We then have
 $p_1^{n_1}b=a$ and $p_1^{n_1}c=a$ and $p_1^{n_1}\neq 0$ so $b=c$.
 Using Corollary~\ref{cor-irr-prime} again we see that $b$ is not
 divisible by $p_1$ so $c$ is not divisible by $p_1$ so $m_1-n_1=0$ so
 $m_1=n_1$.  A similar argument works if $n_1\geq m_1$, so $n_1=m_1$
 in all cases.  The same method shows that $n_i=m_i$ for all $i$, and
 given this, it is clear that $u=v$ as well.
\end{proof}


\section{Finite free modules over a Euclidean domain}
\label{sec-free}

Throughout this section, we let $R$ denote a Euclidean domain.

\begin{definition}\lbl{defn-finite-free}
 A \emph{finite free module} over $R$ is an $R$-module $M$ that is
 isomorphic to $R^d$ for some nonnegative integer $d$.
\end{definition}

To understand this definition more explicitly, we introduce the notion
of a basis.
\begin{definition}\lbl{defn-basis}
 Let $M$ be an $R$-module.  We say that a list $\{m_1,\ldots,m_d\}$ of
 elements of $M$ is a \emph{basis} if for every element $m\in M$ there
 is precisely one list $(u_1,\ldots,u_d)\in R^d$ such that
 $m=u_1m_1+\ldots+u_dm_d$.
\end{definition}
\begin{proposition}\lbl{prop-basis}
 Let $M$ be an $R$-module.  Then $M$ is a finite free module if and
 only if it has a basis.  
\end{proposition}
\begin{proof}
 Suppose that $\{m_1,\ldots,m_d\}$ is a basis.  As in
 example~\ref{eg-free-hom}, we can define a homomorphism
 $\phi\:R^d\xra{}M$ by $\phi(u_1,\ldots,u_d)=u_1m_1+\ldots+u_dm_d$.
 By the definition of a basis, for each $m\in M$ there is precisely
 one element $u\in R^d$ such that $\phi(u)=m$.  This means that $\phi$
 is a bijection, and thus an isomorphism.  Thus $M\simeq R^d$, so $M$
 is free.

 Conversely, suppose that $M$ is free, so we have an isomorphism
 $\phi\:R^d\xra{}M$ for some $d$.  By the second half of
 Example~\ref{eg-free-hom}, there is a list $m_1,\ldots,m_d$ of
 elements of $M$ such that $\phi(u_1,\ldots,u_d)=u_1m_1+\ldots+u_dm_d$
 for all $(u_1,\ldots,u_d)\in R^d$.  As $\phi$ is an isomorphism, for
 each element $m\in M$ there is a unique element $u\in R^d$ with
 $\phi(u)=m$, and this means precisely that $\{m_1,\ldots,m_d\}$ is a
 basis. 
\end{proof}

\begin{example}\lbl{eg-sub-free-i}
 Put $M=\{(x,y,z)\in\Z^3\st x=y\}$.  Then the vectors $m_1:=(1,1,0)$
 and $m_2:=(0,0,1)$ clearly lie in $M$.  Moreover, any vector in $M$
 can be written uniquely in the form $(x,x,z)=xm_1+zm_2$.  Thus
 $\{m_1,m_2\}$ is a basis for $M$ as a $\Z$-module, so $M$ is free.
\end{example}
\begin{example}\lbl{eg-sub-free-ii}
 Put $M=\{(x,y,z)\in\Z^3\st x+y+z=0\pmod{3}\}$, considered as a module
 over $\Z$.  I claim that this is free as a $\Z$-module.  To prove
 this, we need to find some elements of $M$ which can form a basis.
 Firstly, if $x+y+z=0$ then $(x,y,z)$ will certainly lie in $M$.  Some
 obvious vectors satisfying $x+y+z=0$ are $m_1:=(1,-1,0)$ and
 $m_2:=(0,1,-1)$.  Also, the vector $m_3:=(0,0,3)$ satisfies $x+y+z=3$
 so $x+y+z=0\pmod{3}$ so $m_3\in M$.  We can thus define a map
 $\phi\:\Z^3\xra{}M$ by $\phi(u_1,u_2,u_3)=u_1m_1+u_2m_2+u_3m_3$.

 Now suppose we have an element $m=(x,y,z)\in M$.  Note that
 $x+y+z=0\pmod{3}$, so $x+y+z=3t$ for some $t\in\Z$.  We have
 \[ m-xm_1=(x,y,z)-x(1,-1,0)=(0,x+y,z), \]
 and so 
 \[ m - xm_1 - (x+y)m_2 = (0,x+y,z) - (x+y)(0,1,-1)
     = (0,0,x+y+z) = (0,0,3t) = tm_3, 
 \]
 so $m=xm_1+(x+y)m_2+tm_3=\phi(x,x+y,t)$.  This shows that $\phi$ is
 surjective.  

 Now suppose that $(u,v,w)\in\ker(\phi)$, so $um_1+vm_2+wm_3=0$.  By
 looking at the $x$ coordinates we see that $u.1+v.0+w.0=0$ so $u=0$.
 Thus, the original equation becomes $vm_2+wm_3=0$.  By looking at the
 $y$ coordinates we see that $v.1+w.0=0$, so $v=0$, so the equation
 becomes $wm_3=0$.  By looking at the $z$ coordinates we see that
 $3w=0$ but $w$ is just an integer so the only way $3w$ can be $0$ is
 if $w=0$.  This proves that $\ker(\phi)=\{(0,0,0)\}$, so $\phi$ is
 injective and thus is an isomorphism.
\end{example}

We next give a convenient test for showing that certain modules are
\emph{not} free.
\begin{definition}\lbl{defn-torsion}
 For any $R$-module $M$, an element $m\in M$ is a \emph{torsion
 element} if there is some nonzero element $a\in R$ such that $am=0$.
 We write $\tors(M)$ for the set of all torsion elements.  We say that
 $M$ is \emph{torsion-free} if $\tors(M)=\{0\}$.  Equivalently, a
 module $M$ is torsion-free if whenever $a\in R$ and $x\in M$ are both
 nonzero, their product $ax$ is also nonzero.
\end{definition}
\begin{lemma}\lbl{lem-free-torsion-free}
 Every finite free module is torsion-free.
\end{lemma}
\begin{proof}
 Every finite free module is isomorphic to $R^d$ for some $d$, so it
 is enough to show that $R^d$ is torsion-free.  Let $a$ be a nonzero
 element of $R$, and let $u=(u_1,\ldots,u_d)$ be a nonzero element of
 $R^d$.  This means that $u_i\neq 0$ for some $i$.  As $R$ is an
 integral domain, it follows that $au_i\neq 0$, and thus that the
 vector $au=(au_1,\ldots,au_d)$ is not the zero vector.  
\end{proof}

\begin{example}\lbl{eg-Z-torsion}
 Suppose that $n>1$, and consider $\Z_n$ as a $\Z$-module.  Then
 $\ov{1}$ is a nonzero element of $\Z_n$ and $n$ is a nonzero element
 of $\Z$ but $n.\ov{1}=\ov{n}=\ov{0}$.  Thus $\Z_n$ is not
 torsion-free as a $\Z$-module, so it cannot be a finite free module.
\end{example}
\begin{example}
 Consider $\CRR$ as a module over $\R[D]$ in the usual way.  Then the
 function $f(t)=e^t$ is a nonzero element of $\CRR$, and $D-1$ is a
 nonzero element of $\R[D]$, but $(D-1)f=f'-f=0$.  Thus $\CRR$ is not
 torsion-free, so it is not a finite free module over $\CRR$.  For an
 even simpler proof, just consider the constant function $g(t)=1$, so
 $Dg=g'=0$, again showing that $\CRR$ is not torsion-free. 
\end{example}

We now prove a key theorem, which makes the theory of modules over
Euclidean domains much simpler than for other rings.
\begin{theorem}\lbl{thm-hered}
 Any submodule of a finite free module over a Euclidean domain is
 free.
\end{theorem}
Note that Examples~\ref{eg-sub-free-i} and~\ref{eg-sub-free-ii}
illustrate this.
\begin{proof}
 As every finite free module is isomorphic to $R^d$ for some $d$, it
 will be enough to show that every submodule of $R^d$ is free.  We do
 this by induction on $d$.  

 The case $d=0$ is easy.  The module $R^0$ is just $\{0\}$, the only
 submodule of this is $\{0\}$ itself, and this is free because it is
 $R^0$.

 The case $d=1$ is essentially given by Theorem~\ref{thm-pid}.  Let
 $M$ be a submodule of $R^1=R$, or in other words an ideal in $R$.  By
 Theorem~\ref{thm-pid} we have $M=Ra$ for some $a\in R$.  If $a=0$
 then $M=\{0\}=R^0$ so $M$ is free.  If $a\neq 0$ then I claim that
 $\{a\}$ is a basis for $M$.  Indeed, as $M=Ra$, every element
 $m\in M$ can certainly be written as $m=ua$ for some $u\in R$.  If we
 have $ua=m=va$ then $(u-v)a=0$ but $a\neq 0$ and $R$ is an integral
 domain so $u-v=0$ so $u=v$.  Thus $m$ can be written \emph{uniquely}
 in the form $m=ua$, so $\{a\}$ is a basis and so $M$ is free.

 Now suppose that $d>1$ and that we have proved that every submodule
 of $R^{d-1}$ is free.  We need to show that every submodule
 $M\leq R^d$ is free.  Let $F$ be the set of vectors of the form
 $(x_1,x_2,\ldots,x_{d-1},0)$ in $R^d$.  Note that $F$ is a copy of
 $R^{d-1}$, so every submodule of $F$ is free.  In particular,
 $M\cap F$ is a submodule of $F$ so it is free, with basis
 $\{m_1,\ldots,m_r\}$ say.  Define a homomorphism $\pi\:M\xra{}R$ by
 $\pi(x_1,\ldots,x_d)=x_d$.  The image of $\pi$ is a submodule of $R$
 so it has the form $Ra$ for some $a\in R$.

 Suppose that $a=0$.  Then $\img(\pi)=Ra=\{0\}$, so $\pi(m)=0$ for all
 $m\in M$, so every element of $M$ has the form
 $(x_1,\ldots,x_{d-1},0)$.  This means that every element of $M$ is an
 element of $F$, so $M$ is a submodule of $F$, so $M$ is free.

 Suppose instead that $a\neq 0$.  As $a\in Ra=\img(\pi)$, we can
 choose $m_{r+1}\in M$ such that $\pi(m_{r+1})=a$.  I claim that
 $\{m_1,\ldots,m_r,m_{r+1}\}$ is a basis for $M$.  To see this, let
 $m$ be an element of $M$.  Then $\pi(m)\in\img(\pi)=Ra$, so
 $\pi(m)=u_{r+1}a$ for some $u_{r+1}\in R$.  Put
 $m'=m-u_{r+1}m_{r+1}$, so $m'\in M$ and
 \[ \pi(m') = \pi(m) - u_{r+1}\pi(m_{r+1}) = 
      u_{r+1} a - u_{r+1} a = 0.
 \]
 This means that the last coordinate of $m'$ is $0$, so $m'\in F$.  As
 $m'\in M\cap F$ and $\{m_1,\ldots,m_r\}$ is a basis for $M\cap F$ we
 see that $m'=u_1m_1+\ldots+u_rm_r$ for some $u_1,\ldots,u_r\in R$, so
 \[ m=m'+u_{r+1}m_{r+1}=u_1m_1+\ldots+u_{r+1}m_{r+1}. \]
 This shows that the elements $m_1,\ldots,m_{r+1}$ generate $M$.

 Now suppose that we have elements $v_1,\ldots,v_{r+1}\in R$
 satisfying $v_1m_1+\ldots+v_{r+1}m_{r+1}=0$.  I claim that
 $v_1=\ldots=v_{r+1}=0$.  Indeed, as $m_1,\ldots,m_r\in F$ we have
 $\pi(m_1)=\ldots=\pi(m_r)=0$, so when we apply $\pi$ to the previous
 equation we get $v_{r+1}\pi(m_{r+1})=0$, or equivalently
 $v_{r+1}a=0$.  As $a\neq 0$ and $R$ is an integral domain this means
 that $v_{r+1}=0$.  Thus, our original equation becomes
 $v_1m_1+\ldots+v_rm_r=0$.  As $\{m_1,\ldots,m_r\}$ is a basis for
 $M\cap F$, the only way we can have $v_1m_1+\ldots+v_rm_r=0$ is if
 $v_1=\ldots=v_r=0$, as claimed. 

 It now follows that $\{m_1,\ldots,m_{r+1}\}$ is a basis for $M$, so
 $M$ is free.  This completes the inductive step, and thus the proof
 of the theorem.
\end{proof}

\begin{corollary}\lbl{cor-Noether}
 Let $M$ be a finitely generated module over a Euclidean domain $R$,
 and let $N$ be a submodule of $M$.  Then $N$ is also finitely
 generated.
\end{corollary}
\begin{proof}
 As $M$ is finitely generated, there is a list $m_1,\ldots,m_d$ of
 elements of $M$ such that an arbitrary element $m\in M$ can be
 written in the form $u_1m_1+\ldots+u_dm_d$.  Define
 $\phi\:R^d\xra{}M$ by $\phi(u_1,\ldots,u_d)=u_1m_1+\ldots+u_dm_d$;
 this is clearly a surjective homomorphism. 
 Put $L=\{u\in R^d\st\phi(u)\in N\}$.  I claim that this is a
 submodule of $R^d$.  Indeed, if $u,v\in L$ then
 $\phi(u),\phi(v)\in N$ so $\phi(u+v)=\phi(u)+\phi(v)\in N$ so
 $u+v\in L$.  Similarly, if $u\in L$ and $a\in R$ then $\phi(u)\in N$
 so $\phi(au)=a\phi(u)\in N$ so $au\in L$, so $L$ is a submodule as
 claimed.  Submodules of $R^d$ are finite free modules by the theorem,
 so we can choose a basis $\{p_1,\ldots,p_r\}$ for $L$.  Put
 $n_i=\phi(p_i)$; as $p_i\in L$ we have $n_i\in N$.  I claim that the
 elements $n_1,\ldots,n_r$ generate $N$.  Indeed, suppose $n\in N$.
 Then $n\in M$ and the homomorphism $\phi\:R^d\xra{}M$ is surjective
 so we have $n=\phi(u)$ for some $u\in R^d$.  As $\phi(u)=n\in N$ we
 see that $u\in L$, so $u$ can be written in the form
 $u=v_1p_1+\ldots+v_rp_r$ for some $v_1,\ldots,v_r\in R$.  It follows
 that
 \[ n=\phi(u)=v_1\phi(p_1)+\ldots+v_r\phi(p_r)=v_1n_1+\ldots+v_rn_r.
 \]
 This shows that the elements $n_1,\ldots,n_r$ generate $N$ as
 claimed, so $N$ is finitely generated. 
\end{proof}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{basis}
\begin{exercise}\exlabel{ex-find-bases}
 Find bases over $\Z$ for the following submodules of $\Z^3$.  Justify
 your answers.
 \begin{itemize}
  \item[(a)] $M_0=\{(x,y,z)\st x-y+z=0\pmod{5}\}$
  \item[(b)] $M_1=\{(x,y,z)\st x=y\pmod{2} \text{ and } y=z\pmod{3}\}$
  \item[(c)] $M_2=\{(x,y,z)\st 6x+15y+10z=0\}$. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Put $u=(1,1,0)$ and $v=(0,1,1)$ and $w=(0,0,5)$.  I claim
   that these vectors form a basis for $M_0$ over $\Z$.  Indeed, it is
   easy to see that $u$, $v$ and $w$ all lie in $M_0$.  Moreover, if
   $m=(x,y,z)\in M_0$ then $x-y+z=5t$ for some $t$, so $z=5t-x+y$, and
   one checks that 
   \begin{align*}
    xu+(y-x)v+tw &= (x,x,0) + (0,y-x,y-x) + (0,0,5t) \\
                 &= (x,y,5t+y-x) = (x,y,z) = m.
   \end{align*}
   This shows that $m$ lies in the submodule generated by $u$, $v$ and
   $w$, and it is clear that $u$, $v$ and $w$ are linearly independent
   over $\Z$, so they form a basis as claimed.
  \item[(b)] Here we put $u=(2,0,0)$ and $v=(3,3,0)$ and $w=(1,1,1)$;
   these are easily seen to be elements of $M_1$.  Given an arbitrary
   element $m=(x,y,z)\in M_1$, we note that $x-y=2s$ and $y-z=3t$ for
   some integers $s,t$.  It follows that
   \begin{align*}
    su + tv + zw &= (2s,0,0) + (3t,3t,0) + (z,z,z) \\
                 &= (x-y,0,0) + (y-z,y-z,0) + (z,z,z) \\
                 &= (x,y,z) = m.
   \end{align*}
   This shows that $m$ lies in the submodule generated by $u$, $v$ and
   $w$, and it is clear that $u$, $v$ and $w$ are linearly independent
   over $\Z$, so they form a basis as claimed.
  \item[(c)] Here we put $u=(5,-2,0)$ and $v=(0,2,-3)$; these are
   easily seen to be elements of $M_2$.  Now consider an arbitrary
   element $m=(x,y,z)\in M_2$, so $6x+15y+10z=0$.  We can reduce this
   equation modulo $5$: as $6x=x\pmod{5}$ and $15y=10z=0\pmod{5}$, we
   deduce that $x=0\pmod{5}$.  Similarly, we can reduce modulo $2$ to
   show that $y=0\pmod{2}$, and reduce mod $3$ to see that
   $z=0\pmod{3}$.  We thus have $(x,y,z)=(5r,2s,3t)$ for some
   $r,s,t\in\Z$.  The equation $6x+15y+10z=0$ now gives
   $30r+30s+30t=0$, so $r+s+t=0$.  It follows that
   \begin{align*}
    ru-tv &= (5r,-2r,0) - (0,2t,-3t) \\
          &= (5r,-2(t+r),3t) \\
          &= (5r,2s,3t) = (x,y,z) = m.
   \end{align*}
   This shows that $m$ lies in the submodule generated by $u$ and $v$,
   and it is clear that $u$, $v$ and $w$ are linearly independent over
   $\Z$, so they form a basis as claimed.
 \end{itemize}
\end{solution}

\begin{exercise}\exlabel{ex-split-quotient}
 Let $d_1,\ldots,d_n$ be elements of a ring $R$, and let $N$ be the
 submodule of $R^n$ generated by the elements $d_1e_1,\ldots,d_ne_n$.
 Prove that $R^n/N\simeq R/d_1\op\ldots\op R/d_n$.  [You may wish to
 start by defining a homomorphism
 $\al\:R^n\xra{}R/d_1\op\ldots\op R/d_n$.]
\end{exercise}
\begin{solution}
 We can define $\al\:R^n\xra{}R/d_1\op\ldots\op R/d_n$ by 
 \[ \al(a_1,\ldots,a_n) = (a_1+Rd_1,\ldots,a_n+Rd_n). \]
 Suppose we have an element
 $(u_1,\ldots,u_n)\in R/d_1\op\ldots\op R/d_n$, so $u_i\in R/d_i$ for
 $i=1,\ldots,n$.  For each $i$ we can choose $a_i\in R$ such that
 $u_i=a_i+Rd_i$, and then $\al(a_1,\ldots,a_n)=(u_1,\ldots,u_n)$.
 Thus $\al$ is surjective, and the First Isomorphism Theorem now tells
 us that $R^n/\ker(\al)\simeq R/d_1\op\ldots\op R/d_n$.  We now need
 to determine the kernel of $\al$.  If $\al(a_1,\ldots,a_n)=0$ then
 $a_i+Rd_i$ must be the zero element of $R/d_i$ for all $i$.  This
 means that $a_i\in Rd_i$, so $a_i=b_id_i$ say.  It follows that
 $\un{a}=\sum_ia_ie_i=\sum_ib_i.(d_ie_i)\in N$.  Conversely, it is
 clear that $\al(d_ie_i)=0$ for all $i$, so that $N\sse\ker(\al)$, so
 $\ker(\al)=N$.  Thus $R^n/N\simeq R/d_1\op\ldots\op R/d_n$ as
 claimed. 
\end{solution}

\begin{exercise}\exlabel{ex-compatible-bases}
 Put
 \begin{align*}
  F &= \{(w,x,y,z)\in\Z^4\st w+x+y+z \text{ is even }\} \\
  G &= \{(w,x,y,z)\in\Z^4\st 
          w-x,\;x-y,\text{ and } y-z 
          \text{ are divisible by } 4 \}.
 \end{align*}
 Find integers $d_1,d_2,d_3,d_4>0$ and vectors
 $u_1,u_2,u_3,u_4\in\Z^4$ such that $\{u_1,u_2,u_3,u_4\}$ is a basis
 for $F$ and $\{d_1u_1,d_2u_2,d_3u_3,d_4u_4\}$ is a basis for $G$.
 [It is possible to do this using matrix methods, but intelligent
 trial and error is likely to be easier.]  It follows that $G\sse F$,
 so we can form the factor group $F/G$.  Deduce a description of $F/G$
 as a direct sum of cyclic groups.
\end{exercise}
\begin{solution}
 One solution is as follows:
 \begin{align*}
  u_1 &= (1,1,1,1) \hspace{6em} d_1=1 \\
  u_2 &= (0,0,0,2) \hspace{6em} d_1=2 \\
  u_3 &= (0,1,1,0) \hspace{6em} d_1=4 \\
  u_4 &= (0,0,1,1) \hspace{6em} d_1=4.
 \end{align*}
 Put $v_i=d_iu_i$.  It is clear that the set $\{u_1,\ldots,u_4\}$ is
 linearly independent, as is the set $\{v_1,\ldots,v_4\}$.  It will
 thus be enough to show that the elements $u_i$ generate $F$, and the
 elements $v_i$ generate $G$.

 It is clear that the elements $u_i$ all lie in $F$.  Suppose we have
 an element $f=(w,x,y,z)\in F$, so $w+x+y+z=2t$ for some integer $t$.
 We then have 
 \begin{align*}
  wu_1 + (t-w-x)u_2 + (x-w)u_3 + (y-x)u_4 &=
   (w,w,w,w)+(0,0,0,2t-2w-2x)+ \\
  &\;\; (0,x-w,x-w,0)+(0,0,y-x,y-x) \\
  &= (w,x,y,2t-w-x-y) = (w,x,y,z).
 \end{align*}
 Thus $f$ lies in the $\Z$-submodule generated by
 $\{u_1,\ldots,u_4\}$, as required.

 Next, we have
 \begin{align*}
  v_1 &= (1,1,1,1) \\
  v_2 &= (0,0,0,4) \\
  v_3 &= (0,4,4,0) \\
  v_4 &= (0,0,4,4).
 \end{align*}
 These vectors clearly lie in $G$.  Suppose we have an element
 $g=(w,x,y,z)\in G$, so $w-x=4r$, $x-y=4s$ and $y-z=4t$ for some
 integers $r,s,t$.  We then have $x=w-4r$ and $y=x-4s=w-4r-4s$ and
 $z=y-4t=w-4r-4s-4t$, so $g=(w,w-4r,w-4r-4s,w-4r-4s-4t)$.  From this
 we see directly that $g=wv_1-(t+r)v_2+rv_3+sv_4$.  Thus $f$ lies in
 the $\Z$-submodule generated by $\{u_1,\ldots,u_4\}$, as required.

 We now deduce that 
 \[ \frac{F}{G}\simeq
    \frac{\Z\op\Z\op\Z\op\Z}{\Z\op2\Z\op4\Z\op4\Z}\simeq
    \Z_2\op\Z_4\op\Z_4.
 \]
\end{solution}

\section{Row and column operations}
\label{sec-col-ops}

We saw in the last section that a submodule of a finite free module
over a Euclidean domain is free.  We next give a systematic method for
finding a basis for such a submodule.

Suppose we have vectors $u_1,\ldots,u_n$ in $R^m$, and we let $N$ be
the submodule generated by $u_1,\ldots,u_n$.  It will be convenient to
form the $n\tm m$ matrix $A$ whose columns are the vectors $u_i$.  

\begin{definition}\lbl{defn-col-ops}
 An \emph{elementary column operation} on a matrix $A$ over a ring $R$
 is any of the following operations:
 \begin{itemize}
  \item[(a)] Add a multiple of one column to another column
  \item[(b)] Multiply a column by an invertible element of $R$
  \item[(c)] Exchange two columns.
 \end{itemize}
 We say that a matrix $A$ is in (unreduced) column echelon form if 
 \begin{itemize}
  \item[(a)] All nonzero columns occur to the left of all the zero
   columns.
  \item[(b)] If the $i$'th and $(i+1)$'st columns are nonzero, then
   the top nonzero entry in the $(i+1)$'s column is below the top
   nonzero entry in the $i$'th column.
 \end{itemize}
\end{definition}
\begin{example}
 The matrix
 \[ 
    \bbm 1&0&0&0 \\ 2&0&0&0 \\ 3&6&0&0 \\ 4&7&9&0 \\ 5&8&10&0 \ebm
 \]
 is in column echelon form, but the following matrices are not
 \[ \bbm 0&3&0\\1&4&0\\2&5&0 \ebm \hspace{3em}
    \bbm 0&1&0\\0&2&4\\0&3&5 \ebm \hspace{3em}
    \bbm 0&0&8\\0&4&0\\2&0&0 \ebm.
 \]
\end{example}

\begin{proposition}\lbl{prop-col-red}
 Let $A$ be an $n\tm m$ matrix over a Euclidean domain $R$, and let
 $N$ be the submodule of $R^m$ generated by the columns of $A$.  Then
 $A$ can be reduced by elementary column operations to a matrix $B$ in
 column echelon form, and the nonzero columns of $B$ form a basis for
 $N$.  
\end{proposition}
\begin{remark}\lbl{rem-unreduced-echelon}
 The resulting matrix $B$ is not unique; in general, a matrix $A$ can
 be  converted to many different matrices $B$ in column echelon form.
\end{remark}

The algorithm is a mixture of the Euclidean algorithm and the usual
Gaussian algorithm for column-reducing a matrix over a field.  Suppose
that the top row of $A$ is nonzero (if not, we simply ignore any rows
of zeros at the top and work with the first nonzero row).  Of all the
nonzero elements in the top row, we find one whose valuation is as
small as possible.  After swapping the columns around, we can assume
that this element occurs in the top left corner of the matrix, so it
is the entry $a_{11}$.  If we were working over a field, we would
subtract $a_{k1}/a_{11}$ times the first column from the $k$'th column
(for $k=2,\ldots,n$), and then the top row would have the form
$(a_{11},0,\ldots,0)$.  As we are not working over a field, we do not
necessarily have $a_{k1}/a_{11}\in R$, so we cannot perform these
operations.  As the next best thing, we divide $a_{k1}$ by $a_{11}$ by
the division algorithm to obtain $a_{k1}=a_{11}q_k+r_k$ with either
$r_k=0$ or $\nu(r_k)<\nu(a_{11})$.  We then subtract $q_k$ times the
first column from the $k$'the column for $k=2,\ldots,n$.  The top row
is now $(a_{11},r_2,\ldots,r_n)$.  If $r_2=\ldots=r_n=0$ then we have
a matrix of the form $\blockmat{a_{11}}{0}{*}{A'}$.  We can then
column-reduce the smaller matrix $A'$ by the same process, and
eventually we get a column-reduced form for $A$.  Generally, however,
the elements $r_2,\ldots,r_n$ will not all be zero.  Among those that
are nonzero, we choose one whose valuation is as small as possible.
Note that by construction this valuation is strictly less than that of
$a_{11}$ and valuations are always nonnegative integers, so this kind
of step can only occur finitely many times.  We swap the columns
around to put the element of minimum valuation in the top left corner
and repeat the whole process.

We end up with a matrix $B$ in column echelon form.  Let $N'$ be the
submodule of $R^m$ generated by the columns of $B$.  It is clear from
the form of $B$ that its columns are linearly independent, so they
form a basis for $N'$.  To prove Proposition~\ref{prop-col-red}, it
will be enough to check that $N'=N$, or equivalently that when we
perform an elementary column operation on a matrix, the module spanned
by the columns does not change.  We will explain this by example
rather than giving a formal proof.  Suppose that the matrix has three
columns, say $u_1$, $u_2$ and $u_3$.
\begin{itemize}
 \item[(a)] A typical operation of the first type replaces the list
  $(u_1,u_2,u_3)$ by $(u_1,u_2+c u_1,u_3)$ for some $c\in R$.  If a
  vector $v$ can be written as $a_1u_1+a_2u_2+a_3u_3$, it can also be
  written as $(a_1-ca_2)u_1+a_2(u_2+cu_1)+a_3u_3$, so
  $\spn\{u_1,u_2,u_3\}\sse\spn\{u_1,u_2+cu_1,u_3\}$.  Conversely, if
  $v$ can be written as $b_1u_1+b_2(u_2+cu_1)+b_3u_3$, it can also be
  written as $(b_1+cb_2)u_1+b_2u_2+b_3u_3$, so
  $\spn\{u_1,u_2+cu_1,u_3\}\sse\spn\{u_1,u_2,u_3\}$.
 \item[(b)] A typical operation of the second type replaces the list
  $(u_1,u_2,u_3)$ by $(u_1,cu_2,u_3)$ for some invertible element
  $c\in R$.  If a vector $v$ can be written as $a_1u_1+a_2u_2+a_3u_3$,
  it can also be written as $a_1u_1+(a_2c^{-1})(cu_2)+a_3u_3$, so
  $\spn\{u_1,u_2,u_3\}\sse\spn\{u_1,cu_2,u_3\}$.  Conversely, if $v$
  can be written as $b_1u_1+b_2(cu_2)+b_3u_3$, it can also be written
  as $b_1u_1+(b_2c)u_2+b_3u_3$, so
  $\spn\{u_1,cu_2,u_3\}\sse\spn\{u_1,u_2,u_3\}$.
 \item[(c)] A typical operation of the third type replaces the list
  $(u_1,u_2,u_3)$ by $(u_2,u_1,u_3)$, and this clearly does not change
  the span.
\end{itemize}
A formal proof would be essentially the same but would need more
elaborate notation.

\begin{example}\lbl{eg-col-red-i}
 Here we perform a column reduction over $\Z$ by strictly following the
 algorithm.
 \begin{align*}
           \bbm 5&8&11&3 \\ 16&25&34&9 \ebm
  &\xra{1} \bbm 3&5&8&11 \\ 9&16&25&34 \ebm \\
  &\xra{2} \bbm 3&2&2&2  \\ 9&7&7&7 \ebm \\
  &\xra{3} \bbm 2&3&2&2  \\ 7&9&7&7 \ebm \\
  &\xra{4} \bbm 2&1&0&0  \\ 7&2&0&0 \ebm \\
  &\xra{5} \bbm 1&2&0&0  \\ 2&7&0&0 \ebm \\
  &\xra{6} \bbm 1&0&0&0  \\ 2&3&0&0 \ebm
 \end{align*}
 In step $1$ we note that the element of smallest valuation in the top
 row is the $3$ in the $4$'th column.  We therefore move the $4$'th
 column round to be the first column.  In step $2$ we divide the
 remaining entries in the top row by $3$.  We have $5=1\tm 3+2$ and
 $8=2\tm 3+2$ and $11=3\tm 3+2$ so we subtract $1$ times the first
 column from the second column, $2$ times the first column from the
 third column, and $3$ times the first column from the fourth column.
 In step~$3$ we note that the element of smallest valuation on the top
 row is $2$, so we swap the first two columns to put a $2$ in the top
 left corner.  Then in step~$4$, we subtract the first column from
 each of the other columns.  The element of smallest valuation in the
 top row is now $1$, so we swap the first two columns to put the $1$
 in the top left corner; this is step~$5$.  Finally, we subtract twice
 the first column from the second column.  Our matrix is now in the
 form $\blockmat{a_{11}}{0}{*}{A'}$, and the matrix $A'=(3\;0\;0)$ is
 already in echelon form so we are finished.

 If we use a little imagination rather than strictly following the
 algorithm, we can reduce the matrix more quickly:
 \begin{align*}
           \bbm 5&8&11&3 \\ 16&25&34&9 \ebm
  &\xra{1} \bbm -1&-1&-1&3 \\ -2&-2&-2&9 \ebm \\
  &\xra{2} \bbm -1&0&0&0  \\ -2&0&0&3 \ebm \\
  &\xra{3} \bbm 1&0&0&0  \\ 2&3&0&0 \ebm.
 \end{align*}

 The conclusion is that if $N$ is the submodule of $\Z^2$ generated by
 the vectors $(5,16)$, $(8,25)$, $(11,34)$ and $(3,9)$ then
 $\{(1,2),(0,3)\}$ is a basis for $N$.
\end{example}

\begin{example}\lbl{eg-col-red-ii}
 Here is a column reduction over $\Q[x]$.
 \begin{align*}
           \bbm x^2 & x-1 & x^3 \\ 0 & -1 & x \\ 0 & -1 & 0 \ebm 
  &\xra{1} \bbm 1&x-1&1\\x+1 & -1 & x^2+2x+1\\ x+1&-1&x^2+x+1\ebm\\
  &\xra{2} \bbm 1&0&0 \\ x+1&-x^2&x^2+x\\ x+1&-x^2&x^2 \ebm \\
  &\xra{3} \bbm 1&0&0 \\ x+1&x^2&x \\ x+1&x^2& 0 \ebm\\
  &\xra{4} \bbm 1&0&0 \\ x+1&x&0 \\ x+1&0&x^2 \ebm
 \end{align*}
 In step~$1$ we subtracted $x+1$ times the middle column from the
 first column, and $x^2+x+1$ times the middle column from the third
 column.  In step~$2$ we subtracted $x-1$ times the first column from
 the middle column, and and subtracted the first column from the last
 column.  In step~$4$ we multiplied the middle column by $-1$ and then
 subtracted it from the last column.  In step~$4$ we subtracted $x$
 times the last column from the middle column, and then swapped the
 middle and last columns.

 The conclusion is that the vectors $m_1:=(1,x+1,x+1)$, $m_2:=(0,x,0)$
 and $m_3:=(0,0,x^2)$ form a basis for the submodule of $\Q[x]^3$
 generated by the columns of the original matrix.

 In fact, the columns of the original matrix also form a basis for
 this submodule (just because they are linearly independent over
 $\Q[x]$).  However, the basis $\{m_1,m_2,m_3\}$ is easier to work
 with because it is in echelon form.
\end{example}

\begin{example}\lbl{eg-power-vecs}
 Put $v_k=(k,k^2,k^3,k^4)\in\Z^4$, and let $N$ be the subgroup of
 $\Z^4$ generated by $v_1,\ldots,v_5$.  These vectors are the columns
 of the first matrix written below:
 \begin{align*}
  \bbm 1&2&3&4&5\\
       1&4&9&16&25\\
       1&8&27&64&125\\
       1&16&81&256&625
  \ebm
 &\xra{} 
  \bbm 1&0&0&0&0\\
       1&2&6&12&20\\
       1&6&24&60&120\\
       1&14&78&252&620
  \ebm \\
 &\xra{} 
  \bbm 1&0&0&0&0\\
       1&2&0&0&0\\
       1&6&6&24&60\\
       1&14&36&168&480
  \ebm \\
 &\xra{}
  \bbm 1&0&0&0&0\\
       1&2&0&0&0\\
       1&6&6&0&0\\
       1&14&36&24&0
  \ebm
 \end{align*}
 We deduce that the vectors $(1,1,1,1)$, $(0,2,6,14)$, $(0,0,6,36)$
 and $(0,0,0,24)$ form a basis for $N$.  Note that the ``leading
 terms'' of these vectors are $1!$, $2!$, $3!$ and $4!$; this is part
 of a general pattern.
\end{example}

The above algorithm helps us to find a basis for a submodule $N$ of
$R^n$.  However, we often want instead to investigate the structure of
$R^n/N$, and so far we do not have a method for this.  The simplest
case is where $N$ is generated by the elements $d_1e_1,\ldots,d_re_r$
for some $r\leq n$ and some nonzero elements $d_1,\ldots,d_r$ in $R$,
where $e_i$ is the $i$'th standard basis vector.  It is then easy to
see that
\[ R^n/N \simeq R/d_1\op\ldots\op R/d_r\op R^{n-r}. \]
More generally, suppose we can find a (non-standard) basis
$u_1,\ldots,u_n$ for $R^n$ and nonzero elements $d_1,\ldots,d_r$ such
that $N$ is generated by $d_1u_1,\ldots,d_ru_r$.  We again find that
$R^n/N\simeq R/d_1\op\ldots\op R/d_r\op R^{n-r}$.  Our next algorithm
will enable us to find bases of this type.

\begin{definition}\lbl{defn-normal-form}
 A matrix over a Euclidean domain is in \emph{normal form} if it has
 the form $\blockmat{D}{0}{0}{0}$, where
 \begin{enumerate}
 \item $D$ is an $r\tm r$ matrix for some $r$ (called the \emph{rank})
 \item The diagonal entries in $D$ are nonzero, and the other entries
  are zero
 \item If the diagonal entries are $d_1,\ldots,d_r$, then
  $d_1|d_2|\ldots|d_r$. 
 \end{enumerate}
\end{definition}
For example, the following matrix over $\Z$ is in normal form:
\[ \left(\begin{array}{ccc|cc}
         2 & 0 &  0 & 0 & 0 \\
         0 & 6 &  0 & 0 & 0 \\
         0 & 0 & 12 & 0 & 0 \\ \hline
         0 & 0 &  0 & 0 & 0 
   \end{array}\right)
\]

We have already defined elementary column operations over a Euclidean
domain, and elementary row operations are defined in the obvious
analogous way.

\begin{theorem}\lbl{thm-gauss}
 Let $A$ be an $n\tm m$ matrix over a Euclidean domain $R$, and let
 $N$ be the quotient of $R^m$ by the span of the columns of $A$.  Then
 $A$ can be transformed by row and column operations to a matrix $B$
 in normal form.  Moreover, if the nonzero diagonal entries in $B$ are
 $d_1,\ldots,d_r$ then $N\simeq R/d_1\op\ldots\op R/d_r\op R^{m-r}$.
\end{theorem}

The proof will follow after some auxiliary definitions and preliminary
results.
\begin{remark}\lbl{rem-mod-unit}
 It could happen that some of the elements $d_i$ are units.  In this
 case we have $Rd_i=R$ and so $R/d_i=R/R=0$ so we can drop the term
 $R/d_i$ from the direct sum.  
\end{remark}

\begin{definition}\lbl{defn-nu-A}
 Let $R$ be a Euclidean domain with valuation $\nu$.  For any nonzero
 matrix $A$ over $R$, we let $\nu(A)$ be the smallest of the
 valuations of all the nonzero entries in $A$.  For example, if $R=\Z$
 we have $\nu\bsm 5 & -4\\0&-3\esm=\nu(-3)=3$.
\end{definition}
\begin{definition}
 We say that a nonzero $n\tm m$ matrix $A$ over $R$ is
 \emph{prenormal} if it has the form $\blockmat{d}{0}{0}{B}$, where
 \begin{enumerate}
 \item $d$ is a nonzero element of $R$
 \item $B$ is an $(n-1)\tm(m-1)$ matrix over $R$
 \item every entry in $B$ is divisible by $d$.
 \end{enumerate}
\end{definition}

\begin{lemma}\lbl{lem-prenormal}
 Let $A$ be a nonzero matrix over a Euclidean domain $R$.  Then $A$
 can be transformed into prenormal form by elementary row and column
 operations. 
\end{lemma}
\begin{proof}
 Let $a_{ij}$ be the entry in the $i$'th column and $j$'th row of $A$.
 Put $n=\nu(A)$, which is a nonnegative integer.  By definition we
 have $\nu(a_{ij})=n$ for some $i,j$, and after reordering the rows
 and columns we may assume that $\nu(a_{11})=n$.  Put $d=a_{11}$.   

 Suppose that every entry in $A$ is divisible by $d$.  Then for each
 $i>1$ we have $a_{1i}=q_id$ for some $q_i\in R$.  We can subtract
 $q_i$ times the top row from the $i$'th row for each $i$ to get a new
 matrix in which every row except the first starts with $0$.  It is
 easy to see that in this new matrix, every entry is still divisible
 by $d$.  We can then subtract multiples of the first column from the
 other columns to get a matrix of the form $\blockmat{d}{0}{0}{B}$
 where every entry in $B$ is divisible by $d$.  Thus $A$ can be made
 prenormal, as claimed.

 Now suppose instead that some entry in $A$ is not divisible by $d$.
 I claim that $A$ can be transformed to a new matrix $A'$ with
 $\nu(A')<n=\nu(A)$.  Indeed, if some entry $a_{1i}$ in the first
 column is not divisible by $d$ then we have $a_{1i}=dq+r$ for some
 nonzero remainder $r$ with $\nu(r)<\nu(d)=n$.  If we subtract $q$
 times the first row from the $i$'th row we get a new matrix $A'$ in
 which the $i$'th row starts with $r$, so $\nu(A')\leq\nu(r)<n$.  A
 similar argument works if some entry $a_{j1}$ in the first row is not
 divisible by $d$.  This leaves the case where all entries in the
 first row or the first column are divisible by $d$, but some entry
 $a_{ij}$ (with $i,j>1$) is not divisible by $d$.  After reordering
 some rows and columns we may assume that $a_{22}$ is not divisible by
 $d$.  We thus have $a_{22}=dq+r$ where $r\neq 0$ and
 $\nu(r)<\nu(d)=n$.  We also have $a_{12}=ud$ and $a_{21}=vd$ for some
 $u,v$.  The top left corner of our matrix looks like this:
 \[ \bbm a & u d \\ v d & q d + r \ebm. \]
 We now subtract $v$ times the first row from the second row, then add
 the second row to the top row, then subtract $(q-uv+u)$ times the
 first column from the second column.  The effect on the top left
 corner is as follows:
 \[ \bbm d & ud \\ vd & qd+r \ebm \xra{}
    \bbm d & ud \\ 0 & (q-uv)d + r \ebm \xra{}
    \bbm d & (q-uv+u)d + r \\ 0 & (q-uv)d + r \ebm \xra{}
    \bbm d & r \\ 0 & (q-uv)d+r \ebm.
 \]
 Thus, the matrix $A'$ that we end up with has $r$ as an entry, so
 $\nu(A')\leq\nu(r)<n$ as claimed.

 We now repeat the whole process.  We find that either $A'$ can be
 made prenormal, or it can be transformed to another matrix $A''$ with
 $\nu(A'')<\nu(A')$, and so on.  As valuations of matrices are
 nonnegative integers, we cannot keep on reducing them indefinitely,
 so eventually we must get to a matrix that can be made prenormal.
\end{proof}

\begin{proposition}\lbl{prop-gauss}
 Any matrix $A$ over a Euclidean domain $R$ can be transformed to
 normal form by row and column operations.
\end{proposition}
\begin{proof}
 Let $A_0$ be a matrix over a Euclidean domain, of shape $n\tm m$ say.
 If $A_0=0$ then $A_0$ is already in normal form.  Otherwise, by the
 Proposition, we can convert $A_0$ by row and column operations to the
 form $\blockmat{d_1}{0}{0}{A_1}$, where every entry in $A_1$ is
 divisible by $d_1$.  Clearly $A_1$ has shape $(n-1)\tm(m-1)$.  If
 $A_1$ is nonzero we can convert it to the form
 $\blockmat{d_2}{0}{0}{A_2}$, where the entries in $A_2$ are divisible
 by $d_2$.  As this is obtained from $A_1$ by row and column
 operations, we see that all the entries are still divisible by $d_1$,
 and in particular $d_1|d_2$.  This converts $A_0$ to the form
 \[ \left(\begin{array}{cc|c}
      d_1&0&0 \\ 0&d_2&0 \\ \hline 0&0&A_2
    \end{array}\right).
 \]
 The theorem follows by iterating this process in the evident way.
\end{proof}

\begin{example}\lbl{eg-gauss}
 \[ \bbm 13&3\\7&2 \ebm \xra{}
    \bbm 6&1\\7&2 \ebm \xra{}
    \bbm 6&1\\-5&0 \ebm \xra{}
    \bbm 0&1\\5&0 \ebm \xra{}
    \bbm 1&0\\0&5 \ebm.
 \]
 This row reduction follows the method implicit in the above proof,
 except that we have not bothered to reorder the columns until the
 end.  We start by finding the entry of smallest valuation, which is
 $2$.  We subtract the bottom row from the top row to make the entry
 above the $2$ have smaller valuation than $2$ does.  Now the $1$ in
 the top right hand corner has smaller valuation than everything else,
 and everything is divisible by $1$.  We can thus use row and column
 operations to clear away the rest of the entries in the same row or
 column as the $1$.  We then swap the columns to put the matrix in
 normal form.
\end{example}
\begin{example}
 \[ \bbm 2&0\\ 0&3 \ebm  \xra{}
    \bbm 2&3\\ 0&3 \ebm  \xra{}
    \bbm 2&1\\ 0&3 \ebm  \xra{}
    \bbm 2&1\\-6&0 \ebm  \xra{}
    \bbm 1&0\\ 0&6 \ebm.
 \]
 Here we start with a diagonal matrix whose entry of smallest
 valuation is in the $2$ in the top left corner.  However, the other
 entries are not all divisible by $2$, so the matrix is not
 prenormal.  We first perform a row operation to ensure that there is
 a non-divisible entry on the top row.  We then subtract $2$ times the
 first column from the second column, leaving a remainder of $1$ in
 the top right corner, which has valuation smaller than $2$.  We can
 now clear the entries below and to the left of the $1$ and then
 reorder to get a normal matrix.
\end{example}
\begin{example}\lbl{eg-power-vecs-ii}
 Put $v_k=(k,k^2,k^3,k^4)\in\Z^4$, and let $N$ be the subgroup of
 $\Z^4$ generated by $v_1,\ldots,v_5$.  We showed in
 Example~\ref{eg-power-vecs} that the corresponding matrix can be
 column-reduced as follows:
 \[
  \bbm 1&2&3&4&5\\
       1&4&9&16&25\\
       1&8&27&64&125\\
       1&16&81&256&625
  \ebm
 \xra{} 
  \bbm 1&0&0&0&0\\
       1&2&0&0&0\\
       1&6&6&0&0\\
       1&14&36&24&0
  \ebm
 \]
 We can now perform row operations to reduce this matrix further.
 This works easily in this particular example, because the entries
 underneath the $1$ are divisible by $1$, the entries under the $2$
 are divisible by $2$, and the entry under the $6$ is divisible by
 $6$.  Explicitly, we subtract the first row from each of the other
 rows; then we subtract $3$ times the second row from the third row,
 and $7$ times the second row from the last row; then we subtract $6$
 times the third row from the last row.  This gives the matrix
 \[ 
  \bbm 1&0&0&0&0\\
       0&2&0&0&0\\
       0&0&6&0&0\\
       0&0&0&24&0
  \ebm.
 \]
 The conclusion will be that $\Z^4/N\simeq\Z_2\op\Z_6\op\Z_{24}$.  (We
 have omitted the $\Z_1$ because $\Z_1=0$.)
\end{example}


\begin{definition}\lbl{defn-elementary-matrix}
 Let $E$ be an $n\tm n$ matrix over a ring $R$.  We say that $E$ is
 \emph{elementary} if either
 \begin{enumerate}
 \item It is obtained from the identity matrix by exchanging two rows
  (or equivalently, exchanging two columns); or
 \item The diagonal entries are all equal to $1$, there is a single
  nonzero entry off the diagonal, and all other entries are zero; or
 \item One of the diagonal entries is an invertible element of $R$,
  the remaining diagonal entries are equal to $1$, and all other
  entries are zero.
 \end{enumerate}
 Note that elementary matrices are always invertible, and that their
 inverses are also elementary matrices.
\end{definition}

Just as in the case of fields, if $A'$ is obtained from $A$ by a
single elementary row operation then $A'=EA$ for some elementary
matrix $E$.  For example, let $A$ be an $n\tm 3$-matrix.  Then 
\begin{enumerate}
\item Exchanging the first and third rows is the same as multiplying on
 the left by $\bsm 0&0&1\\0&1&0\\1&0&0\esm$.
\item Adding the third row to the first is the same as multiplying on
 the left by $\bsm 1&0&1\\0&1&0\\0&0&1\esm$.
\item Multiplying the second row by $-1$ is the same as multiplying
 the matrix on the left by $\bsm 1&0&0\\0&-1&0\\0&0&1\esm$.
\end{enumerate}
Similarly, if $A'$ is obtained from $A$ by a single column operation
then $A'=AE$ for some elementary matrix $E$.  Thus, if $A'$ is
obtained from $A$ by a sequence of row and column operations then
$A'=PAQ$ for some matrices $P$ and $Q$ which are products of
elementary matrices and thus are invertible.  Using this, the
following corollary follows immediately from
Proposition~\ref{prop-gauss}
\begin{corollary}\lbl{cor-gauss}
 If $A$ is an $n\tm m$ matrix over a Euclidean domain, then there
 exist invertible square matrices $P$ and $Q$ (of size $m$ and $n$
 respectively) such that $PAQ$ is in normal form. \qed
\end{corollary}

\begin{lemma}\lbl{lem-standard-presentation}
 Let $F$ be a free module over $R$, and let $u_1,\ldots,u_m$ be a
 basis for $F$.  Suppose that $r\leq m$ and $d_1,\ldots,d_r\in R$.
 Let $M$ be the submodule of $F$ generated by $d_1u_1,\ldots,d_ru_r$.
 Then $F/M\simeq R/d_1\op\ldots\op R/d_r\op R^{m-r}$.
\end{lemma} 
\begin{proof}
 Write $Q=R/d_1\op\ldots\op R/d_r\op R^{m-r}$ for brevity.  Any
 element $x\in F$ can be written uniquely in the form
 $x_1u_1+\ldots+x_mu_m$ with $x_1,\ldots,x_m\in R$.  It therefore
 makes sense to define a map $\al\:F\xra{}Q$ by 
 \[ \al(x_1u_1+\ldots+x_mu_m) = 
     (x_1+Rd_1,\ldots,x_r+Rd_r,x_{r+1},\ldots,x_m). 
 \]
 This is easily seen to be a homomorphism. Suppose we have an element
 $y=(y_1,\ldots,y_m)\in Q$, so $y_i\in R/d_i$ for $i\leq r$ and
 $y_i\in R$ for $i>r$.  For $i\leq r$ we can choose $x_i\in R$ such
 that $y_i=x_i+Rd_i$.  We can then define
 $u=(x_1,\ldots,x_r,y_{r+1},\ldots,y_m)\in R^m$ and we find that
 $\al(u)=y$.  Thus $\al$ is surjective, and the First Isomorphism
 Theorem tells us that $Q\simeq F/\ker(\al)$.

 We now need to understand $\ker(\al)$.  If
 $x=\sum_ix_iu_i\in\ker(\al)$ then
 $(x_1+Rd_1,\ldots,x_r+Rd_r,x_{r+1},\ldots,x_m)=(0,\ldots,0)$ which
 means that $x_i=0$ for $i>r$ and $x_i\in Rd_i$ for $i\leq r$.  This
 means that for $i\leq r$ we have elements $y_i\in R$ such that
 $x_i=y_id_i$ and so 
 \[ x=x_1u_1+\ldots+x_ru_r=y_1(d_1u_1)+\ldots+y_r(d_ru_r), \]
 so $x\in\spn\{d_1u_1,\ldots,d_ru_r\}=M$.  This shows that
 $\ker(\al)\sse M$, and the reverse inclusion is easy so
 $\ker(\al)=M$.  We thus have $Q\simeq F/M$ as claimed.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm-gauss}]
 Let $A$ be an $n\tm m$ matrix over a Euclidean domain $R$, let let
 $M$ be the submodule of $R^m$ spanned by the columns of $A$, and put
 $N=R^m/M$.  Choose $P$ and $Q$ as in Corollary~\ref{cor-gauss}, so
 the matrix $C:=PAQ$ is in normal form, with diagonal entries
 $d_1,\ldots,d_r$ say.  Put $B=AQ$.  This is obtained from $A$ by
 elementary column operations, so the columns of $B$ span the same
 submodule as the columns of $A$; in other words, they span $N$.

 Next, note that $PB=C$ so $B=P^{-1}C$.  Let $u_1,\ldots,u_m$ be the
 columns of $P^{-1}$; as $P^{-1}$ is invertible, these form a basis
 for $R^m$.  I claim that the columns of $P^{-1}C$ are
 $d_1u_1,\ldots,d_ru_r,0,\ldots,0$ (with $n-r$ zeros).  This is clear
 in the following special case, where $n=4$, $m=3$ and $r=2$:
 \begin{align*}
  P^{-1}  &= \bbm u_{11} & u_{21} & u_{31} \\
                  u_{12} & u_{22} & u_{32} \\
                  u_{13} & u_{23} & u_{33} \ebm \\
  C       &= \bbm d_1 & 0 & 0 & 0 \\
                  0 & d_2 & 0 & 0 \\
                  0 & 0 & 0 & 0 \ebm \\
  P^{-1}C &= \bbm d_1 u_{11} & d_2 u_{21} & 0 & 0 \\
                  d_1 u_{12} & d_2 u_{22} & 0 & 0 \\
                  d_1 u_{13} & d_2 u_{23} & 0 & 0 \ebm
 \end{align*}
 The general case is the same except for more elaborate notation.

 Thus $M$ is spanned by $d_1u_1,\ldots,d_ru_r$, and it follows that 
 $N=R^m/M$ is isomorphic to $R/d_1\op\ldots\op R/d_r\op R^{m-r}$.
\end{proof}

\begin{corollary}\lbl{cor-class}
 Let $M$ be a finitely generated module over a Euclidean domain $R$.
 Then
 \[ M\simeq R/d_1\op\ldots\op R/d_r \op R^s \]
 for some $r,s\geq 0$ and some nonunits $d_1,\ldots,d_r\in R$ with
 $d_1|d_2|\ldots|d_r$. 
\end{corollary}
\begin{proof}
 As $M$ is finitely generated, we can find a finite list
 $v_1,\ldots,v_m$ of elements that generate $M$.  We can then define a
 homomorphism $\al\:R^m\xra{}M$ by
 \[ \al(a_1,\ldots,a_m)=a_1v_1+\ldots+a_mv_m. \]
 This is surjective because the elements $v_i$ span $M$.  Thus, the
 First Isomorphism Theorem for modules tells us that
 $M\simeq R^m/\ker(\al)$.  We also know that $\ker(\al)$ is a
 submodule of $R^m$ and thus is a finite free module, with basis
 $u_1,\ldots,u_n$ say.  Let $A$ be the $n\tm m$ matrix with columns
 $u_1,\ldots,u_n$, so $M$ is isomorphic to the quotient of $R^m$ by
 the span of the columns of $A$.  The claim now follows 
\end{proof}

\begin{corollary}\lbl{cor-fg-Ab}
 Let $M$ be a finitely generated Abelian group.  Then
 \[ M\simeq \Z_{d_1}\op\ldots\op\Z_{d_r}\op \Z^s \]
 for some $r,s\geq 0$ and some natural numbers $d_1,\ldots,d_r$ with
 $1<d_1|d_2|\ldots|d_r$. \qed
\end{corollary}


%\ip{Cxcr0}
\begin{exercise}\exlabel{ex-col-red-i}
 Reduce the following matrix over $\C[x]$ to column echelon form.
 \[ A = \bbm x-1 & x & x+1 \\ x & 0 & x \\ x+1 & x & x-1 \ebm. \]
\end{exercise}
\begin{solution}
 \begin{align*}
   \bbm x-1 & x & x+1 \\ x & 0 & x \\ x+1 & x & x-1 \ebm 
   &\xra{1} \bbm -1 & x & 1 \\ x & 0 & x \\ 1 & x & -1 \ebm \\
   &\xra{2} \bbm 1 & 0 & 0 \\ -x & x^2 & 2x \\ -1 & 2x & 0 \ebm \\
   &\xra{3} \bbm 1 & 0 & 0 \\ -x & 2x & 0 \\ -1 & 0 & 2x \ebm \\
   &\xra{4} \bbm 1 & 0 & 0 \\ 0 & x & 0 \\ -1 & 0 & x \ebm.
 \end{align*}
 We will refer to the three columns as $C_1$, $C_2$ and $C_3$.  In
 step~1 we subtracted $C_2$ from $C_1$ and from $C_3$ columns.  In
 step~2 we multiplied $C_1$ by $-1$, then subtracted $xC_1$ from $C_2$
 and subtracted $C_1$ from $C_3$.  In step~3 we exchanged $C_2$ and
 $C_3$, and then subtracted $\half x C_2$ from $C_3$.  This gives us a
 matrix in column echelon form.  In step~4 we tidy up a little further
 by multiplying $C_2$ and $C_3$ by $\half$ and then addin $C_2$ to
 $C_1$. 
\end{solution}

%\ip{Cxrr0}
\begin{exercise}\exlabel{ex-gauss-i}
 Consider the following matrix over $\C[x]$:
 \[ A = \bbm x & 0 & -1 \\ -1 & x & -1 \\ 0 & -1 & x+1 \ebm. \]
 Let $M$ be the quotient of $\C[x]^3$ by the span of the columns of
 $A$. 
 \begin{itemize}
  \item[(a)] Reduce $A$ to normal form by row and column operations.
  \item[(b)] Give a polynomial $f(x)$ such that $M\simeq\C[x]/f(x)$.
  \item[(c)] Give a list of basic $\C[x]$-modules whose direct sum is
   isomorphic to $M$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)]
   In step~$1$ we add $x$ times the middle row to the top row, and
   then multiply the middle row by $-1$.  In step~$2$ we add $x$ times
   the first column to the middle column, and subtract the first
   column from the last column.  In step~$3$ we subtract $x^2$ times
   the bottom row from the top row.  Finally, in step~$4$ we add $1+x$
   times the middle column to the last column, and then move the top
   row down to the bottom. 
   \begin{align*}
    \bbm x & 0 & -1 \\ -1 & x & -1 \\ 0 & -1 & x+1 \ebm &\xra{1}
    \bbm 0 & x^2 & -1-x \\ 1 &-x &1 \\ 0 & 1 & -1-x \ebm \xra{2}
    \bbm 0 & x^2 & -1-x \\ 1 &0 &0 \\ 0 & 1 & -1-x \ebm \\
    &\xra{3}
    \bbm 0&0& x^3+x^2-x-1 \\ 1&0&0 \\ 0 & 1 & -1-x \ebm \xra{4}
    \bbm 1&0&0 \\ 0&1&0 \\ 0&0&x^3+x^2-x-1 \ebm.
   \end{align*}
   This gives us a matrix in normal form.
  \item[(b)] It follows that 
   \[ M\simeq \C[x]/(x^3+x^2-x-1). \]
  \item[(c)] We have $x^3+x^2-x-1=(x^2-1)(x+1)=(x-1)(x+1)^2$.  As
   $x-1$ and $x+1$ are coprime, The Chinese Remainder Theorem implies
   that 
   \[ M \simeq \C[x]/(x^3+x^2-x-1) \simeq
        \C[x]/(x-1) \op \C[x]/(x+1)^2 = B(1,1) \op B(-1,2). 
   \]
 \end{itemize}
\end{solution}

%\ip{Cxrr1}
\begin{exercise}\exlabel{ex-gauss-ii}
 Consider the following matrix over $\C[x]$:
 \[ A = \bbm x & 0 & 1 \\ 1 & x & 0 \\ 0 & 1 & x \ebm. \]
 Let $M$ be the quotient of $\C[x]^3$ by the span of the columns of
 $A$. 
 \begin{itemize}
  \item[(a)] Reduce $A$ to normal form by row and column operations.
  \item[(b)] Give a polynomial $f(x)$ such that $M\simeq\C[x]/f(x)$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] 
   \begin{align*}
    \bbm x & 0 & 1 \\ 1 & x & 0 \\ 0 & 1 & x \ebm 
    &\xra{1} \bbm 0 & 0 & 1 \\ 1 & x & 0 \\ -x^2 & 1 & x \ebm \\
    &\xra{2} \bbm 1 & 0 & 0 \\ 0 & 1 & x \\ 0 & -x^2 & 1 \ebm \\
    &\xra{3} \bbm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -x^2 & 1+x^3 \ebm \\
    &\xra{4} \bbm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1+x^3 \ebm.
   \end{align*}
   We refer to the rows as $R_1$, $R_2$ and $R_3$ and to the columns
   as $C_1$, $C_2$ and $C_3$. 

   In step~1 we subtract $xC_3$ from $C_1$.  In step~2 we move $C_3$
   to the front and then subtract $xR_1$ from $R_3$.  In step~3 we
   subtract $xC_2$ from $C_3$.  In step~4 we add $x^2R_2$ to $R_3$. 
  \item[(b)]
   It follows from~(a) that $M\simeq R/1\op R/1\op R/(1+x^3)$.  As
   $R/1=\{0\}$ it follows that $M\simeq R/(1+x^3)$, so we may take
   $f(x)=1+x^3$.
 \end{itemize}
\end{solution}

%\ip{Cxrr2}
\begin{exercise}\exlabel{ex-gauss-iii}
 Consider the following matrix over $\C[x]$:
 \[ A = \bbm x & 0 & 0 \\ 0 & x-1 & 0 \\ 0 & 0 & x^2-x \ebm. \]
 Let $M$ be the quotient of $\C[x]^3$ by the span of the columns of
 $A$. 
 \begin{itemize}
  \item[(a)] Reduce $A$ to normal form by row and column operations.
  \item[(b)] Describe $M$ as a direct sum of cyclic modules over
   $\C[x]$. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] 
   \begin{align*}
             \bbm x & 0 & 0 \\ 0 & x-1 & 0 \\ 0 & 0 & x^2-x \ebm
    &\xra{1} \bbm x & x-1 & 0 \\ 0 & x-1 & 0 \\ 0 & 0 & x^2-x \ebm
     \xra{2} \bbm 1 & x-1 & 0 \\ 1-x & x-1 & 0 \\ 0 & 0 & x^2-x \ebm\\
    &\xra{3} \bbm 1 & 0 & 0 \\ 1-x & x^2-x & 0 \\ 0 & 0 & x^2-x \ebm
     \xra{4} \bbm 1 & 0 & 0 \\ 0 & x^2-x & 0 \\ 0 & 0 & x^2-x \ebm.
   \end{align*}
   In step~1 we add $R_2$ to $R_1$, and in step~2 we subtract $C_2$
   from $C_1$.  In step~3 we subtract $(x-1)C_1$ from $C_2$, and in
   step~4 we subtract $(1-x)R_1$ from $R_2$.
  \item[(b)]
   It follows that $M\simeq\C[x]/(x^2-x)\op\C[x]/(x^2-x)$.  (As usual,
   we have omitted the factor of $\C[x]/1$, because $\C[x]/1=\{0\}$.)
 \end{itemize}
\end{solution}

%\ip{RS4}
\begin{exercise}\exlabel{ex-gauss-iv}
 Let $M$ be the subgroup of $\Z^5$ generated by the columns of the
 following matrix.  
 \[ A = \bbm 1 &  1 &  2 &  3 &  3 \\
             1 & -1 &  0 &  1 & -1 \\
             1 &  1 &  2 & -1 & -1 \\
             1 &  1 & -1 &  0 &  0 \\
             1 & -1 &  0 & -1 &  1 \ebm
 \]
 Determine the structure of $\Z^5/M$.

 [Readers familiar with representation theory may be interested to
 know that $A$ is the character table of the symmetric group $S_4$.
 This means that $\Z^5/M=C/R$, where $R$ is the character ring of
 $S_4$ and $C$ is the ring of integer-valued class functions.  You can
 do the question without knowing any of this, however.]
\end{exercise}
\begin{solution}
 We need to reduce $A$ to normal form by row and column operations.
 We start by subtracting multiples of the first column from the other
 columns to clear out the entries in the top row.  We then subtract
 the first row from each of the other rows to clear out the first
 column.  Finally, we multiply all the rows except the first one by
 $-1$.  This leaves the following matrix:
 \[ \left( \begin{array}{c|cccc}
             1 &  0 &  0 &  0 &  0 \\ \hline
             0 &  2 &  2 &  2 &  4 \\
             0 &  0 &  0 &  4 &  4 \\
             0 &  0 &  3 &  3 &  3 \\
             0 &  2 &  0 &  4 &  2 \end{array}\right)
 \]
 From here onwards, we can ignore the first row and column and just
 work with the remaining $4\tm 4$ block.  This can be reduced as
 follows: 
 \[ \bbm  2 &  2 &  2 &  4 \\
          0 &  0 &  4 &  4 \\
          0 &  3 &  3 &  3 \\
          2 &  0 &  4 &  2 \ebm \xra{1}
    \bbm  1 &  2 & -1 & -1 \\
          4 &  0 &  0 &  4 \\
          3 &  0 &  3 &  3 \\
          2 &  2 &  0 &  4 \ebm \xra{2}
    \left(\begin{array}{c|ccc}
          1 &  0 &  0 &  0 \\\hline
          0 & -8 &  4 &  8 \\
          0 & -6 &  6 &  6 \\
          0 & -2 &  2 &  6 \end{array}\right)
 \]
 In step $1$ we subtracted the third row from the first row to get
 $4-3=1$ in the top right corner, then moved the last column to the
 front so as to get a $1$ in the top left corner.  In step~$2$ we
 subtracted multiples of the first column from the remaining columns
 to clear out the top row, and then cleared out the first column.

 From here onwards we can ignore the first row and column and just
 work with the remaining $3\tm 3$ block.  This can be reduced as
 follows:
 \[ \bbm -8 & 4 & 8 \\ -6 & 6 & 6 \\ -2 & 2 & 6 \ebm \xra{1}
    \bbm 2 & -2 & 6 \\ -8 & 4 & 8 \\ -6 & 6 & 6 \ebm \xra{2}
    \bbm 2 & 0 & 0 \\ 0 & -4 & -16 \\ 0 & 0 & -12 \ebm \xra{3}
    \bbm 2 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 12 \ebm.
 \]
 In step~$1$, we moved the bottom row to the top and multiplied it by
 $-1$.  In step~$2$ we added multiples of the first column to the
 other columns to clear out the top row, then cleared out the first
 column.  In step~$3$ we subtracted $4$ times the middle column from
 the last column, and then multiplied both these columns by $-1$.

 The conclusion is that $\Z^5/M\simeq \Z_2\op\Z_4\op\Z_{12}$.
\end{solution}

%\ip{Zcr0}
\begin{exercise}\exlabel{ex-col-red-primes}
 Reduce the following matrix over $\Z$ to column echelon form.
 \[ A = \bbm 71 & 97 & 113 & 149 \\ 1 & 1 & 1 & 1 \ebm. \]
 (The entries in the top row are the $20$'th, $25$'th, $30$'th and
 $35$'th prime numbers.  You could actually derive the answer from
 this fact, but it is probably easier to just do the calculation.)
\end{exercise}
\begin{solution}
 \begin{align*}
  \bbm 71 & 97 & 113 & 149 \\ 1 & 1 & 1 & 1 \ebm 
   &\xra{1} \bbm 71 & 26 & 42 & 7 \\ 1 & 0 & 0 & -1 \ebm \\
   &\xra{2} \bbm 1 & 5 & 0 & 7 \\ 11 & 3 & 6 & -1 \ebm \\
   &\xra{3} \bbm 1 & 0 & 0 & 0 \\ 11 & -52 & 6 & -78 \ebm \\
   &\xra{4} \bbm 1 & 0 & 0 & 0 \\ 11 & 2 & 6 & 0 \ebm \\
   &\xra{5} \bbm 1 & 0 & 0 & 0 \\ 11 & 2 & 0 & 0 \ebm.
 \end{align*}
 We refer to the columns as $C_1,\ldots,C_4$.  In step~1 we subtract
 $C_1$ from $C_2$ and $C_3$, and $2C_1$ from $C_4$.  In step~2 we
 subtract $10C_4$ from $C_1$, $3C_4$ from $C_2$ and $6C_4$ from $C_3$.
 In step~3 we subtract $5C_1$ from $C_2$, and $7C_1$ from $C_4$.  In
 step~4 we add $9C_3$ to $C_2$ and $13C_3$ to $C_4$.  Finally, in
 step~5 we subtract $3C_2$ from $C_3$.
\end{solution}

%\ip{detord}
\begin{exercise}\exlabel{ex-det-deg}
 \begin{itemize}
  \item[(a)] Let $A$ be and $n\tm n$ matrix over $\Z$, and suppose
   that $B$ is obtained from $A$ by row and column operations.  Show
   that $|\det(A)|=|\det(B)|$.  
  \item[(b)] Let $A$ be an $n\tm n$ matrix over $\Z$, and suppose
   that $|\det(A)|=d\neq 0$.  Let $M$ be the quotient of $\Z^n$ by the
   span of the columns of $A$.  Show that $M$ is a finite Abelian
   group of order $d$.
  \item[(c)] Let $A$ be an $n\tm n$ matrix over $\C[x]$, with
   $\det(A)=f(x)\neq 0$.  Let $M$ be the quotient of $\C[x]^n$ by the
   span of the columns of $A$.  Show that $\dim_\C(M)$ is the degree
   of the polynomial $f(x)$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] If we add one row of a matrix to another row, then the
   determinant is unchanged.  If we swap two rows, then the
   determinant just changes sign.  The only invertible elements in
   $\Z$ are $1$ and $-1$.  If we multiply a row by one of these
   invertible elements, then the determinant is either unchanged or
   just changes sign.  Similar remarks apply to column operations.
   Thus, if $B$ is obtained from $A$ by a sequence of row and column
   operations then $\det(B)=\pm\det(A)$ and so $|\det(B)|=|\det(A)|$.
  \item[(b)] Let $A$ be an $n\tm n$ matrix over $\Z$, and suppose
   that $|\det(A)|=d\neq 0$.  Let $B$ be a matrix in normal form
   obtained from $A$ by row and column operations, so $B$ has the form 
   \[ B = \blockmat{D}{0_{(n-r)\tm r}}{0_{r\tm(n-r)}}{0_{(n-r)\tm(n-r)}},
   \]
   where $D$ is a diagonal matrix with nonzero diagonal entries
   $d_1,\ldots,d_r$ say.  By multiplying some rows by $-1$ if
   necessary, we may assume that $d_i>0$ for all $i$.  By part~(a) we
   have $|\det(B)|=|\det(A)|=d>0$.  If we had $r<n$ then the last
   $n-r$ columns in $B$ would be zero and we would have $\det(B)=0$,
   giving a contradiction.  We must thus have $r=n$ and so $B=D$.  As
   $D$ is a diagonal matrix we have
   $d=|\det(B)|=|\det(D)|=d_1d_2\ldots d_r$.  We also know that
   $M\simeq \Z_{d_1}\op\ldots\op\Z_{d_r}$ and thus
   $|M|=|\Z_{d_1}|\ldots|\Z_{d_r}|=d_1\ldots d_r$.  Thus $|M|=d$ as
   claimed. 
  \item[(c)] Let $B$ be a matrix in normal form obtained from $A$ by
   row and column operations.  Just as in~(a) we see that $\det(B)$ is
   an invertible element of $\C[x]$ times $\det(A)$.  The invertible
   elements of $\C[x]$ are the nonzero constants, and it follows that
   the degree of $\det(B)$ is the same as the degree of $\det(A)$.  

   In particular, we see that $\det(B)\neq 0$ so $B$ cannot contain
   blocks of zeros, so it must just be a diagonal matrix with nonzero
   entries $g_1(x),\ldots,g_n(x)$ say.  Let $m_i$ be the degree of
   $g_i$.  Then $\det(B)=g_1\ldots g_n$ so
   $\deg(f)=\deg(\det(B))=m_1+\ldots+m_n$.

   On the other hand, we also have
   $M\simeq\C[x]/g_1\op\ldots\op\C[x]/g_n$, so
   $\dim_\C(M)=\sum_{i=1}^n\dim_\C(\C[x]/g_i)$.  Using the division
   algorithm we see that every element of $\C[x]/g_i$ has the form
   $a_0+\ldots+a_{m_i-1}x^{m_i-1}+\C[x]g_i(x)$ for some unique list of
   coefficients $a_0,\ldots,a_{m_i-1}$.  This means that
   $\{1,x,\ldots,x^{m_i-1}\}$ is a basis for $\C[x]/g_i$, so
   $\dim_\C(\C[x]/g_i)=m_i$.  It follows that
   $\dim_\C(M)=\sum_{i=1}^n\dim_\C(\C[x]/g_i)=\sum_{i=1}^nm_i=\deg(f)$
   as claimed.
 \end{itemize}
\end{solution}

%\ip{hilbert}
\begin{exercise}\exlabel{ex-hilbert-matrix}
 Let $A$ be the $3\tm 3$ matrix whose $(i,j)$'th entry is $1/(i+j-1)$,
 as shown below:
 \[ A = \bbm 1 & 1/2 & 1/3 \\ 1/2 & 1/3 & 1/4 \\ 1/3 & 1/4 & 1/5\ebm.
 \]
 This is called a \emph{Hilbert matrix}.  Put $B=120 A$ (so the
 entries in $B$ are integers) and let $M$ be the quotient of $\Z^3$ by
 the span of the columns of $B$.  Describe $M$ as a direct sum of
 cyclic groups. 
\end{exercise}
\begin{solution}
 We have 
 \[ B = \bbm 120 & 60 & 40 \\ 60 & 40 & 30 \\ 40 & 30 & 24 \ebm. \]
 This can be put in normal form as follows:
 \begin{align*}
           \bbm 120 & 60 & 40 \\ 60 & 40 & 30 \\ 40 & 30 & 24 \ebm 
  &\xra{1} \bbm 40 & 20 & 40 \\ 0 & 10 & 30 \\ -8 & 6 & 24 \ebm 
   \xra{2} \bbm 60 & 20 & -40 \\ 10 & 10 & -10 \\ -2 & 6 & 0 \ebm \\
  &\xra{3} \bbm 60 & 200 & -40 \\ 10 & 40 & -10 \\ -2 & 0 & 0 \ebm
   \xra{4} \bbm 0 & 200 & 40 \\ 0 & 40 & 10 \\ 2 & 0 & 0 \ebm \\
  &\xra{5} \bbm 2 & 0 & 0 \\ 0 & 40 & 10 \\ 0 & 200 & 40 \ebm 
   \xra{6} \bbm 2 & 0 & 0 \\ 0 & 10 & 40 \\ 0 & 40 & 200 \ebm  \\
  &\xra{7} \bbm 2 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 40 & 40 \ebm 
   \xra{8} \bbm 2 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 40 \ebm.
 \end{align*}
 In step~1 we subtract $2C_3$ from $C_1$ and $C_3$ from $C_2$.  In
 step~2 we add $C_2$ to $C_1$.  In step~3 we add $3C_1$ to $C_2$.  In
 step~4 we multiply $C_3$ and $R_3$ by $-1$ and then subtract $30R_3$
 from $R_1$ and $5R_3$ from $R_2$.  In step~5 we swap $R_1$ and $R_3$,
 and in step~6 we swap $C_2$ and $C_3$.  In step~7 we subtract $4C_2$
 from $C_3$, and in step~8 we subtract $4R_2$ from $R_3$.

 The conclusion is that $M\simeq\Z_{2}\op\Z_{10}\op\Z_{40}$.
\end{solution}

%\ip{kk3}
\begin{exercise}\exlabel{ex-cubic-span}
 Let $M$ be the subgroup of $\Z^2$ generated by the vectors $(k,k^3)$
 for $k=2,3,4,5$.  Find a basis for $M$ over $\Z$, and determine the
 order of $\Z^2/M$.
\end{exercise}
\begin{solution}
 The generators of $M$ are the columns of the matrix
 $\bbm 2&3&4&5\\8&27&64&125 \ebm$.  We can perform column operations
 to simplify this matrix as follows:
 \begin{align*}
  \bbm 2&3&4&5\\8&27&64&125 \ebm &
   \xra{} \bbm 2&1&0&1 \\ 8&19&48&109 \ebm 
   \xra{} \bbm 1&2&0&1 \\ 19&8&48&109 \ebm  \\
  &\xra{} \bbm 1&0&0&0 \\ 19&-30&48&90 \ebm 
   \xra{} \bbm 1&0&0&0 \\ 19&30&18&0 \ebm  \\
  &\xra{} \bbm 1&0&0&0 \\ 19&12&18&0 \ebm 
   \xra{} \bbm 1&0&0&0 \\ 1&6&0&0 \ebm.
 \end{align*}
 It follows that $M$ is generated by the vectors $(1,1)$ and $(0,6)$,
 and these elements are clearly linearly independent, so they form a
 basis for $M$ over $\Z$.  Note also that $(1,1)$ and $(0,1)$ form a
 basis for $\Z^2$; using this, it is easy to see that
 $\Z^2/M\simeq\Z_6$, and so $|\Z^2/M|=6$.
\end{solution}

%\ip{thirty}
\begin{exercise}\exlabel{ex-thirty}
 Let $A$ be the following matrix over $\Z$:
 \[ A = \bbm 6&0&0 \\ 0&10&0 \\ 0&0&15 \ebm. \]
 \begin{itemize}
  \item[(a)] Reduce $A$ to normal form by row and column operations
   over $\Z$.
  \item[(b)] What does the answer tell you about the group
   $\Z_6\op\Z_{10}\op\Z_{15}$?
  \item[(c)] Can you prove this fact in an easier way?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Note that $A$ is not in normal form to start with,
   because $10$ and $15$ are not divisible by $6$.  The first step is
   to bring some numbers that are not divisible by $6$ up onto the top
   row.  It is convenient to add both the second and third rows to the
   first row.  This has the effect that the greatest common divisor of
   the numbers on the new first row is $1$.  We can then perform
   column operations until we have a $1$ in the top right corner:
   \[ \bbm 6&0&0\\0&10&0\\0&0&15\ebm \xra{}
      \bbm 6&10&15\\0&10&0\\0&0&15\ebm \xra{}
      \bbm 6&-2&3\\0&10&0\\0&0&15\ebm \xra{}
      \bbm 6&-2&1\\0&10&10\\0&0&15\ebm.
   \]
   We then subtract multiples of the first row from the other rows, to
   clear out the entries underneath the $1$.  After that, we perform
   column operations to clear out the entries to the left of the $1$.
   We then exchange the first and last columns to put the $1$ in the
   top left corner.
   \[ \bbm 6&-2&1\\0&10&10\\0&0&15\ebm \xra{}
      \bbm 6&-2&1\\-60&30&0\\-90&30&0\ebm \xra{}
      \bbm 0&0&1\\-60&30&0\\-90&30&0\ebm \xra{}
      \bbm 1&0&0\\0&30&-60\\0&30&-90\ebm. 
   \]
   We now add twice the middle column to the last column, subtract the
   middle row from the last row, and multiply the last row by $-1$:
   \[ \bbm 1&0&0\\0&30&-60\\0&30&-90\ebm \xra{}
      \bbm 1&0&0\\0&30&0\\0&30&-30\ebm \xra{}
      \bbm 1&0&0\\0&30&0\\0&0&-30\ebm \xra{}
      \bbm 1&0&0\\0&30&0\\0&0&30\ebm.
   \]
   The last matrix (which we will call $B$) is in normal form.
  \item[(b)]
   As $B$ is obtained from $A$ by row and column operations, the
   quotient of $\Z^3$ by the span of the columns of $A$ is isomorphic
   to the quotient of $\Z^3$ by the span of the columns of $B$.  In
   other words,
   \[ \frac{\Z\op\Z\op\Z}{6\Z\op 10\Z\op 15\Z} \simeq
      \frac{\Z\op\Z\op\Z}{1\Z\op 30\Z\op 30\Z},
   \]
   or equivalently $\Z_6\op\Z_{10}\op\Z_{15}\simeq\Z_{30}\op\Z_{30}$.
   (We have omitted the factor of $\Z_1$, because $\Z_1=\{0\}$.)
  \item[(c)] The Chinese Remainder Theorem implies that 
   \begin{align*}
    \Z_6    &\simeq \Z_2\op\Z_3 \\
    \Z_{10} &\simeq \Z_2\op\Z_5 \\
    \Z_{15} &\simeq \Z_3\op\Z_5 \\
    \Z_{30} &\simeq \Z_2\op\Z_3\op\Z_5,
   \end{align*}
   so
   \begin{align*}
    \Z_6\op\Z_{10}\op\Z_{15} &\simeq
      (\Z_2\op\Z_3)\op(\Z_2\op\Z_5)\op(\Z_3\op\Z_5) \\
    &\simeq (\Z_2\op\Z_3\op\Z_5)\op(\Z_2\op\Z_3\op\Z_5) \\
    &\simeq \Z_{30}\op\Z_{30}.
   \end{align*}
 \end{itemize}
\end{solution}


% \section{Finite subgroups of fields}
% \label{sec-field-subgroups}
%
% We now digress slightly to discuss an interesting application of
% Corollary~\ref{cor-fg-Ab}.  Let $K$ be a field, and let $K^\tm$ be the
% set of nonzero elements of $K$, which is a group under
% multiplication.  This will of course be written in multiplicative
% notation, so a little translation will be required before we can apply
% results from previous sections, which were written in additive
% notation. 
% \begin{theorem}\lbl{thm-cyclic}
%  Any finite subgroup of $K^\tm$ is cyclic.
% \end{theorem}
% The proof will be given after some examples and preliminary results.
% We start by remarking that if $H$ is a finite subgroup of $K^\tm$ of
% order $n$ say, and $x\in H$, then $x^n=1$.  This means that all
% elements of $H$ are roots of unity.
% \begin{example}
%  The only finite subgroups of $\Q^\tm$ are $\{1\}$ and $\{1,-1\}$,
%  both of which are clearly cyclic.
% \end{example}
% \begin{example}
%  Put $H_n=\{z\in\C\st z^n=1\}$.  Then $H_n$ is generated by
%  $e^{2\pi i/n}$ and thus is cyclic.
% \end{example}

% \begin{lemma}\lbl{lem-roots}
%  Let $f(x)$ be a polynomial of degree $d$ over $K$.  Then there are at
%  most $d$ different elements $a\in K$ such that $f(a)=0$.
% \end{lemma}
% \begin{proof}
%  Suppose we have distinct elements $a_1,\ldots,a_r$ with
%  $f(a_1)=\ldots=f(a_r)=0$.  We need to show that $r\leq d$.

%  As $f(a_1)=0$ we see that $f(x)$ is divisible by $x-a_1$, say
%  $f(x)=(x-a_1)g_1(x)$.  Note that $(a_2-a_1)g_1(a_2)=f(a_2)=0$ but
%  $a_1\neq a_2$ so $(a_2-a_1)\neq 0$ so $g_1(a_2)=0$.  This means that
%  $g_1(x)=(x-a_2)g_2(x)$ for some polynomial $g_2$, so that
%  $f(x)=(x-a_1)(x-a_2)g_2(x)$.  Next note that
%  $(a_3-a_1)(a_3-a_2)\neq 0$ but $(a_3-a_1)(a_3-a_2)g_2(a_3)=f(a_3)=0$
%  so $g_2(a_3)=0$ so $g_2(x)=(x-a_3)g_3(x)$ for some polynomial $g_3$.
%  After iterating this procedure $r$ times we see that $f(x)$ is
%  divisible by $(x-a_1)\ldots(x-a_r)$, so the degree of $f$ is at least
%  $r$, in other words $r\leq d$ as claimed.
% \end{proof}

% \begin{lemma}\lbl{lem-exponent}
%  Let $M$ be a finite Abelian group (written additively) which is not
%  cyclic.  Then there is an integer $d$ with $0<d<|M|$ such that $dm=0$
%  for all $m\in M$.
% \end{lemma} 
% \begin{proof}
%  We can decompose $M$ as $\Z/d_1\op\ldots\op\Z/d_r\op\Z^s$ as in
%  Corollary~\ref{cor-fg-Ab}.  As $M$ is finite we must have $s=0$.  As
%  $M$ is not cyclic we must have $r>1$.  As $|M|=d_1d_2\ldots d_r$ and
%  $d_1>1$ we must have $d_r<|M|$.  As $d_1|d_2|\ldots|d_r$ and $d_im=0$
%  for all $m\in \Z/d_i$ it is not hard to see that $d_rm=0$ for all
%  $m\in M$, as required.
% \end{proof}

% \begin{proof}[Proof of Theorem~\ref{thm-cyclic}]
%  Let $K$ be a field, and $H$ a finite subgroup of $K^\tm$, of order
%  $n$ say.  Suppose that $H$ is not cyclic.  By
%  Lemma~\ref{lem-exponent} (translated into multiplicative notation),
%  there is some integer $d$ with $0<d<n$ such that $a^d=1$ for all
%  $a\in H$.  Thus, the $n$ elements of the group $H$ are $n$ different
%  roots of the polynomial $x^d-1$, which has degree $d<n$.  This
%  contradicts Lemma~\ref{lem-roots}, so $H$ must be cyclic after all.
% \end{proof}

% \begin{corollary}
%  For any prime $p$, the group $\Z_p^\tm$ is isomorphic to the cyclic
%  group $C_{p-1}$.
% \end{corollary}
% \begin{proof}
%  We have seen that $\Z_p$ is a field.  There are precisely $p$
%  elements, of which $p-1$ are nonzero.  Thus the whole group
%  $\Z_p^\tm$ is finite, and thus cyclic by the Theorem.  As it has
%  order $p-1$ it is isomorphic to $C_{p-1}$.
% \end{proof}
% \begin{example}
%  We can check directly that $\Z_{11}^\tm$ is generated by $\ov{2}$ and
%  thus is cyclic.  Indeed, we have $|\Z_{11}^\tm|=10$, so the order of
%  $\ov{2}$ in this group divides $10$, so it must be $1$, $2$, $5$ or
%  $10$.  We have $\ov{2}^1\neq\ov{1}$ and $\ov{2}^2=\ov{4}\neq\ov{1}$
%  and $\ov{2}^5=\ov{32}=\ov{-1}\neq\ov{1}$ so the order is not $1$, $2$
%  or $5$ so it must be $10$.  Thus the subgroup generated by $\ov{2}$
%  has order $10$, so it must be the whole group.
% \end{example}

% The corollary also works for any field with finitely many elements.
% For example, if $p$ is a prime of the form $4k+3$ it can be shown that
% the ring $K=\Z[i]/p$ is a field with $p^2$ elements, and it follows
% that $K^\tm\simeq C_{p^2-1}$.  Moreover, for any prime $p$ and any
% $n>0$ it turns out that there is a field with $p^n$ elements, and that
% any two such fields are isomorphic.  If $K$ is such a field then
% $K^\tm\simeq C_{p^n-1}$.

\section{Primary decomposition}
\label{sec-primary}

Corollary~\ref{cor-class} gives us in some sense a complete
list of all finitely generated $R$-modules.  However, it turns out not
to be the most convenient kind of list to work with.  Moreover, it
still leaves us with a question about uniqueness.  For example, both
$\Z_4\op\Z_{12}$ and $\Z_2\op\Z_{24}$ appear in the list of finitely
generated Abelian groups, and so far we have no way of knowing whether
they might be isomorphic.  In this section we will modify the
classification given in Corollary~\ref{cor-class} to get a new
classification which is more useful in practice and which allows us to
prove a uniqueness theorem. 

Let $R$ be a Euclidean domain, and choose a complete set of
irreducibles $\CP$.  

Let $M$ be an $R$-module.  Recall that in
Definition~\ref{defn-torsion} we defined $\tors(M)$ to be the set of
torsion elements in $M$, in other words the set of elements $m\in M$
such that there is a nonzero element $a\in R$ with $am=0$.
\begin{proposition}\lbl{prop-tors-submodule}
 $\tors(M)$ is a submodule of $M$.
\end{proposition}
\begin{proof}
 Suppose that $m,n\in\tors(M)$.  Then there are nonzero elements
 $a,b\in R$ such that $am=0$ and $bn=0$.  It follows that $ab\neq 0$
 and $ab(m+n)=b(am)+a(bn)=b.0+a.0=0$, so $m+n\in\tors(M)$.  Similarly,
 for any $c\in R$ we have $a(cm)=c(am)=0$, so $cm\in\tors(M)$, so
 $\tors(M)$ is a submodule as claimed.
\end{proof}

\begin{definition}\lbl{defn-torsion-module}
 We say that $M$ is a \emph{torsion module} if $\tors(M)=M$.  We say
 that $M$ is a \emph{finite torsion module} if it is finitely
 generated as well as being a torsion module.
\end{definition}
\begin{definition}
 A \emph{basic} $R$-module is an $R$-module of the form $R/p^k$ for
 some $p\in\CP$ and $k>0$.
\end{definition}
We will show that any finite torsion module is isomorphic to a direct
sum of basic modules.

\begin{example}\lbl{eg-torsion-group}
 For any $m\in \Z_{12}$ we have $12m=0$, so $m$ is a torsion element.
 This shows that $\Z_{12}$ is a torsion module over $\Z$.  More
 generally, let $M$ be any finite Abelian group, considered as a
 $\Z$-module.  If $d$ is the order of $M$ then $dm=0$ for all
 $m\in M$, which shows that $M$ is a torsion module.  We saw in
 Example~\ref{eg-finite-fg} that $M$ is also finitely generated, so it
 is a finite torsion module.
\end{example}
\begin{example}\lbl{eg-poly-torsion}
 Let $W_2$ be the set of functions of the form $f(t)=a+bt+ct^2$,
 considered as an $\R[D]$-module in the usual way.  For a function $f$
 of this form we have $f'(t)=b+2ct$ so $f''(t)=2c$ so $f'''(t)=0$.
 This means that $D^3f=0$, so $f$ is a torsion element of $W$.  It
 follows that $W_2$ is a torsion module.  We saw in
 Example~\ref{eg-poly-cyclic} that $W_2$ is a cyclic module over
 $\R[D]$ and thus is finitely generated, so it is a finite torsion
 module over $\R[D]$.  Similarly, the space $W_d$ of polynomials of
 degree at most $d$ is a finite torsion module over $\R[D]$.
\end{example}
\begin{example}\lbl{eg-QZ-torsion}
 Consider the Abelian group $M=\Q/\Z$ as a $\Z$-module.  Any element
 $m\in M$ has the form $a/b+\Z$ for some $a,b\in\Z$ with $b\neq 0$.
 it follows that $bm=a+\Z$, but $a\in\Z$ so $a+\Z=\Z$, which is the
 zero element of the group $M$.  Thus $bm=0$, proving that $m$ is a
 torsion element.  It follows that $M$ is a torsion module.  It is not
 finitely generated, however.
\end{example}
\begin{example}\lbl{eg-eigen-torsion}
%%% Gareth didn't understand this.
 Let $K$ be a field, and let $\lm$ and $\mu$ be elements of $K$.
 Consider $K^2$ as a module over $K[x]$ using the endomorphism
 $x.(u,v)=\phi(u,v)=(\lm u,\mu v)$.  Then 
 \[ (x-\lm).(u,v)=(\lm u,\mu v)-(\lm u,\lm v)= (0,(\mu-\lm)v), \]
 and
 \[ (x-\mu).(u,v)=(\lm u,\mu v)-(\mu u,\mu v)= ((\lm-\mu)u,0), \]
 so 
 \[ (x-\mu)(x-\lm).(u,v) = (x-\mu).(0,(\mu-\lm)v) = (0,0). \]
 In other words, the element $p(x)=(x-\mu)(x-\lm)\in K[x]$ satisfies
 $p(x).(u,v)=(0,0)$ for all $(u,v)\in K^2$, which proves that $K^2$ is
 a torsion module.
\end{example}
\begin{example}\lbl{eg-fin-dim-torsion}
 More generally, let $V$ be any module over $K[x]$ that is
 finite-dimensional (of dimension $d$ say) when considered as a vector
 space over $K$.  I claim that $V$ is a finite torsion module over
 $K[x]$.  Indeed, suppose that $v\in V$.  We then have vectors
 $v,xv,x^2v,\ldots,x^dv$ in $V$.  There are $d+1$ vectors in this
 list, but $V$ only has dimension $d$, so the vectors in our list must
 be linearly dependent.  Thus, there are scalars $a_0,\ldots,a_d\in K$
 (not all zero) such that $a_0v+a_1xv+\ldots+a_dx^dv=0$.  This means
 that the polynomial $f(x)=a_0+a_1x+\ldots+a_dx^d$ is nonzero and
 satisfies $f.v=0$, so $v$ is a torsion element.  This shows that $V$
 is a torsion module.  Moreover, we can choose a basis
 $v_1,\ldots,v_d$ for $V$ as a vector space, and it follows that these
 elements generate $V$ as a module over $K[x]$, so $V$ is finitely
 generated. 
\end{example}


Next recall that for any two rings $R_0,R_1$ we can form the product
ring $R_0\tm R_1$.  The elements of $R_0\tm R_1$ are pairs $(a_0,a_1)$
with $a_0\in R_0$ and $a_1\in R_1$.  Addition and multiplication are
defined in the obvious way:
\begin{align*}
 (a_0,a_1)+(b_0,b_1) &= (a_0+b_0,a_1+b_1) \\
 (a_0,a_1)(b_0,b_1)  &= (a_0b0,a_1b_1).
\end{align*}
The additive identity is the element $(0,0)$, and the multiplicative
identity is the element $(1,1)$.

The following result is called the \emph{Chinese remainder theorem}.
\begin{proposition}\lbl{prop-chinese}
 If $a$ and $b$ are coprime then $R/ab\simeq R/a\tm R/b$ as rings (or
 as $R$-modules).
\end{proposition}
\begin{proof}
 Define $\al\:R\xra{}R/a\tm R/b$ by $\al(t)=(t+aR,t+bR)$.  Note that
 $\al(s+t)=\al(s)+\al(t)$ and $\al(st)=\al(s)\al(t)$ and $\al(1)=1$,
 so $\al$ is a homomorphism of rings.

 As $a$ and $b$ are coprime, we have $xa+yb=1$ for some $x,y\in R$.  

 Suppose that $\al(t)=(0,0)$.  Then $t+aR$ is the zero coset $0+aR$,
 so $t$ is divisible by $a$, say $t=au$ for some $u$.  Similarly
 $t=bv$ for some $v$.  This means that 
 \[ t = 1.t = (xa+yb)t=xat+ybt=xa(bv)+yb(au)=(xv+yu)ab, \]
 so $t$ is divisible by $ab$.  Conversely, if $t$ is divisible by $ab$
 then it is divisible by both $a$ and $b$, so $\al(t)=(0,0)$.  Thus
 $\ker(\al)=Rab$.

 Now suppose we have some element $(u+aR,v+bR)\in R/a\tm R/b$.
 Consider the element $t=ybu+xav\in R$.  Note that
 $t=(1-xa)u+xav=u+xa(v-u)=u\pmod{a}$ and
 $t=ybu+(1-yb)v=v+yb(u-v)=v\pmod{b}$, so
 $\al(t)=(t+Ra,t+Rb)=(u+Ra,v+Rb)$.  This shows that $\al$ is
 surjective.  Thus, Theorem~\ref{thm-FIT-rings} says that
 $R/ab\simeq R/a\op R/b$.
\end{proof}

\begin{corollary}\lbl{cor-basic-split}
 Any finite torsion $R$-module is isomorphic to a direct sum of basic
 $R$-modules. 
\end{corollary}
\begin{proof}
 Let $M$ be a finite torsion $R$-module.  By Corollary~\ref{cor-class}
 we have $M\simeq R/d_1\op\ldots\op R/d_r\op R^s$ say.  If $s>0$ then
 $M$ contains a copy of $R$ so it cannot be a torsion module, contrary
 to assumption.  Thus we must have $s=0$ and so
 $M=R/d_1\op\ldots\op R/d_r$.  It will thus be enough to show that
 $R/d$ is a direct sum of basic modules for all $d\neq 0$.  For this
 we factor $d$ as $up_1^{n_1}\ldots p_r^{m_r}$ as in
 Theorem~\ref{thm-ufd}.  As $Rd=Rdu^{-1}$ we may replace $d$ by
 $du^{-1}$ and thus assume that $d=p_1^{n_1}\ldots p_r^{n_r}$.  Put
 $q_i=p_i^{n_i}$ and $b_i=q_iq_{i+1}\ldots q_r$, so $b_1=d$.  Note
 that $d=q_1b_2$ and $q_1$ and $b_2$ are coprime.  Thus
 Proposition~\ref{prop-chinese} tells us that
 $R/d\simeq R/q_1\op R/b_2$.  Similarly, $b_2=q_2b_3$ and $q_2$
 and $b_3$ are coprime so $R/b_2\simeq R/q_2\op R/b_3$ so
 $R/d=R/q_1\op R/q_2\op R/b_3$.  Continuing if the obvious way we find
 that $R/d=R/q_1\op\ldots\op R/q_r$, and the modules $R/q_i$ are
 basic, as required.
\end{proof}
\begin{example}\lbl{eg-finite-abelian}
 In the case $R=\Z$, we deduce that any finite Abelian group is a
 direct sum of groups of the form $\Z_{p^k}=\Z/p^k$ where $p$ is prime
 and $k>0$.  Suppose that $M=B_1\op\ldots\op B_t$ where
 $B_i\simeq\Z/p_i^{n_i}$.  (Note that the primes $p_i$ need not all be
 different.)  We then have
 \[ |M|=|B_1|\ldots|B_t|=p_1^{n_1}\ldots p_t^{n_t}. \]

 Consider for example the case where $|M|=81=3^4$.  Clearly all the
 primes $p_i$ must be equal to $3$, and
 $3^4=|M|=3^{n_1}\ldots 3^{n_t}=3^{n_1+\ldots n_t}$ so
 $n_1+\ldots+n_t=4$.  Given this, it is not hard to see that the
 possibilities are as follows:
 \begin{align*}
  M &\simeq \Z_{81} \\
  M &\simeq \Z_{27}\op \Z_3 \\
  M &\simeq \Z_9\op\Z_9 \\
  M &\simeq \Z_9\op\Z_3\op\Z_3 \\
  M &\simeq \Z_3\op\Z_3\op\Z_3\op\Z_3.
 \end{align*}

 Now consider instead the case where $|M|=36=2^2 3^2$.  Then all the
 primes $p_i$ must be either $2$ or $3$.  The orders $|B_i|=p_i^{n_i}$
 must divide $36$ so we must have $n_i=1$ or $2$.  Using this it is
 not hard to see that the possibilities are as follows:
 \begin{align*}
  M &\simeq \Z_4\op\Z_9 \\
  M &\simeq \Z_4\op\Z_3\op\Z_3 \\
  M &\simeq \Z_2\op\Z_2\op\Z_9 \\
  M &\simeq \Z_2\op\Z_2\op\Z_3\op\Z_3.
 \end{align*}
\end{example}

It remains to discuss the question of uniqueness.  Could it happen,
for example, that the groups $A:=\Z_9\op\Z_9$ and
$B:=\Z_9\tm\Z_3\tm\Z_3$ are isomorphic?  The answer is no, because $A$
contains $9$ elements satisfying $3a=0$, whereas $B$ contains $27$
elements satisfying $3b=0$.  An elaboration of this argument will show
that any finite torsion module can be written in an essentially unique
way as a direct sum of basic modules.

\begin{definition}\lbl{defn-f-p-k}
 Suppose that $p\in\CP$ and $k\in\N$.  For any finite torsion module
 $M$, we define
 \[ F_p^k(M) = \{ x \in p^{k-1}M \st px=0 \}. \]
 This is easily seen to be a submodule of $M$.  As $M$ is finitely
 generated, we see from Corollary~\ref{cor-Noether} that $F_p^k(M)$
 is finitely generated.  As $px=0$ for all $x\in F_p^k(M)$, we can
 regard $F_p^k(M)$ as a module over $R/p$.  As $R/p$ is a field, every
 finitely generated module over it has a well-defined dimension, so we
 can define
 \[ f_p^k(M) = \dim_{R/p}(F_p^k(M)). \]
 We also define
 \[ g_p^k(M) = f_p^k(M) - f_p^{k+1}(M). \]
\end{definition}
\begin{remark}\lbl{rem-f-p-k-sum}
 It is easy to see that a pair $(x,y)\in M\op N$ lies in
 $F_p^k(M\op N)$ if and only if $x\in F_p^k(M)$ and $y\in F_p^k(N)$.
 It follows that $F_p^k(M\op N)=F_p^k(M)\op F_p^k(N)$ and thus that
 $f_p^k(M\op N)=f_p^k(M)+f_p^k(N)$ and
 $g_p^k(M\op N)=g_p^k(M)+g_p^k(N)$.
\end{remark}
\begin{remark}\lbl{rem-f-p-k-iso}
 Suppose that $M\simeq M'$.  I claim that $F_p^k(M)\simeq F_p^k(M')$,
 and thus that $f_p^k(M)=f_p^k(M')$ and $g_p^k(M)=g_p^k(M')$.  Indeed,
 let $\phi\:M\xra{}M'$ be an isomorphism.  Then if $x\in F_p^k(M)$
 then $x=p^{k-1}y$ for some $y$ and $px=0$, so
 $\phi(x)=p^{k-1}\phi(y)$ and $p\phi(x)=0$, so $\phi(x)\in F_p^k(M')$.
 Thus, $\phi$ gives a homomorphism from $F_p^k(M)$ to $F_p^k(M')$.
 Similarly, the homomorphism $\phi^{-1}\:M'\xra{}M$ restricts to give
 a homomorphism from $F_p^k(M')$ to $F_p^k(M)$.  It is easy to see
 that these two maps are inverse to each other, so
 $F_p^k(M)\simeq F_p^k(M')$ as claimed.
\end{remark}

\begin{proposition}\lbl{prop-g-p-k-basic}
 We have 
 \[ g_p^k(R/q^j) = \begin{cases}
     1 & \text{ if } p=q \text{ and } k=j \\
     0 & \text{ otherwise. }
 \end{cases} \]
\end{proposition}
\begin{proof}
 We first prove that
 \[ f_p^k(R/q^j) = \begin{cases}
     0 & \text{ if } p\neq q \\
     0 & \text{ if } p=q \text{ and } k>j \\
     1 & \text{ if } p=q \text{ and } k\leq j.
 \end{cases} \]
 First suppose that $p\neq q$.  Then $p^k$ and $q^j$ are coprime, so
 $ap^k+bq^j=1$ for some $a,b\in R$.  If $x\in F_p^k(R/q^j)$ then
 $x=p^{k-1}y$ for some $y$ and $px=0$ so $p^ky=0$.  On the other hand,
 it is clear from the definition of $R/q^j$ that $q^jz=0$ for all
 $z\in R/q^j$, so $q^jy=0$.  We thus have $y=1.y=ap^ky+bq^jy=0$, and
 thus $x=p^{k-1}y=0$.  Thus $F_p^k(R/q^j)=0$ and so $f_p^k(R/q^j)=0$,
 as required.

 Now suppose that $q=p$ and $j<k$.  If $x\in R/p^j$ then $p^jx=0$ and
 $k-1\geq j$ so $p^{k-1}x=0$.  Thus $p^{k-1}.(R/p^j)=0$ and therefore
 $F_p^k(R/p^j)=0$ so $f_p^k(R/p^j)=0$.

 Now suppose instead that $q=p$ and $k\leq j$.  Put $e=p^{j-1}+p^jR\in
 R/p^j$, so clearly $pe=0$.  We also have $e=p^{k-1}e'$, where
 $e'=p^{j-k}+p^jR\in R/p^j$, so $e\in p^{k-1}(R/p^j)$, so
 $e\in F_p^k(R/p^j)$.  If $\ov{a}$ is another element of
 $F_p^k(R/p^j)$ then $p\ov{a}=0$ so $pa$ is a multiple of $p^j$ so $a$
 is a multiple of $p^{j-1}$ so $\ov{a}$ is a multiple of $e$.  As
 $e\neq 0$ and $e$ spans $F_p^k(R/p^j)$ we conclude that
 $F_p^k(R/p^j)$ has dimension one, so $f_p^k(R/p^j)=1$, as required.

 It is now easy to deduce our description of $g_p^k(R/q^j)$.  If
 $q\neq p$ then $f_p^k(R/q^j)=0$ for all $k$ and it follows easily
 that $g_p^k(R/q^j)=0$.  Suppose instead that $p=q$.  If $k>j$ then
 $k+1>j$ as well so $f_p^k(R/p^j)=f_p^{k+1}(R/p^j)=0$ so
 $g_p^k(R/p^j)=0$ as claimed.  If $k<j$ then both $k$ and $k+1$ are
 less than or equal to $j$, so $f_p^k(R/p^j)=f_p^{k+1}(R/p^j)=1$ so
 $g_p^k(R/p^j)=0$ as claimed.  If $k=j$ then $f_p^k(R/p^j)=1$ and
 $f_p^{k+1}(R/p^j)=0$ so $g_p^k(R/p^j)=1$ as claimed.
\end{proof}

\begin{corollary}\lbl{cor-primary}
 Let $M$ be an finite torsion module.  Then $M$ can be expressed in a
 unique way as a direct sum of basic modules.  The number of copies of
 $R/p^k$ in the direct sum is $g_p^k(M)$.
\end{corollary}
\begin{proof}
 We know from Corollary~\ref{cor-basic-split} that $M$ can be written
 as $B_1\op\ldots\op B_t$, where each $B_i$ is a basic module.  Let
 $n_p^k$ be the number of copies of $R/p^k$ in this list.  We know
 that $g_p^k(M)=g_p^k(B_1)+\ldots+g_p^k(B_t)$.  In this sum we get a
 $1$ for every $B_i$ that is a copy of $R/p^k$ and a $0$ for all other
 $B_i$'s.  Thus implies that $g_p^k(M)=n_p^k$.  

 Now suppose we have another splitting, say $M=C_1\op\ldots\op C_s$
 where each $C_j$ is basic.  Let $m_p^k$ be the number of copies of
 $R/p^k$ in this list.  The same argument as before shows that
 $g_p^k(M)=m_p^k$, so $m_p^k=n_p^k$.  Thus the lists $B_1,\ldots,B_t$
 and $C_1,\ldots,C_s$ contain the same number of copies of $R/p^k$ for
 all $p$ and $k$, so the two lists must be the same up to reordering.
 Thus, $M$ can be written in an essentially unique way as a direct sum
 of basic modules.
\end{proof}

%============================================================
%============================================================

\begin{center}
 \Large \textbf{Exercises}
\end{center}

%\ip{Cxdec0}
\begin{exercise}\exlabel{eg-decompose-i}
 Write the $\C[x]$-module $M:=\C[x]/(x^4-x^2)\op\C[x]/(x^4-2x^2+1)$ as a
 direct sum of basic $\C[x]$-modules.
\end{exercise}
\begin{solution}
 We have
 \begin{align*}
  x^4 - x^2 &= (x-1)(x+1)x^2 \\
  x^4-2x^2+1 &= (x-1)^2(x+1)^2 
 \end{align*}
 so 
 \begin{align*}
  \C[x]/(x^4-x^2) &\simeq \C[x]/(x-1) \op \C[x]/(x+1) \op \C[x]/x^2 \\
  \C[x]/(x^4-2x^2+1) &\simeq \C[x]/(x-1)^2 \op \C[x]/(x+1)^2 \\
  M &\simeq \C[x]/(x-1) \op \C[x]/(x-1)^2 \op \\
    &\hphantom{\simeq} \C[x]/(x+1) \op \C[x]/(x+1)^2 \op \\
    &\hphantom{\simeq} \C[x]x^2 \\
    &= B(1,1) \op B(1,2) \op B(-1,1) \op B(-1,2) \op B(0,2).
 \end{align*}
\end{solution}

%\ip{Fpklist}
\begin{exercise}\exlabel{ex-FpkM-calc}
 Let $M$ be the $\Z$-module $\Z_2\op\Z_4\op\Z_9$.  List all the
 elements of the subgroups $F_2^1(M)$, $F_2^2(M)$ and $F_3^1(M)$.
\end{exercise}
\begin{solution}
 The elements of $M$ have the form $(\ov{a},\ov{b},\ov{c})$ with
 $\ov{a}\in\Z_2=\{\ov{0},\ov{1}\}$ and
 $\ov{b}\in\Z_4=\{\ov{0},\ov{1},\ov{2},\ov{3}\}$ and
 $\ov{c}\in\Z_9=\{\ov{0},\ov{1},\ldots,\ov{8}\}$.  We have
 \begin{align*}
   F_2^1(M) &=
    \{(\ov{a},\ov{b},\ov{c}) \st
      2\ov{a}=\ov{0}\;,\;2\ov{b}=\ov{0}\;,\;2\ov{c}=\ov{0} \} \\
   &= \{(\ov{a},\ov{b},\ov{c}) \st 2 | 2a\;,\; 4|2b \text{ and } 9|2c\}.
 \end{align*}
 Clearly $2$ always divides $2a$, and $4$ divides $2b$ iff $b$ is
 even, and $9$ divides $2c$ iff $9$ divides $c$.  Thus $\ov{a}$ can be
 either element of $\Z_2$, $\ov{b}$ can be $\ov{0}$ or $\ov{2}$, and
 $\ov{c}$ must be $\ov{0}$.  Thus
 \[ F_2^1(M) = \{ (\ov{0},\ov{0},\ov{0}),
                  (\ov{0},\ov{2},\ov{0}),
                  (\ov{1},\ov{0},\ov{0}),
                  (\ov{1},\ov{2},\ov{0}) \}.
 \]
 The elements of $F_2^2(M)$ are the elements of $F_2^1(M)$ that have
 the form $2m$ for some $m\in M$.  Thus 
 \[ F_2^2(M) = \{ (\ov{0},\ov{0},\ov{0}), (\ov{0},\ov{2},\ov{0}) \}. \]
 By similar arguments we have  
 \[ F_3^1(M) = \{ (\ov{0},\ov{0},\ov{0}), 
                  (\ov{0},\ov{0},\ov{3}),
                  (\ov{0},\ov{0},\ov{6}) \}.
 \]
\end{solution}

%\ip{ab225}
\begin{exercise}\exlabel{ex-classify-CCV}
 \begin{itemize}
  \item[(a)] List all the Abelian groups of order $225$ up to
   isomorphism.  You should write all the groups as direct sums of
   basic $\Z$-modules.
  \item[(b)] Which of the groups in your list is isomorphic to
   $\Z_{225}$?
  \item[(c)] Let $M$ be an Abelian group of order $225$.  Suppose that
   there is an element in $M$ of order $25$, and that there are $9$
   elements $x\in M$ satisfying $3x=0$.  Which of the groups in your
   list is isomorphic to $M$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The prime factorisation of $225$ is $3^25^2$.  Thus any
   group of order $225$ is the direct sum of a group of order $3^2=9$
   and a group of order $5^2=25$.  The $3$-primary part could be
   $\Z_9$ or $\Z_3\op\Z_3$ , and the $5$-primary part could be
   $\Z_{25}$ or $\Z_5\op\Z_5$.  The possibilities are:
   \begin{align*}
    M_1 &= \Z_3\op\Z_3\op\Z_5\op\Z_5 \\
    M_2 &= \Z_9\op\Z_5\op\Z_5 \\
    M_3 &= \Z_3\op\Z_3\op\Z_{25} \\
    M_4 &= \Z_9\op\Z_{25}.
   \end{align*}
  \item[(b)] As $9$ and $25$ are coprime, The Chinese Remainder
   Theorem says that $\Z_{225}=\Z/(9\tm 25)\simeq\Z_9\op\Z_{25}=M_4$.
  \item[(c)] Only the groups $M_3$ and $M_4$ have any elements of
   order $25$.  The group $M_4$ has only $3$ elements satisfying
   $3x=0$, so we must have $M\simeq M_3$.
 \end{itemize}
\end{solution}

%\ip{ab10000}
\begin{exercise}\exlabel{ex-ten-thousand}
 Let $p$ be a prime number.
 \begin{itemize}
  \item[(a)] List all the Abelian groups of order $p^4$, up to
   isomorphism.
  \item[(b)] Let $M$ be an Abelian group of order $p^4$, and put
   $N=\{x\in M\st px=0\}$.  Suppose that $\dim_{\Z/p}N=3$.  Which of
   the groups in your list is isomorphic to $M$?
  \item[(c)] How many isomorphism classes of Abelian groups of order
   $10000$ are there?  You should justify your answer, but you need
   not list all the groups.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Any Abelian group $M$ of order $p^4$ can be written as a
   direct sum of groups of the form $\Z_{p^k}$ with $1\leq k\leq 4$.
   If $M\simeq\Z_{p^{k_1}}\op\ldots\Z_{p^{k_r}}$ then
   \[ p^4=|M|=p^{k_1}\tm\ldots\tm p^{k_r}=p^{k_1+\ldots+k_r}, \] 
   so $k_1+\ldots+k_r=4$.  As each $k_i$ is at least $1$ this means
   that $r\leq 4$.  Using this and some trial and error we see that
   the possibilities are as follows:
   \begin{align*}
    M_1 &= \Z_p\op\Z_p\op\Z_p\op\Z_p \\
    M_2 &= \Z_p\op\Z_p\op\Z_{p^2} \\
    M_3 &= \Z_p\op\Z_{p^3} \\
    M_4 &= \Z_{p^2}\op\Z_{p^2} \\
    M_5 &= \Z_{p^4}.
   \end{align*}
  \item[(b)] In the usual notation, the question tells us that
   $f^1_p(M)=3$.  It is a standard fact that $f^1_p(\Z_{p^k})=1$ for
   all $k\geq 1$, and that $f^1_p(A\op B)=f^1_p(A)+f^1_p(B)$.  Using
   this, we find that
   \begin{align*}
    f^1_p(M_1) &= 4 \\
    f^1_p(M_2) &= 3 \\
    f^1_p(M_3) &= 2 \\
    f^1_p(M_4) &= 2 \\
    f^1_p(M_5) &= 1.
   \end{align*}
   The only possibility is thus that $M\simeq M_2$.
  \item[(c)] We have $10000=2^4\tm 5^4$.  As $2$ and $5$ are coprime,
   any Abelian group of order $2^4\tm 5^4$ is the direct sum of a
   group of order $2^4$ and a group of order $5^4$, in a unique way.
   By part~(a), there are $5$ possibilities for the $2$-primary part
   and $5$ possibilities for the $5$-primary part, giving $5\tm 5=25$
   possible groups of order $10000$.
 \end{itemize}
\end{solution}

%\ip{abp5}
\begin{exercise}\exlabel{ex-p-fifth}
 \begin{itemize}
  \item[(a)] Given a prime number $p$, list all the isomorphism classes
   of Abelian groups of order $p^5$.
  \item[(b)] Let $M$ be an Abelian group of order $2^5$.  Suppose that
   $M$ has precisely $8$ elements satisfying $2m=0$, but that all
   elements satisfy $4m=0$.  Which of the groups in your list is
   isomorphic to $M$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)]
   The possibilities are as follows:
   \begin{align*}
    M_1 &= \Z_p\op\Z_p\op\Z_p\op\Z_p\op\Z_p \\
    M_2 &= \Z_{p^2}\op\Z_p\op\Z_p\op\Z_p \\
    M_3 &= \Z_{p^2}\op\Z_{p^2}\op\Z_p \\
    M_4 &= \Z_{p^3}\op\Z_p\op\Z_p \\
    M_5 &= \Z_{p^3}\op\Z_{p^2} \\
    M_5 &= \Z_{p^4}\op\Z_p \\
    M_6 &= \Z_{p^5}.
   \end{align*}
  \item[(b)] Take $p=2$.  The groups $M_4,\ldots,M_6$ all contain
   elements of order $p^3=8$, but all elements in $M$ have $4m=0$, so
   we must have $M\simeq M_i$ for some $i\leq 3$.  By assumption we
   have $|F_2^1(M)|=8=2^3$ so $f_2^1(M)=\dim_{\Z_2}F_2^1(M)=3$.
   However, we know that $f_p^k(\Z_{p^j})=1$ for all $j>0$ so
   $f_p^1(M_1)=5$, $f_p^1(M_2)=4$ and $f_p^1(M_3)=3$.  We must
   therefore have $M\simeq M_3$.
 \end{itemize} 
\end{solution}

%\ip{diffdec}
\begin{exercise}\exlabel{ex-diffop-basis}
 Suppose that $f\in\CRR$ satisfies $f'''=f'$.  Find elements
 $e_0,e_1,e_{-1}\in\R[D]$ such that 
 \begin{align*}
  f &= e_{-1}f + e_0f + e_1f \\
  (D-1)e_{-1}f &= 0 \\
  D e_0f &= 0 \\
  (D+1)e_1 f &= 0. 
 \end{align*}
\end{exercise}
\begin{solution}
 First note that $(D-1)D(D+1)f=(D^3-D)f=f'''-f'=0$.  Suppose we take
 $e_{-1}=a_{-1}D(D+1)$ for some $a_{-1}\in\R$.  Then
 $(D-1)e_{-1}f=a_{-1}(D-1)D(D+1)f=0$, as required.  Similarly, if we
 take $e_0=a_0(D-1)(D+1)$ and $e_1=a_1(D-1)D$ then we will have
 $De_0f=0$ and $(D+1)e_1f=0$.

 If we can arrange that $e_{-1}+e_0+e_1=1$ then the remaining
 condition $f=e_{-1}f+e_0f+e_1f$ will clearly also be satisfied.  We
 have
 \begin{align*}
  e_{-1}+e_0+e_1 &= a_{-1}(D^2+D) + a_0(D^2-1) + a_1(D^2-D) \\
   &= -a_0 +  (a_{-1}-a_1)D + (a_{-1}+a_0+a_1)D^2.
 \end{align*}
 For this to equal $1$, we must have $a_0=-1$ and $a_1=a_{-1}$ and
 $a_{-1}+a_0+a_1=0$, so $a_1=a_{-1}=1/2$.  Thus
 \begin{align*}
  e_{-1} &= (D^2+D)/2 \\
  e_0    &= 1-D^2 \\
  e_1    &= (D^2-D)/2. 
 \end{align*}
\end{solution}

%\ip{diffeq}
\begin{exercise}\exlabel{ex-diffop-solve}
 In this problem we solve the differential equation $f'''=f'$.
 \begin{itemize}
  \item[(a)] Write the equation $f'''=f'$ in the form $p(D)f=0$ for
   some polynomial $p$.
  \item[(b)] Put $M=\{f\in\CRR\st f'''=f'\}$.  Factorise $p(D)$ and
   thus write $M$ as a direct sum of three submodules.
  \item[(c)] Show that if $f'''=f'$ then there are constants
   $a,b,c\in\R$ such that $f(t)=ae^t+be^{-t}+c$ for all $t$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] $p(D)=D^3-D$.
  \item[(b)] Note that $p(D)=(D-1)(D+1)D$ and all the factors are
   coprime to each other and $p(D)M=\{0\}$.  It follows that if we put
   \begin{align*}
    M_0 &= \{f\in\CRR\st (D-1)f=0\} = \{f \st f'=f\} \\
    M_1 &= \{f\in\CRR\st (D+1)f=0\} = \{f \st f'=-f\}\\
    M_2 &= \{f\in\CRR\st Df=0\} = \{f \st f'=0\}
   \end{align*}
   then $M=M_0\op M_1\op M_2$.
  \item[(c)] By standard methods we have 
   \[ 
    M_0 = \{ae^t\st a\in\R\} \hspace{4em}
    M_1 = \{be^{-t}\st b\in\R\} \hspace{4em}
    M_2 = \R.
   \]
   As $M=M_0\op M_1\op M_2$, any function $f$ satisfying $f'''=f'$ can
   be written as $f=f_0+f_1+f_2$ with $f_i\in M_i$, or equivalently as
   $ae^t+be^{-t}+c$ for some $a,b,c\in\R$.
 \end{itemize}
\end{solution}

%\ip{difftor}
\begin{exercise}\exlabel{ex-diffop-torsion}
 Consider $\CRC$ as a module over $\C[D]$.  Let $W$ be the space of
 functions of the form $p_1(t)e^{\lm_1t}+\ldots+p_r(t)e^{\lm_rt}$ for
 some $\lm_1,\ldots,\lm_r\in\C$ and polynomials
 $p_1,\ldots,p_r\in\C[t]$.  In this (quite substantial) problem we
 will show that $\tors(\CRC)=W$.
 \begin{itemize}
  \item[(a)] For each $\lm\in\C$, let $W_\lm$ be the space of
   functions of the form $f(t)=p(t)e^{\lm t}$, where $p$ is
   polynomial.  Calculate $(D-\lm)^kf$, and deduce that $W_\lm$ is a
   torsion module over $\C[D]$.
  \item[(b)] Suppose that $f\in\CRC$ and that $(D-\lm)^kf=0$ for some
   $k\geq 0$.  Prove that the function $g(t):=f(t)e^{-\lm t}$
   satisfies $D^kg=0$.
  \item[(c)] Suppose that a function $g\in\CRC$ satisfies $D^kg=0$.
   Prove by induction on $k$ that $g$ is a polynomial of degree less
   than $k$.  
  \item[(d)] Deduce that every function $f$ with $(D-\lm)^kf=0$ lies
   in $W_\lm$.
  \item[(e)] Suppose that $f\in \CRC$ and that $p(D)f=0$ for some
   nonzero element $p(D)\in\C[D]$.  By factoring $p(D)$ and
   considering the module $\{g\in\CRC\st p(D)g=0\}$, show that
   $f\in W_{\lm_1}+\ldots+W_{\lm_r}$ for some $r$.
  \item[(f)] Deduce that $\tors(\CRC)=W$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We have
   $f'(t)=p'(t)e^{\lm t}+\lm p(t)e^{\lm t}=p'(t)e^{\lm t}+\lm f(t)$.
   Thus $((D-\lm)f)(t)=p'(t)e^{\lm t}$.  It follows inductively that
   $((D-\lm)^kf)(t)=p^{(k)}(t)e^{\lm t}$ for all $k\geq 0$.  If $p$ is
   a polynomial of degree $d$ then $p^{(k)}$ is a polynomial of degree
   $d-k$ for $k=1,\ldots,d$, and $p^{(k)}=0$ for $k>d$.  It follows
   that $(D-\lm)^kf=0$ for $k>d$.  This means that $f$ is a torsion
   element of $W_\lm$.  This holds for every element of $W_\lm$, so
   $W_\lm$ is a torsion module.
  \item[(b)] We have
   \[ g'(t)=f'(t)e^{-\lm t}-\lm f(t)e^{-\lm t}=((D-\lm)f)(t)e^{-\lm t}.
   \]
   By extending this inductively, we find that
   $g^{(k)}(t)=((D-\lm)^kf)(t)e^{-\lm t}=0$, or in other words
   $D^kg=0$.
  \item[(c)] In the case $k=1$ we have $Dg=0$ so $g$ is constant and
   so certainly polynomial of degree less than $1$.

   Suppose we have shown that all functions with $D^{k-1}g=0$ are
   polynomial of degree less than $k-1$.  If $D^kf=0$ then
   $D^{k-1}f'=0$ so $f'$ is polynomial, say
   $f'(t)=a_0+\ldots+a_{k-2}t^{k-2}$.  Put 
   \[ F(t)=f(t)-f(0)=\int_{s=0}^t f(t) = 
       a_0t+\ldots+a_{k-2}t^{k-1}/(k-1),
   \]
   which is clearly a polynomial of degree less than $k$.  As $f(0)$
   is just a constant we deduce that $f(t)=F(t)+f(0)$ is also a
   polynomial of degree less than $k$.
  \item[(d)] If $(D-\lm)^kf=0$ then the function $g(t)=f(t)e^{-\lm t}$
   satisfies $D^kg=0$, so $g$ is a polynomial, so $f(t)=g(t)e^{\lm t}$
   is an element of $W_\lm$.
  \item[(e)] Suppose that $p(D)f=0$.  We can factor $p(D)$ as
   $u(D-\lm_1)^{k_1}\ldots(D-\lm_r)^{k_r}$ for some nonzero constant
   $u$, where the $\lm_i$'s are all different and the $k_i$'s are all
   strictly positive.  It follows that the terms $(x-\lm_i)^{k_i}$ are
   all coprime to each other.  If we put $V=\{f\st p(D)f=0\}$ and
   $V_i=\{f\st (D-\lm_i)^{k_i}f=0\}$, it follows that
   $V=V_1\op\ldots\op V_r$.  However, we know from~(d) that
   $V_i\sse W_{\lm_i}$.  Thus
   \[ f \in V_1 + \ldots + V_r \sse W_{\lm_1} + \ldots + W_{\lm_r}. \]
   It follows immediately from the definitions that $f\in W$.
  \item[(f)] Suppose that $f\in\tors(\CRC)$.  Then $p(D)f=0$ for some
   nonzero element $p(D)\in\C[D]$, so~(e) tells us that $f\in W$.
   Conversely, if $f\in W$ then we can write
   $f(t)=p_1(t)e^{\lm_1t}+\ldots+p_r(t)e^{\lm_rt}$ for some
   $\lm_1,\ldots,\lm_r\in\C$ and polynomials
   $p_1,\ldots,p_r\in\C[t]$.  If we put $f_i(t)=p_i(t)e^{\lm_it}$ then
   $f_i\in W_{\lm_i}$, so $f_i$ is a torsion element by~(a).  Thus
   $f=f_1+\ldots+f_r$ is a sum of torsion elements and thus is a
   torsion element, as claimed.  More concretely, if the degree of
   $p_i$ is $k_i-1$ then the element
   $q(D)=(D-\lm_1)^{k_1}\ldots(D-\lm_r)^{k_r}$ is nonzero and
   satisfies $q(D)f=0$. 
 \end{itemize}
\end{solution}

%\ip{xxx}
\begin{exercise}\exlabel{ex-matrix-monomials}
 Consider the following matrix over $\C[x]$:
 \[ A = \bbm x^3 & x^2 & x \\ x & x^2 & x \\ x & x & x \ebm. \]
 Let $M$ be the quotient of $\C[x]^3$ by the span of the columns of
 $A$. 
 \begin{itemize}
  \item[(a)] Reduce $A$ to normal form by row and column operations.
  \item[(b)] Give a list of three cyclic $\C[x]$-modules whose direct
   sum is isomorphic to $M$.
  \item[(c)] Gve a list of basic $\C[x]$-modules whose direct sum is
   isomorphic to $M$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)]
   In step $1$ we subtract $x^2$ times the last column from the first
   column, and subtract $x$ times the last column from the second
   column.  In step $2$ we swap the first and last columns.  In step
   $3$ we subtract the middle row from the bottom row, and then
   subtract the top row from the middle row.  Finally, we swap the
   middle row and the bottom row.
   \begin{align*}
    \bbm  x^3 & x^2 & x \\ x & x^2 & x \\ x & x & x \ebm &\xra{1}
      \bbm  0 & 0 & x \\ x-x^3&0&x \\ x-x^3&x-x^2&x \ebm \xra{2}
      \bbm  x & 0 & 0 \\ x&0&x-x^3 \\ x&x-x^2&x-x^3 \ebm \\
      &\xra{3}
      \bbm  x & 0 & 0 \\ 0&0&x-x^3 \\ 0&x-x^2&0 \ebm \xra{4}
      \bbm  x & 0 & 0 \\ 0&x-x^2&0 \\ 0&0&x-x^3 \ebm.
   \end{align*}
   Note that $x-x^2=x(1-x)$ and $x-x^3=x(1-x)(1+x)$, so
   $x|(x-x^2)|(x-x^3)$, so the last matrix is in normal form.
  \item[(b)] It follows that 
   \[ M\simeq \C[x]/x\op \C[x]/(x-x^2) \op \C[x]/(x-x^3). \]
  \item[(c)] The Chinese Remainder Theorem implies that
   \begin{align*}
    \C[x]/(x-x^2) &\simeq \C[x]/x \op \C[x]/(x-1) \\
                  &\simeq B(0,1) \op B(1,1) \\
    \C[x]/(x-x^3) &\simeq \C[x]/x \op \C[x]/(x-1) \op \C[x]/(x+1) \\
                  &\simeq B(0,1) \op B(1,1) \op B(-1,1).
   \end{align*}
   Thus 
   \begin{align*}
    M &\simeq \C[x]/x\op \C[x]/(x-x^2) \op \C[x]/(x-x^3) \\
      &\simeq B(0,1) \op (B(0,1)\op B(1,1)) \op 
              (B(0,1) \op B(1,1) \op B(-1,1)) \\
      &\simeq B(0,1)^3 \op B(1,1)^2 \op B(-1,1).
   \end{align*}
 \end{itemize}
\end{solution}

\section{Canonical forms for square matrices }
\label{sec-jordan}

Given any $n\tm n$ matrix $A$ over a field $K$ we have a module $M_A$
over $K[x]$.  We see from Example~\ref{eg-fin-dim-torsion} that $M_A$
is a finite torsion module, and Corollary~\ref{cor-primary} gives a
classification of such modules.  In this section, we will see what
this tells us about square matrices. 

For simplicity, we will restrict attention to the case $K=\C$, where
the irreducibles are easy to understand.  As we saw in
Example~\ref{eg-complete-set}, the set
\[ \CP = \{x-\lm\st \lm\in\C\} \]
is a complete set of irreducibles in $\C[x]$.  The basic $\C[x]$
modules are thus the modules
\[ B(\lm,k) := B_{x-\lm}^k = \C[x]/(x-\lm)^k. \]
Thus Corollary~\ref{cor-primary} says that any finite torsion module
over $\C[x]$ can be written essentially uniquely as a direct sum of
$B(\lm,k)$'s, in an essentially unique way.  

The next proposition explains the most basic case of this.  Recall
that $M_\lm$ is the module whose elements are the complex numbers,
with the multiplication rule $f.z=f(\lm)z$.
\begin{proposition}\lbl{prop-B-lm-one}
 $B(\lm,1)\simeq M_\lm$.
\end{proposition}
\begin{proof}
 Define a map $\al\:\C[x]\xra{}M_\lm$ by $\al(f)=f(\lm)$.  This is a
 $\C[x]$-module map because 
 \[ \al(g.f)=(gf)(\lm)=g(\lm)f(\lm)=g.f(\lm)=g.\al(f). \]
 If $a$ is a constant polynomial then $\al(a)=a$, and this shows that
 $\al$ is surjective.  We also have $\al(f)=0$ iff $f(\lm)=0$ iff
 $f(x)$ is divisible by $x-\lm$, so $\ker(\al)$ is the principal ideal
 $\C[x].(x-\lm)$.  Thus, the first isomorphism theorem gives us an
 isomorphism $\alb\:B(\lm,1)=\C[x]/(x-\lm)\xra{}M_\lm$.
\end{proof}

Now suppose that $A$ is a diagonalizable $n\tm n$ matrix over $\C$,
with eigenvalues $\lm_1,\ldots,\lm_n$ say.  Then there exists a matrix
$P$ (whose columns are eigenvectors of $A$) such that $D:=P^{-1}AP$ is
a diagonal matrix, with entries $\lm_1,\ldots,\lm_n$ on the diagonal.
Recall from Example~\ref{eg-block-sum} that the direct sum of matrices
is defined by 
\[ A\op B=\blockmat{A}{0}{0}{B}.\]
If we regard $\lm_i$ as a $1\tm 1$ matrix and use this notation, we
find that $D=\lm_1\op\ldots\op\lm_n$.

We saw in Corollary~\ref{cor-semisimple} that 
$M_A\simeq M_{\lm_1}\op\ldots\op M_{\lm_n}$, and we can now rewrite
this as
\[ M_A\simeq B(\lm_1,1)\op\ldots\op B(\lm_n,1). \]

To extend this picture to nondiagonalizable matrices, we need the
following definition.
\begin{definition}\lbl{defn-jordan-block}
 Given $\lm\in\C$ and $k>0$ we let $J(\lm,k)$ be the $k\tm k$ matrix
 such that
 \begin{enumerate}
 \item Every entry on the diagonal is $\lm$
 \item Every entry just below the diagonal is $1$
 \item Every other entry is $0$.
 \end{enumerate}
 This is called a \emph{Jordan block} of size $k$ and eigenvalue
 $\lm$.  For example, we have
 \[ J(\lm,4) = \left(\begin{array}{cccc}
     \lm &   0 &   0 &   0 \\
       1 & \lm &   0 &   0 \\
       0 &   1 & \lm &   0 \\
       0 &   0 &   1 & \lm
    \end{array}\right).
 \]
\end{definition}

\begin{proposition}\lbl{prop-B-J}
 $B(\lm,k)$ is isomorphic to $M_{J(\lm,k)}$.
\end{proposition}
\begin{proof}
 Put $y=x-\lm\in\C[x]$, so that $B(\lm,k)=\C[x]/y^k$.  Put
 $A=J(\lm,k)-\lm I$, so for $v\in M_{J(\lm,k)}=\C^k$ we have
 $y.v=J(\lm,k)v-\lm v=Av$.  From the definition of $J(\lm,k)$ we see
 that $A$ has $1$'s just below the diagonal and $0$'s everywhere
 else.  In the case $k=4$ we have
 \[ y.v = Av =
         \bbm 0&0&0&0 \\ 1&0&0&0 \\ 0&1&0&0 \\ 0&0&1&0 \ebm
         \bbm v_1 \\ v_2 \\ v_3 \\ v_4 \ebm =
         \bbm 0   \\ v_1 \\ v_2 \\ v_3 \ebm.
 \] 
 It is not hard to see that the general case follows the same pattern,
 so we have 
 \[ y.(v_1,\ldots,v_k)=(0,v_1,\ldots,v_{k-1}). \] 
 It follows that $y^2.(v_1,\ldots,v_k)=(0,0,v_1,\ldots,v_{k-2})$ and
 so on, so $y^{k-1}.(v_1,\ldots,v_k)=(0,\ldots,0,v_1)$ and $y^kv=0$.

 Let $\{e_1,\ldots,e_k\}$ be the usual basis for $M_{J(\lm,k)}=\C^k$
 over $\C$, so $y.e_i=e_{i+1}$ for $i<k$ and $y.e_k=0$.  We define
 $\al\:\C[x]\xra{}M_{J(\lm,k)}$ by $\al(f)=f.e_1=f(J(\lm,k))e_1$.
 This is easily seen to be a map of $\C[x]$-modules.

 I next claim that $\al$ is surjective.  Indeed, for any
 $v=(v_1,\ldots,v_k)\in M_{J(\lm,k)}$ we can put
 $f=v_1+v_2y+\ldots+v_ky^{k-1}\in\C[x]$ and we find that 
 \[ \al(f)=f.e_1=v_1e_1+v_2y.e_1+\ldots+v_ky^{k-1}.e_k =
     v_1e_1+\ldots+v_ke_k = v,
 \]
 which proves surjectivity.

 We next claim that $\ker(\al)=\C[x].y^k$.  Indeed, suppose we have
 some polynomial $f(x)$ with $\al(f)=f.e_1=0$.  By putting $x=y+\lm$
 and expanding everything out, we can write $f(x)$ in the form
 $a_0+a_1y+\ldots+a_dy^d$.  (For example, if $f(x)=x^2+x+1$ then
 $f(x)=(y+\lm)^2+(y+\lm)+1=(1+\lm+\lm^2)+(1+2\lm)y+y^2$.)  We then
 have 
 \[ f.e_1 = a_0e_1+\ldots+a_{k-1}y^{k-1}.e_1 = 
            a_0e_1+\ldots+a_{k-1}e_k = 
            (a_0,a_1,\ldots,a_{k-1}).
 \]
 As $f.e_1=0$ we must have $a_0=\ldots=a_{k-1}=0$ so
 $f(x)=a_ky^k+\ldots+a_dy^d$, so $f(x)$ is divisible by $y^k$ as
 required.

 The first isomorphism theorem now tells us that
 \[ M_{J(\lm,k)}=\img(\al)\simeq \C[x]/\ker(\al)=\C[x]/y^k=B(\lm,k) \]
 as claimed.
\end{proof}

\begin{theorem}\lbl{thm-jordan}
 Any square matrix $A$ over $\C$ is conjugate to a matrix $A'$ (called
 the \emph{Jordan normal form} or \emph{JNF} of $A$) that is a a
 direct sum of Jordan blocks.
\end{theorem}
\begin{proof}
 We know from Corollary~\ref{cor-primary} that $M_A$ is isomorphic to
 a direct sum of modules of the form $B(\lm,k)$, say
 \[ M_A\simeq B(\lm_1,k_1)\op\ldots\op B(\lm_t,k_t). \]
 Put $A'=J(\lm_1,k_1)\op\ldots\op J(\lm_t,k_t)$, so
 \[ M_{A'}\simeq M_{J(\lm_1,k_1)}\op\ldots\op M_{J(\lm_t,k_t)}
          \simeq B(\lm_1,k_1)\op\ldots\op B(\lm_t,k_t)\simeq M_A.
 \]
 As $M_A\simeq M_{A'}$, Proposition~\ref{prop-conj-iso} tells us that
 $A$ is conjugate to $A'$, as claimed.
\end{proof}

The row-reduction algorithm described previously can be used write
$M_A$ as a direct sum of cyclic modules, or in other words modules of
the form $\C[x]/f(x)$.  If we factor $f(x)$ as
$(x-\lm_1)^{k_1}\ldots(x-\lm_r)^{k_r}$ (with all the $\lm$'s
different) we see from Proposition~\ref{prop-chinese} that
\[ \C[x]/f(x) \simeq
   \C[x]/(x-\lm_1)^{k_1} \op \ldots \C[x]/(x-\lm_r)^{k_r} =
   B(\lm_1,k_1) \op \ldots \op B(\lm_r,k_r).
\]
We can use this to give an explicit expression for $M_A$ as a direct
sum of $B(\lm,k)$'s and thus to determine the JNF of $A$.

A different, and usually easier, approach is to use
Corollary~\ref{cor-primary}, which tells us that the number of copies
of $B(\lm,k)$ in the decomposition of $M_A$ is just
$g_{x-\lm}^k(M_A)$, as defined in Definition~\ref{defn-f-p-k}.  To
calculate these numbers $g_{x-\lm}^k(M_A)$, we need to recall the
definition of the characteristic polynomial and minimal polynomial of
a matrix.

\begin{definition}\lbl{defn-char-poly}
 The \emph{characteristic polynomial} $\chr(A)$ of a square matrix
 $A$ is the polynomial $\det(tI-A)$.
\end{definition}
Note that this is easy to calculate directly from $A$; for example, if
\[ A = \bbm 1&2&3 \\ 4&5&6 \\ 7&8&9 \ebm \]
then the characteristic polynomial is
\begin{align*}
 \left|\begin{array}{ccc}
  t-1&-2&-3 \\ -4&t-5&-6 \\ -7&-8&t-9
 \end{array}\right| &=
    (t-1) \left|\begin{array}{cc} t-5&-6\\-8&t-9\end{array}\right| -
    (-2)  \left|\begin{array}{cc} -4&-6 \\ -7&t-9 \end{array}\right| +
    (-3)  \left|\begin{array}{cc} -4&t-5\\ -7&-8 \end{array}\right|\\
 &= (t-1)((t-5)(t-9)-48) + 2(-4(t-9)-42) -3(32+7(t-5)) \\
 &= t^3 - 15 t^2 -18 t.
\end{align*}
It is also not hard to see that for any $n\tm n$ matrix $A$ we have
$\chr(A)=t^n+\text{ lower terms }$; in other words, $\chr(A)$ is a
monic polynomial of degree $n$.

We now define the minimal polynomial of a square matrix $A$.  First,
we put $I=\{f\in\C[x]\st f(A)=0\}$.  Clearly if $f(A)=g(A)=0$ and $h$
is arbitrary then $(f+g)(A)=f(A)+g(A)=0$ and $(hf)(A)=h(A)f(A)=0$, so
$I$ is an ideal.  By Theorem~\ref{thm-pid}, we see that $I=\C[x]g$ for
some polynomial $g$.  I next claim that $I$ is never the zero ideal.
Indeed, the set of all $n\tm n$-matrices over $\C$ is a vector space
over $\C$ of dimension $n^2$, so any list of $n^2+1$ such matrices
must be linearly dependent.  In particular, the list
$I,A,\ldots,A^{n^2}$ is linearly dependent, so there is some list
$a_0,\ldots,a_{n^2}$ (not all zero) such that
$a_0I+a_1A+\ldots+a_{n^2}A^{n^2}=0$.  If we put
$f(x)=a_0+a_1x+\ldots+a_{n_2}x^{n^2}$ we find that $f\neq 0$ and 
$f\in I$, so $I\neq 0$.  As $I=\C[x]g$, we deduce that $g\neq 0$.
After multiplying $g$ by a nonzero constant, we may assume that $g$ is
a monic polynomial.  This justifies the following definition:
\begin{definition}\lbl{defn-min-poly}
 The \emph{minimal polynomial} $\min(A)$ of a square matrix $A$ is the
 unique monic polynomial that generates the ideal
 $I_A=\{f\in\C[x]\st f(A)=0\}$.
\end{definition}

\begin{theorem}\lbl{thm-char-min}
 Let $A$ be an $n\tm n$ matrix, with
 $\chr(A)=\prod_{i=1}^r(t-\lm_i)^{r_i}$, where all the $\lm_i$'s are
 distinct and $r_i>0$.  Then
 \begin{itemize}
  \item[(a)] We have $\min(A)=\prod_{i=1}^r(t-\lm_i)^{s_i}$, where
   $0<s_i\leq r_i$.  In other words, the roots of $\min(A)$ are
   precisely the same as the roots of $\chr(A)$, and the
   multiplicities of the roots in $\min(A)$ are at most as large as
   the multiplicities in $\chr(A)$.
  \item[(b)] The JNF of $A$ contains only blocks of the form
   $J(\lm_i,k)$, where $\lm_i$ is a root of the characteristic
   polynomial, as before.  The number of such blocks is
   $\dim(\ker(A-\lm_iI))=n-\rank(A-\lm_iI)$.  The maximum value of $k$
   that occurs with $\lm_i$ is precisely $s_i$, and the sum of all the
   $k$'s that occur with $\lm_i$ is $r_i$.
 \end{itemize}
\end{theorem}
\begin{remark}\lbl{rem-cayley-hamilton}
 In particular, part~(a) says that $\min(A)(t)$ divides $\chr(A)(t)$,
 so $\chr(A)(t)\in I_A$, so if we substitute the matrix $A$ into its
 characteristic polynomial (in other words, evaluate $\chr(A)(A)$) we
 get the zero matrix.  This is the \emph{Cayley-Hamilton Theorem}.
\end{remark}

The proof will follow after some examples and preliminary results.
\begin{example}\lbl{eg-JNF-i}
 Consider the following matrix over $\C$:
 \[ A = \bbm 2&3&4\\0&2&3\\0&0&2 \ebm. \]
 The characteristic polynomial is 
 \[ \chr(A) = \det\bbm t-2&-3&-4 \\ 0&t-2&-3 \\ 0&0&t-2 \ebm 
            = (t-2)^3.
 \]
 Here we have used the fact that if a matrix has zeros everywhere
 below the diagonal, then the determinant is just the product of the
 entries on the diagonal.  It follows from the theorem that the
 minimal polynomial must be $(t-2)$ or $(t-2)^2$ or $(t-2)^3$.
 However, we find that 
 \[ A-2I     = \bbm 0&3&4\\0&0&3\\0&0&0\ebm \hspace{5em}
    (A-2I)^2 = \bbm 0&0&9\\0&0&0\\0&0&0\ebm.
 \]
 If the minimal polynomial is $f(t)$, we must have $f(A)=0$.  Thus,
 the above shows that $\min(A)\neq t-2$ and $\min(A)\neq(t-2)^2$, so
 $\min(A)$ must be $(t-2)^3$.

 We also see that the JNF of $A$ can only contain blocks of the form
 $J(2,k)$, so it must have the form $J(2,k_1)\op\ldots\op J(2,k_r)$,
 and we may as well order the factors so that $0<k_1\leq\ldots\leq
 k_r$.  By comparing characteristic polynomials we see that
 $(t-2)^3=\chr(A)=(t-2)^{k_1+\ldots+k_r}$.  By comparing minimal
 polynomials (and noting that $\max(k_1,\ldots,k_r)=k_r$) we see that
 $(t-2)^3=\min(A)=(t-2)^{k_r}$.  Thus $3=k_1+\ldots+k_r=k_r$.  As all
 the $k_i$'s are supposed to be positive, this can only work if $r=1$
 and $k_1=3$.  Thus the JNF of $A$ is just the single Jordan block
 $J(2,3)$, and thus $M_A\simeq B(2,3)=\C[x]/(x-2)^3$.

 For an alternative approach, we can observe from our formula for
 $A-2I$ that $\rank(A-2I)=2$ and so $\dim(\ker(A-2I))=3-2=1$.
 Part~(b) of Theorem~\ref{thm-char-min} therefore tells us that there
 is only one block in the JNF, so $A$ is conjugate to $J(2,k)$ for
 some $k$.  As $A$ is a $3\tm 3$ matrix, we must have $k=3$.
\end{example}
\begin{example}\lbl{eg-JNF-ii}
 Consider the following matrix over $\C$:
 \[ A = \bbm 2 & i & i & 2 \\
             i & 0 & 0 & i \\
             i & 0 & 0 & i \\
             2 & i & i & 2 \ebm.
 \]
 The characteristic polynomial is the determinant of the first matrix
 shown below:
 \[ \bbm t-2 & -i & -i & -2 \\
          -i &  t &  0 & -i \\
          -i &  0 &  t & -i \\
          -2 & -i & -i & t-2 \ebm \xra{}
    \bbm   t &  0 &  0 & -t \\
           0 &  t & -t &  0 \\
          -i &  0 &  t & -i \\
          -2 & -i & -i & t-2 \ebm \xra{}
    \bbm   t &  0 &  0 &  0 \\
           0 &  t &  0 &  0 \\
          -i &  0 &  t & -2i\\
          -2 & -i & -2i& t-4 \ebm.
 \]
 It is perfectly possible to evaluate the determinant directly, but it
 is more efficient to perform some row and column operations first, as
 shown above.  In the first step we have subtracted the fourth row
 from the first row and the third row from the second row.  In the
 second step we have added the first column to the fourth column and
 the second column to the third column.  None of these operations
 change the determinant.  The determinant of our final matrix is $t^2$
 times the determinant of the $2\tm 2$ block in the bottom right
 corner, which is $t^2-4t+4$.  Thus
 $\det(tI-A)=t^2(t^2-4t+4)=t^2(t-2)^2$.  The factor $(t-2)^2$ comes
 from some Jordan blocks of the form $J(2,k)$, each of which
 contributes a factor $(t-2)^k$.  The only possibilities
 are $J(2,2)$ and $J(2,1)\op J(2,1)$.  Similarly, the factor $t^2$
 comes from $J(0,2)$ or $J(0,1)\op J(0,1)$.  This gives four
 possibilities for the JNF of $A$; we list these below,
 together with their minimal polynomials.  
 \[ \begin{array}{rcl}
   J(0,1)\op J(0,1)\op J(2,1)\op J(2,1) && t(t-2) \\
   J(0,2)\op J(2,1)\op J(2,1)           && t^2(t-2) \\
   J(0,1)\op J(0,1)\op J(2,2)           && t(t-2)^2 \\
   J(0,2)\op J(2,2) & \hspace{5em}       & t^2(t-2)^2.
  \end{array}
 \]

 The minimal polynomial of $A$ must be one of the polynomials in the
 list above.  By direct calculation, we find that
 \begin{align*}
  A(A-2I)   &= 2 \bbm 1&i&i&1\\i&-1&-1&i\\i&-1&-1&i\\1&i&i&1\ebm \\
  A^2(A-2I) &= 4 \bbm 1&i&i&1\\i&-1&-1&i\\i&-1&-1&i\\1&i&i&1\ebm \\
  A(A-2I)^2 &= A^2(A-2I)^2 = 0.
 \end{align*}
 It follows that the minimal polynomial of $A$ is $t(t-2)^2$.  By
 comparing this with the list, we deduce that the JNF of $A$ must be
 $J(0,1)\op J(0,1)\op J(2,2)$.
\end{example}
\begin{example}\lbl{eg-need-rank}
 Consider the following matrix over $\C$:
 \[ A = \bbm 0  & 0 & 1 & 1 \\
             -1 & 1 & 1 & 0 \\
             -1 & 0 & 2 & 1 \\
             0  & 0 & 0 & 1 \ebm.
 \]
 Here we will just calculate the characteristic polynomial directly by
 expanding along the top row, although more efficient methods are
 certainly possible.  We have
 \begin{align*}
  \left| \begin{array}{cccc}
   t & 0 & -1 & -1 \\ 
   1 & t-1 & -1 & 0 \\
   1 & 0 & t-2 & -1 \\
   0 & 0 & 0 & t-1 
  \end{array} \right| &=
  t \left|\begin{array}{ccc}
     t-1&-1&0\\0&t-2&-1\\0&0&t-1
    \end{array}\right| -
  0 \left|\begin{array}{ccc}
     1&-1&0\\ 1&t-2&-1 \\ 0&0&t-1 
    \end{array}\right| + \\
  & \hspace{3em}
  (-1)\left|\begin{array}{ccc}
     1&t-1&0\\ 1&0&-1 \\ 0&0&t-1 
    \end{array}\right| -
  (-1)\left|\begin{array}{ccc}
     1&t-1&-1\\ 1&0&t-2 \\ 0&0&0 
    \end{array}\right|  \\
    \left|\begin{array}{ccc}
     t-1&-1&0\\0&t-2&-1\\0&0&t-1
    \end{array}\right| &= (t-1)^2(t-2) \\
    \left|\begin{array}{ccc}
     1&t-1&0\\ 1&0&-1 \\ 0&0&t-1 
    \end{array}\right| &= -(t-1)^2 \\
    \left|\begin{array}{ccc}
     1&t-1&-1\\ 1&0&t-2 \\ 0&0&0 
    \end{array}\right| &= 0
 \end{align*}
 so 
 \begin{align*}
  \chr(A) &= t(t-1)^2(t-2) - 0 + (-1)(-(t-1)^2) - 0  \\
          &= (t-1)^2(t(t-2)+ 1) \\
          &= (t-1)^2(t^2-2t+1) = (t-1)^4.
 \end{align*}
 It follows that the minimal polynomial is $(t-1)^k$ for some $k$ with
 $1\leq k\leq 4$.  We have
 \begin{align*}
  (A-I) &= \bbm -1 & 0 & 1 & 1 \\
                -1 & 0 & 1 & 0 \\
                -1 & 0 & 1 & 1 \\
                 0 & 0 & 0 & 0 \ebm \\
  (A-I)^2 &= 0.
 \end{align*}
 It follows that the minimal polynomial must be $(t-1)^2$.  We also
 see that $\rank(A-I)=2$, so $\dim(\ker(A-I))=4-2=2$, so there are $2$
 blocks in the JNF.  The JNF is thus $J(1,k_1)\op J(1,k_2)$ for some
 $k_1,k_2$ with $k_1\leq k_2$.  By comparing characteristic
 polynomials we find that $k_1+k_2=2$, and by comparing minimal
 polynomials we find that $k_2=\max(k_1,k_2)=2$.  It follows that
 $k_1=k_2=2$, so the JNF is $J(1,2)\op J(1,2)$.
\end{example}

\begin{proposition}\lbl{prop-conj-char}
 If $A$ is conjugate to $B$, then $\chr(A)=\chr(B)$ and
 $\min(A)=\min(B)$ and $\dim(\ker(A-\lm I))=\dim(\ker(B-\lm I))$ for
 all $\lm\in\C$.
\end{proposition}
\begin{proof}
 As $A$ and $B$ are conjugate, we have $A=PBP^{-1}$ for some
 invertible matrix $P$.  It follows that
 $P(tI-B)P^{-1}=tPP^{-1}-PBP^{-1}=tI-A$ so 
 \[ \det(tI-A)=\det(P(tI-B)P^{-1})=
    \det(P)\det(tI-B)\det(P)^{-1}= \det(tI-B),
 \] 
 so $\chr(A)=\chr(B)$ as claimed. 

 Next, we have $f(A)=0$ iff $f.m=0$ for all $m\in M_A$, and $f(B)=0$
 iff $f.m=0$ for all $m\in M_B$.  As $A$ is conjugate to $B$ the
 modules $M_A$ and $M_B$ are isomorphic, so $f(A)=0$ iff $f(B)=0$.
 (More directly, one can check that $f(A)=Pf(B)P^{-1}$, and again it
 follows that $f(A)=0$ iff $f(B)=0$.)  Thus, the ideals $I_A$ and
 $I_B$ are the same, so they have the same monic generator, in other
 words $\min(A)=\min(B)$.

 Finally, note that $\ker(A-\lm I)=\{m\in M_A\st (x-\lm)m=0\}$ and
 $\ker(B-\lm I)=\{m\in M_B\st (x-\lm)m=0\}$.  As $M_A\simeq M_B$ we
 see that these two vector spaces are isomorphic and thus have the
 same dimension, as claimed.
\end{proof} 

\begin{proposition}\lbl{prop-oplus-char}
 Let $A$ and $B$ be square matrices of sizes $n$ and $m$.  Then
 $\chr(A\op B)=\chr(A)\chr(B)$, and $\min(A\op B)$ is the least
 common multiple of $\min(A)$ and $\min(B)$.  Moreover, for any
 $\lm\in\C$ we have 
 \[ \dim\ker(A\op B-\lm I_{n+m}) = 
     \dim\ker(A-\lm I_n) + \dim\ker(B-\lm I_m). 
 \]
\end{proposition}
\begin{proof}
 We first claim that $\det(A\op B)=\det(A)\det(B)$.  We have
 \[ A\op B = \blockmat{A}{0}{0}{B} = 
    \blockmat{A}{0}{0}{I_m} \blockmat{I_n}{0}{0}{B} =
     (A\op I_m)(I_n\op B),
 \]
 so $\det(A\op B)=\det(A\op I_m)\det(I_n\op B)$.  By expanding along
 the top row we find that $\det(I_n\op B)=\det(I_{n-1}\op B)$, and it
 follows inductively that $\det(I_n\op B)=\det(B)$ for all $n$.
 Similarly, by expanding along the bottom row we see that
 $\det(A\op I_m)=\det(A)$ for all $m$, so the equation
 $\det(A\op B)=\det(A\op I_m)\det(I_n\op B)$ gives
 $\det(A\op B)=\det(A)\det(B)$ as claimed.

 Next, observe that $tI_{n+m}-A\op B=(tI_n-A)\op(tI_m-B)$, so
 $\det(tI_{n+m}-A\op B)=\det(tI_n-A)\det(tI_m-B)=\chr(A)\chr(B)$ as
 claimed.

 Next, note that $f(A\op B)=f(A)\op f(B)$.  Thus
 \begin{align*}
  f \text{ is divisible by } \min(A\op B) 
   & \iffa f(A\op B) = 0\\
   & \iffa f(A) = 0 \text{ and } f(B) = 0 \\
   & \iffa f \text{ is divisible by both }
           \min(A) \text{ and } \min(B).
 \end{align*}
 This means that $\min(A\op B)$ is the least common multiple of
 $\min(A)$ and $\min(B)$.

 Finally, we can regard $\C^{n+m}$ as $\C^n\op\C^m$ and we then have
 $(A\op B).(u,v)=(Au,Bv)$.  It follows that
 $(A\op B-\lm I).(u,v)=(Au-\lm u,Bv-\lm v)$, and thus that
 $\ker(A\op B-\lm I)$ is the set of pairs $(u,v)$ for which
 $(A-\lm I)u=0$ and $(B-\lm I)v=0$.  In other words we have
 $\ker(A\op B-\lm I)=\ker(A-\lm I)\op\ker(B-\lm I)$ and so 
 $\dim\ker(A\op B-\lm I)=\dim\ker(A-\lm I)+\dim\ker(B-\lm I)$.
\end{proof}

\begin{proposition}\lbl{prop-jordan-char}
 We have $\chr(J(\lm,k))=\min(J(\lm,k))=(t-\lm)^k$.  Moreover, we have 
 \[ \dim\ker(J(\lm,k)-\mu I)=
     \begin{cases} 1 & \text{ if } \lm = \mu \\
                   0 & \text{ if } \lm \neq \mu.
     \end{cases}
 \]
\end{proposition}
\begin{proof}
 I first claim that $\det(J(\lm,k))=\lm^k$.  To see this, we need to
 recall the usual row-expansion method for evaluating determinants.
 Suppose we have a $k\tm k$ matrix $A$ with entries $a_1,\ldots,a_k$
 on the top row, and that $A_i$ is obtained from $i$ by deleting the
 top row and the $i$'th column; then
 \[ \det(A) = a_1\det(A_1)-a_2\det(A_2)+\ldots\pm a_k\det(A_k). \]
 If we take $A=J(\lm,k)$ then $a_1=\lm$, $A_1=J(\lm,k-1)$ and
 $a_2=\ldots=a_k=0$, so $\det(J(\lm,k))=\lm\det(J(\lm,k-1))$.
 Moreover, $J(\lm,1)$ is the $1\tm 1$ matrix $(\lm)$ so
 $\det(J(\lm,1))=\lm$.  It follows inductively that
 $\det(J(\lm,k))=\lm^k$ for all $k>0$, as claimed.  Here we illustrate
 the case $k=5$:
 \[ J(\lm,5) = 
     \left(\begin{array}{c|cccc}
       \lm & 0 & 0 & 0 & 0 \\ \hline
       1 & \lm & 0 & 0 & 0 \\
       0 & 1 & \lm & 0 & 0 \\
       0 & 0 & 1 & \lm & 0 \\
       0 & 0 & 0 & 1 & \lm 
     \end{array}\right) =
     \blockmat{\lm}{0}{*}{J(\lm,4)}.
 \]
 
 Next, note that $J(\lm,k)-tI=J(\lm-t,k)$, so
 $\det(J(\lm,k)-tI)=(\lm-t)^k$.  For a $k\tm k$ matrix we have
 $\det(-A)=(-1)^k\det(A)$, so
 $\det(tI-J(\lm,k))=(-1)^k(\lm-t)^k=(t-\lm)^k$ as claimed. 

 We next need to understand which polynomials $f(x)$ have
 $f(J(\lm,k))=0$.  This happens iff $f.m=0$ for all $m$ in the module
 $M_{J(\lm,k)}$, which is isomorphic to $B(\lm,k)=\C[x]/(x-\lm)^k$.
 It is clear that $f.B(\lm,k)=\{0\}$ iff $f$ is divisible by
 $(x-\lm)^k$, so $\min(J(\lm,k))=(x-\lm)^k$ as well.

 Finally, note that $J(\lm,k)-\mu I=J(\lm-\mu,k)$.  If $\mu\neq\lm$ we
 deduce that $\det(J(\lm,k)-\mu I)=(\lm-\mu)^k\neq 0$, so
 $J(\lm,k)-\mu I$ is invertible and $\ker(J(\lm,k)-\mu I)=\{0\}$.  On
 the other hand, if $\mu=\lm$ then $J(\lm,k)-\mu I=J(0,k)$, and it is
 not hard to see that
 $J(0,k).(x_1,\ldots,x_k)=(0,x_1,\ldots,x_{k-1})$.  Thus
 $J(0,k).\un{x}=0$ iff $x_1=\ldots=x_{k-1}=0$, so
 $\un{x}=(0,\ldots,0,x_k)$ for some $x_k\in\C$.  It follows that
 $\ker(J(\lm,k)-\lm I)$ is the span of the standard basis vector
 $e_k$, and so $\dim\ker(J(\lm,k)-\lm I)=1$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm-char-min}]
 Let $A'$ be the JNF of $A$, so $A'$ is conjugate to $A$ and is a
 block sum of matrices of the form $J(\lm,k)$ for various $\lm$'s and
 $k$'s.  We know from Proposition~\ref{prop-conj-char} that
 $\chr(A)=\chr(A')$ and $\min(A)=\min(A')$ and
 $\dim\ker(A-\lm I)=\dim\ker(A'-\lm I)$ for all $\lm$.

 First suppose that all the Jordan blocks in $A'$ have the same
 eigenvalue $\lm$.  Then we can write
 \[ A' = J(\lm,k_1) \op \ldots \op J(\lm,k_d) \]
 for some sequence $k_1,\ldots,k_d$ of positive integers.  We then
 have
 \[ \chr(A')=\prod_i\chr(J(\lm,k_i))=\prod_i(t-\lm)^{k_i}
     = (t-\lm)^k, 
 \]
 where $r=\sum_ik_i$.  We also see that $\min(A')$ is the least common
 multiple of the polynomials $(t-\lm)^{k_i}$, which is $(t-\lm)^s$,
 where $s=\max(k_1,\ldots,k_d)$.  As each $k_i$ is positive this means
 that $0<s\leq r$.  Finally, we see that 
 \begin{align*}
  \dim\ker(A-\lm I) &= \dim\ker(A'-\lm I) \\
   &= \sum_{i=1}^d \dim\ker(J(\lm,k_i)-\lm I) \\
   &= \sum_{i=1}^d 1 = d.
 \end{align*}
 A similar argument shows that $\dim\ker(A-\mu I)=0$ for $\mu\neq\lm$.
 
 More generally, there will be a number of different eigenvalues, say
 $\lm_1,\ldots,\lm_d$.  Let $A'_i$ be the block sum of all the terms
 of the form $J(\lm_i,k)$ for some $k$.  The above argument shows that
 $\chr(A'_i)=(t-\lm_i)^{r_i}$ and $\min(A'_i)=(t-\lm_i)^{s_i}$ for
 some integers $r_i,s_i$ with $0<s_i\leq r_i$.  We also have
 $A'=A'_1\op\ldots\op A'_d$, so 
 \[ \chr(A)=\chr(A')=\prod_i(t-\lm_i)^{r_i}. \]
 Similarly, we find that $\min(A)$ is the least common multiple of the
 polynomials $(t-\lm_1)^{s_1},\ldots,(t-\lm_d)^{s_d}$.  As these
 polynomials are all powers of inequivalent irreducibles, their lcm is
 just their product, so 
 \[ \min(A) = \prod_i (t-\lm_i)^{s_i}. \]

 We also see that $\dim\ker(A-\lm_i)=\sum_j\dim\ker(A_j-\lm_i)$.  The
 terms for $j\neq i$ are zero, and the term for $j=i$ is just the
 number of Jordan blocks in $A_i$.

 The theorem now follows immediately.
\end{proof}

We conclude by studying the structure of cyclic modules over $\C[x]$.

Let $f(x)$ be a monic polynomial of degree $n$ over $\C$.  We then
have a cyclic module $\C[x]/f(x)$ over $\C[x]$.  For any polynomial
$g(x)\in\C[x]$ we have an element
$\ov{g(x)}=g(x)+\C[x]f(x)\in\C[x]/f(x)$, and $\ov{g(x)}=\ov{h(x)}$ iff
$g(x)-h(x)$ is divisible by $f(x)$.  
\begin{proposition}\lbl{prop-cyclic-basis}
 If $f$ is as above then the elements
 $\ov{1},\ov{x},\ldots,\ov{x}^{n-1}$ form a basis for $\C[x]/f(x)$
 over $\C$.  In particular, we have $\dim_\C(\C[x]/f(x))=n$.
\end{proposition}
\begin{proof}
 Every element of $\C[x]/f(x)$ can be written as $\ov{g(x)}$ for some
 $g(x)\in\C[x]$.  We can divide $g$ by $f$ to get $g(x)=f(x)q(x)+r(x)$
 for some polynomial $r(x)$ of degree at most $n-1$.  It follows that
 $g(x)-r(x)$ is divisible by $f(x)$, so $\ov{g(x)}=\ov{r(x)}$.  As
 $\deg(r)<n$ we have $r(x)=a_0+a_1x+\ldots+a_{n-1}x^{n-1}$ for some
 $a_0,\ldots,a_{n-1}\in\C$.  It follows that
 $\ov{r(x)}=\sum_{i=0}^{n-1}a_i\ov{x}^i$, so the elements
 $\ov{1},\ov{x},\ldots,\ov{x}^{n-1}$ span $\C[x]/f(x)$ over $\C$.

 Now suppose we have a linear relation among these elements, say
 $b_0\ov{1}+\ldots+b_{n-1}\ov{x}^{n-1}=\ov{0}$.  This means that the
 polynomial $g(x):=b_0+b_1x+\ldots+b_{n-1}x^{n-1}$ satisfies
 $\ov{g(x)}=\ov{0}$, so $g$ is divisible by $f$.  As the degree of $g$
 is less than the degree of $f$,this can only happen if $g=0$, which
 means that $b_0=\ldots=b_{n-1}=0$.  Thus, the elements
 $\ov{1},\ldots,\ov{x}^{n-1}$ are linearly independent over $\C$.
\end{proof}

\begin{theorem}\lbl{thm-JNF-cyclic}
 Let $A$ be an $n\tm n$ matrix over $\C$, with JNF
 $J(\lm_1,k_1)\op\ldots\op J(\lm_r,k_r)$.  Then the following
 statements are all equivalent (so if any one of them is true, then
 all of them are true):
 \begin{itemize}
  \item[(a)] $M_A$ is a cyclic module over $\C[x]$.
  \item[(b)] The numbers $\lm_i$ are all different.
  \item[(c)] $\min(A)=\chr(A)$.
  \item[(d)] $M_A\simeq\C[x]/\chr(A)(x)$.
  \item[(e)] There is a vector $v\in M_A$ such that
   $\{v,Av,\ldots,A^{n-1}v\}$ is a basis for $M_A$ over $\C$.
 \end{itemize}
\end{theorem}
\begin{proof}
 (a)$\Rightarrow$(b):
  If $M_A$ is cyclic then $M_A\simeq\C[x]/f(x)$ for some polynomial
  $f(x)$.  This must be nonzero, otherwise $M_A$ would be the same as
  $\C[x]$ and thus would have infinite dimension over $\C$, which is
  impossible because $\dim_\C(M_A)=n$.  We can factor $f(x)$ as
  $c(x-\mu_1)^{k_1}\ldots(x-\mu_s)^{k_s}$ for some nonzero constant
  $c$ and numbers $\mu_1,\ldots,\mu_s$.  By collecting terms we may
  assume that the $\mu_i$'s are all different.  Using the Chinese
  Remainder Theorem we find that 
  \[ M_A\simeq\C[x]/f(x)=\bigoplus_i\C[x]/(x-\mu_i)^{k_i}=
     \bigoplus_iB(\mu_i,k_i),
  \]
  so the JNF of $A$ is $J(\mu_1,k_1)\op\ldots\op J(\mu_s,k_s)$.  Thus
  the $\mu$'s are the same as the $\lm$'s (up to possible reordering)
  and so the $\lm_i$'s are all different.
 
 (b)$\Rightarrow$(a): Conversely, suppose that the $\lm_i$'s are all
  different.  Then the polynomials $(x-\lm_i)^{k_i}$ are all coprime to
  each other, so if we put $f(x)=\prod_i(x-\lm_i)^{k_i}$ the Chinese
  Remainder Theorem tells us that 
  \[ \C[x]/f(x) \simeq \bigoplus_i\C[x]/(x-\lm_i)^{k_i} = 
        \bigoplus_i B(\lm_i,k_i) \simeq M_A,
  \]
  so $M_A$ is cyclic.

 (b)$\iffa$(c): Note that $A$ is conjugate to
  $J(\lm_1,k_1)\op\ldots\op J(\lm_r,k_r)$.  We know from
  Propositions~\ref{prop-conj-char}, \ref{prop-oplus-char}
  and~\ref{prop-jordan-char} that $\chr(A)$ is 
  the product of the polynomials $(x-\lm_i)^{k_i}$, and that $\min(A)$
  is their least common multiple.  The product is the same as the
  least common multiple iff the factors $(x-\lm_i)^{k_i}$ are all
  coprime to each other, which is true iff the numbers $\lm_i$ are all
  different. 

 (a)$\iffa$(d): If~(d) holds (ie $M_A\simeq\C[x]/\chr(A)(x)$) then
  $M_A$ has the form $\C[x]/f(x)$ and is certainly cyclic.

  Conversely, suppose that~(a) holds, so $M_A\simeq\C[x]/f(x)$ for some
  $f$.  Just as in the proof that (a)$\Rightarrow$(b) we see that $f$
  is a unit multiple of $\prod_i(x-\lm_i)^{k_i}$, which we know is the
  same as $\chr(A)(x)$.  It follows that $M_A\simeq\C[x]/\chr(A)(x)$,
  so~(d) holds.

 (e)$\Rightarrow$(a): Suppose that $v\in M_A$ and that
  $\{v,Av,\ldots,A^{n-1}v\}$ is a basis for $M_A$ over $\C$.  Then for
  any $w\in M_A$ there exist $a_0,\ldots,a_{n-1}\in\C$ such that
  $w=a_0v+a_1Av+\ldots+a_{n-1}A^{n-1}v$.  Thus, if we put
  $g(x)=\sum_ia_ix^i\in\C[x]$ we find that $w=g.v$.  It follows that
  $v$ generates $M_A$ as a $\C[x]$-module, so $M_A$ is cyclic.

 (a)$\Rightarrow$(e): Suppose that $M_A$ is cyclic, so we can choose
  an isomorphism $\al\:\C[x]/f(x)\xra{}M_A$.  It follows that
  $\deg(f)=\dim(\C[x]/f(x))=\dim(M_A)=n$.  It follows from
  Proposition~\ref{prop-cyclic-basis} that
  $\{\ov{1},\ldots,\ov{x}^{n-1}\}$ is a basis for $\C[x]/f(x)$, and
  thus that $\{\al(\ov{1}),\ldots,\al(\ov{x}^{n-1})\}$ is a basis for
  $M_A$.  If we put $v=\al(\ov{1})$ we find that
  $\al(\ov{x}^k)=\al(x^k.\ov{1})=x^k.\al(\ov{1}=A^kv$.  This means
  that our basis is just $\{v,Av,\ldots,A^{n-1}v\}$, as required.
\end{proof}
 
\begin{example}\lbl{eg-JNF-not-cyclic}
 Consider the following matrices:
 \[ A = \bbm 1&1&1&1\\0&1&1&1\\0&0&1&1\\0&0&0&1 \ebm \hspace{3em}
    B = \bbm 1&0&1&1\\0&1&1&1\\0&0&1&0\\0&0&0&1 \ebm.
 \]
 We find that $\chr(A)=\chr(B)=(t-1)^4$.  Moreover $(A-I)^3\neq 0$, so
 $\min(A)=(t-)^4=\chr(A)$, so $M_A$ is cyclic.  On the other hand,
 $(B-I)^2=0$ (and $B-I\neq 0$) so $\min(B)=(t-1)^2\neq\chr(B)$, so
 $M_B$ is not cyclic.
\end{example}
\begin{example}\lbl{eg-JNF-cyclic}
 Consider the following matrix:
 \[ A = \bbm 0&0&0&-3\\0&0&-1&0\\0&1&0&0\\3&0&0&0 \ebm. \]
 We find that 
 \[ \chr(A)=9+10x^2+x^4=(x^2+1)(x^2+9)=(x+i)(x-i)(x+3i)(x-3i). \]
 As the four roots are distinct, we see that the minimal polynomial is
 the same as the characteristic polynomial, so $M_A$ is cyclic.  If we
 put $v=(1,1,1,1)\in\C^4$ we find that 
 \begin{align*}
  v    &= (1,1,1,1) \\
  Av   &= (-3,-1,1,3) \\
  A^2v &= (-9,-1,-1,-9) \\
  A^3v &= (27,1,-1,-27).
 \end{align*}
 By standard methods we can check that these vectors are linearly
 independent and thus form a basis of $\C^4$.  This gives another
 proof that $M_A$ is cyclic.
\end{example}

%\ip{asym}
\begin{exercise}\exlabel{ex-antisymmetric}
 Let $A$ be the matrix
 \[ \bbm 0 & a & b \\ -a & 0 & c \\ -b & -c & 0 \ebm. \]
 We assume that $a,b,c\in\R$, but we consider $A$ as a matrix over
 $\C$ so we get a module $M_A$ over $\C[x]$.  Prove that 
 \[ M_A \simeq M_0 \op M_{ir} \op M_{-ir}, \]
 where $r=\sqrt{a^2+b^2+c^2}$.  
\end{exercise}
\begin{solution}
 The characteristic polynomial is 
 \begin{align*}
  \left|\begin{array}{ccc} 
   t & -a & -b \\ a & t & -c \\ b & c & t 
  \end{array}\right| &= 
  t      \left|\begin{array}{cc} t & -c \\ c & t \end{array}\right| 
  - (-a) \left|\begin{array}{cc} a & -c \\ b & t \end{array}\right| 
  + (-b) \left|\begin{array}{cc} a &  t \\ b & c \end{array}\right| \\
  &= (t^3 + c^2t) + (a^2t + abc) + (-abc + b^2t) \\
  &= t^3 + r^2 t = t (t+ir)(t-ir).
 \end{align*}
 If $r\neq 0$ then the three roots of $\chr(A)$ are distinct.  It
 follows easily that the Jordan normal form is
 $J(0,1)\op J(ir,1)\op J(-ir,1)$ and thus that
 \[ M_A\simeq B(0,1)\op B(ir,1)\op B(-ir,1) =
    M_0\op M_{ir} \op M_{-ir}.
 \]
 In the exceptional case where $r=0$ we must have $a=b=c=0$ so $A$ is
 just the zero matrix and it is clear that
 $M_A\simeq M_0\op M_0\op M_0$. 
\end{solution}

%\ip{circulant}
\begin{exercise}\exlabel{ex-circulant}
 (This question is quite elaborate.)
 A $3\tm 3$ circulant matrix is a matrix of the following form:
 \[ A = \bbm u & v & w \\ v & w & u \\ w & u & v \ebm. \]
 We will assume that $u$, $v$ and $w$ are real numbers.  Put $a=u+v+v$
 and $b=((u-v)^2+(v-w)^2+(w-u)^2)/2$.
 \begin{itemize}
  \item[(a)] Show that the characteristic polynomial of $A$ is
   $(x-a)(x^2-b)$ [You may wish to start by performing some row and
   column operations rather than just wading in and calculating the
   determinant.]
  \item[(b)] Show that if $uv+vw+wu\neq 0$ and $u,v,w$ are not all the
   same then the minimal polynomial is equal to the characteristic
   polynomial. 
  \item[(c)] Show that if $uv+vw+wu=0$ and $a\neq 0$ then the minimal
   polynomial is $x^2-a^2$.
  \item[(d)] Calculate and factorise the minimal polynomial when
   $u=-13$, $v=11$ and $w=10$.
  \item[(e)] Calculate and factorise the minimal polynomial when
   $u=-2$, $v=3$ and $w=6$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The characteristic polynomial is by definition the
   determinant of the first matrix below.  The second matrix is
   obtained by adding the second and third rows to the top row, and
   the third matrix is obtained by subtracting the first column from
   each of the other columns.  It is a standard fact that these
   operations do not change the determinant.
   \[ \bbm x-u & -v & -w \\ -v & x-w & -u \\ -w & -u & x-v \ebm \xra{}
      \bbm x-a &x-a &x-a \\ -v & x-w & -u \\ -w & -u & x-v \ebm \xra{}
      \bbm x-a &0 &0 \\ -v & x+(v-w) & v-u \\ -w & w-u & x-(v-w)\ebm.
   \]
   The determinant of the last matrix can be expanded out directly to
   give 
   \begin{align*}
    (x-a)(x^2-(v-w)^2-(w-u)(v-u)) &= 
      (x-a)(x^2-v^2-w^2+2vw-vw+uv+uw-u^2) \\
     &= (x-a)(x^2-u^2-v^2-w^2+uv+vw+wu).
   \end{align*}
   On the other hand, we have
   \begin{align*}
    b &= ((u-v)^2+(v-w)^2+(w-u)^2)/2 \\
      &= (u^2+v^2-2uv + v^2+w^-2vw + w^2+u^2-2uw)/2 \\
      &= (2u^2+2v^2+2w^2-2uv-2vw-2wu)/2 \\
      &= u^2+v^2+w^2-uv-vw-wu.
   \end{align*}
   Using this, we can rewrite the characteristic polynomial as
   $(x-a)(x^2-b)$.
  \item[(b)] Suppose that the numbers $u$, $v$ and $w$ are not all the
   same, and that $uv+vw+wu\neq 0$.  The characteristic polynomial of
   $A$ is $(x-a)(x-\sqrt{b})(x+\sqrt{b})$; if we can show that the
   three roots are distinct, it will follows that this is the same as
   the minimal polynomial.

   As $u$, $v$ and $w$ are not all the same, at least one of the
   numbers $u-v$, $v-w$ and $w-u$ is nonzero.  Thus $(u-v)^2$,
   $(v-w)^2$ and $(w-u)^2$ are all nonnegative, and one of them is
   strictly positive, so the number $b=((u-v)^2+(v-w)^2+(w-u)^2)/2$ is
   strictly positive.  This means that $\sqrt{b}>0$, so
   $\sqrt{b}\neq-\sqrt{b}$.

   We also have 
   \[ a^2-b=(u+v+w)^2-(u^2+v^2+w^2-uv-vw-wu)=3(uv+vw+wu)\neq 0, \]
   so $a\neq\pm\sqrt{b}$.  Thus, the three roots are all different, as
   required.

  \item[(c)] If $uv+vw+wu=0$ we see from the above equation that
   $a^2=b$, so the characteristic polynomial is
   $(x-a)(x^2-a^2)=(x-a)^2(x+a)$.  By assumption we have $a\neq 0$ so
   $x-a$ and $x+a$ are coprime.  We know from the general theory that
   the minimal polynomial divides the characteristic polynomial and
   has precisely the same roots, so it is either $(x-a)(x+a)=x^2-a^2$
   or $(x-a)(x+a)^2$.  By direct calculation one finds that 
   \[ A^2-a^2I = (uv+vw+wu)\bbm -2&1&1\\1&-2&1\\1&1&-2 \ebm, \]
   which is the zero matrix because $uv+vw+wu=0$.  Thus the minimal
   polynomial is $x^2-a^2$, as claimed.

  \item[(d)] If $u=-13$, $v=11$ and $w=2$ then $a=-13+11+2=0$ and
   $b=((-13-11)^2+(11-2)^2+(2-(-13))^2)/2=441=21^2$.  The
   characteristic polynomial is $x(x^2-441)=x(x-21)(x+21)$.  The three
   roots are clearly distinct, so this is also the minimal polynomial.

  \item[(e)] If $u=-2$, $v=3$ and $w=6$ then $a=7$ and
   $uv+vw+wu=-6+18-12=0$ so part~(c) applies and the minimal polynomial
   is $x^2-49$.
 \end{itemize}
\end{solution}

%\ip{jordan0}
\begin{exercise}\exlabel{ex-JNF-i}
 Consider the following matrix over $\C$: 
 \[ A = \bbm -1&1&1&-1 \\ 0&-1&0&1 \\ 0&0&-1&-1 \\ 0&0&0&-1 \ebm.  \]
 \begin{itemize}
  \item[(a)] What is the characteristic polynomial of $A$?
  \item[(b)] What is the minimal polynomial of $A$?
  \item[(c)] What is the rank of the matrix $A+I$?
  \item[(d)] Which direct sum of basic $\C[x]$-modules is isomorphic
   to $M_A$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The characteristic polynomial is the determinant of the
   matrix 
   \[ tI - A =
       \bbm 1+t&-1&-1&1 \\ 0&1+t&0&-1 \\ 0&0&1+t&1 \\ 0&0&0&1+t
       \ebm.
   \]
   As this is an upper-triangular matrix, the determinant is just the
   product of the diagonal entries, which is $(1+t)^4$.
  \item[(b)] The minimal polynomial is a divisor of the characteristic
   polynomial, so it must by $(1+t)^k$ for some integer $k\leq 4$.
   The matrix $I+A$ is clearly nonzero, but 
   \[ (I+A)^2 = 
      \bbm 0&1&1&-1 \\ 0&0&0&1 \\ 0&0&0&-1 \\ 0&0&0&0 \ebm
      \bbm 0&1&1&-1 \\ 0&0&0&1 \\ 0&0&0&-1 \\ 0&0&0&0 \ebm = 
      \bbm 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \ebm.
   \]
   It follows that the minimal polynomial of $A$ is $(t+1)^2$.
  \item[(c)] The first two rows of $I+A$ are clearly linearly
   independent, but the third row is minus the second, and the last
   row is zero.  Thus, the rank is two.
  \item[(d)] If $M_A$ is a direct sum of modules of the form
   $B(\lm,k)$, then the numbers $\lm$ must be roots of the minimal
   polynomial of multiplicity at least $k$.  The only root of the
   minimal polynomial is $-1$, which has multiplicity $2$.  It follows
   that the only possible summands in $M_A$ are $B(-1,1)$ (which has
   dimension $1$ over $\C$) and $B(-1,2)$ (which has dimension $2$
   over $\C$).  As $x+1$ has rank $0$ on $B(-1,1)$ and rank $1$ on
   $B(-1,2)$, we see that the number of copies of $B(-1,2)$ is the
   rank of $x+1$ on $M_A$, or in other words the rank of $A+I$, which
   is $2$.  The two copies of $B(-1,2)$ have total dimension $4$,
   which is the dimension of $M_A$, so there cannot be any copies of
   $B(-1,1)$ and we have $M_A\simeq B(-1,2)\op B(-1,2)$.
 \end{itemize}
\end{solution}

%\ip{jordan1}
\begin{exercise}\exlabel{ex-JNF-ii}
 Consider the following matrix over $\C$: 
 \[ A = \bbm 1&2&3&4 \\ 0&1&0&5 \\ 0&0&1&6 \\ 0&0&0&1 \ebm.  \]
 \begin{itemize}
  \item[(a)] What is the characteristic polynomial of $A$?
  \item[(b)] What is the minimal polynomial of $A$?
  \item[(c)] Which direct sum of basic $\C[x]$-modules is isomorphic
   to $M_A$?
  \item[(d)] What is the Jordan normal form of $A$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The characteristic polynomial is the determinant of the
   matrix 
   \[ tI - A =
       \bbm t-1&-2&-3&-4 \\ 0&t-1&0&-5 \\ 0&0&t-1&-6 \\ 0&0&0&t-1
       \ebm.
   \]
   As this is an upper-triangular matrix, the determinant is just the
   product of the diagonal entries, which is $(t-1)^4$.
  \item[(b)] The minimal polynomial is a divisor of the characteristic
   polynomial, so it must be $(t-1)^k$ for some integer $k\leq 4$.
   We have 
   \begin{align*}
    (A-I)^2 &= 
     \bbm 0&2&3&4 \\ 0&0&0&5 \\ 0&0&0&6 \\ 0&0&0&0 \ebm
     \bbm 0&2&3&4 \\ 0&0&0&5 \\ 0&0&0&6 \\ 0&0&0&0 \ebm = 
     \bbm 0&0&0&28 \\ 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \ebm \neq 0 \\
    (A-I)^3 &= 
     \bbm 0&0&0&28 \\ 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \ebm 
     \bbm 0&2&3&4 \\ 0&0&0&5 \\ 0&0&0&6 \\ 0&0&0&0 \ebm = 
     \bbm 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \\ 0&0&0&0 \ebm,
   \end{align*}
   and it follows that the minimal polynomial is $(t-1)^3$.
  \item[(c)] As $1$ is the only root of the minimal polynomial, the
   module $M_A$ is isomorphic to a direct sum of modules of the form
   $B(1,k)$.  As $1$ is a triple root of the minimal polynomial, we
   must have at least one summand of the form $B(1,3)$.  This has
   dimension $3$ and $M_A$ has dimension $4$ so there is only one
   dimension left over, so the other summand must be $B(1,1)$.  We
   thus have $M_A\simeq B(1,3)\op B(1,1)$.
  \item[(d)] We read off from this that $A$ is conjugate to
   $J(1,3)\op J(1,1)$, which is the following matrix:
   \[ \bbm 1&0&0&0\\1&1&0&0\\0&1&1&0\\0&0&0&1 \ebm. \]
 \end{itemize}
\end{solution}

%\ip{jordan2}
\begin{exercise}\exlabel{ex-JNF-iii}
 Consider the following matrix over $\C$: 
 \[ A = \bbm -1&0&1&0 \\ 0&1&2&0 \\ 0&0&-1&0 \\ 0&1&1&1 \ebm. \]
 \begin{itemize}
  \item[(a)] What is the characteristic polynomial of $A$?
  \item[(b)] What are the ranks of $A+I$ and $A-I$?
  \item[(c)] What is the Jordan normal form of $A$?
  \item[(d)] Show that $M_A$ is cyclic.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The characteristic polynomial is the determinant of the
   matrix 
   \[ tI - A =
       \bbm t+1 & 0   & -1  & 0 \\
            0   & t-1 & -2  & 0 \\
            0   & 0   & t+1 & 0 \\
            0   & -1  & -1  & t-1
       \ebm.
   \]
   Expanding along the top row and ignoring the $0$'s we get 
   \[ \det(tI-A) = 
       (t+1)\left| \begin{array}{ccc} 
             t-1 & -2 & 0 \\ 0 & t+1 & 0 \\ -1 & -1 & t-1 
            \end{array} \right| +
       (-1) \left| \begin{array}{ccc} 
             0 & t-1 & 0 \\ 0 & 0 & 0 \\ 0 & -1  & t-1 
            \end{array} \right|
   \]
   The second term here is easily seen to be $0$.  For the first term,
   we have
   \begin{align*}
    \left| \begin{array}{ccc} 
     t-1 & -2 & 0 \\ 0 & t+1 & 0 \\ -1 & -1 & t-1 
    \end{array} \right| &= 
    (t-1) \left|\begin{array}{cc}t+1&0\\-1&t-1\end{array}\right| -
    (-2) \left|\begin{array}{cc}0&0\\-1&t-1\end{array}\right| \\
    &= (t-1)^2(t+1)-(-2)(0) = (t-1)^2(t+1).
   \end{align*}
   It follows that $\chr(A)(t)=(t-1)^2(t+1)^2$.

   A slightly quicker approach is possible if you know that
   determinants can be expanded along any row or column, not just
   along the top row.  We then have
   \begin{align*}
       \left|\begin{array}{cccc} 
        t+1 & 0   & -1  & 0 \\
        0   & t-1 & -2  & 0 \\
        0   & 0   & t+1 & 0 \\
        0   & -1  & -1  & t-1
      \end{array}\right| &\stackrel{1}{=}
       (t+1)\left| \begin{array}{ccc}
             t-1 & -2 & 0 \\ 0 & t+1 & 0 \\ -1 & -1 & t-1 
        \end{array} \right| \\ & \stackrel{2}{=}
       (t+1)(t-1)\left| \begin{array}{cc}
             t+1 & 0 \\ -1 & t-1 
            \end{array} \right| \\
       &= (t+1)^2(t-1)^2,
   \end{align*}
   where in step~$1$ we have expanded using the first column, and in
   step~$2$ we have expanded using the third column.

  \item[(b)] We have
   \[ A+I = \bbm 0&0&1&0\\0&2&2&0\\0&0&0&0\\0&1&1&2 \ebm \hspace{4em}
      A-I = \bbm -2&0&1&0\\0&0&2&0\\0&0&-2&0\\0&1&1&0\ebm.
   \]
   In $A+I$ the nonzero rows are clearly linearly independent, so the
   rank is three.  In $A-I$ the middle two rows are multiples of each
   other but the first, second and fourth rows are linearly
   independent, so the rank is again three.

  \item[(c)] It follows from~(a) that the JNF of $A$ contains only
   blocks of type $J(1,k)$ or $J(-1,k)$.  Theorem~14.7(b) tells us
   that the number of blocks of type $J(1,k)$ is $4-\rank(A-I)$, which
   is equal to $1$ by~(b).  We thus have only one block of type
   $J(1,k)$, which has to contribute a factor $(t-1)^2$ in the
   characteristic polynomial, so it must be $J(1,2)$.  Similarly, we
   have only one block of type $J(-1,k)$ and it must be $J(-1,2)$.
   Thus the JNF of $A$ is $J(1,2)\op J(-1,2)$.

  \item[(d)] It follows from~(c) that 
   \[ M_A\simeq B(1,2)\op B(-1,2) =
       \C[x]/(x-1)^2\op\C[x]/(x+1)^2\simeq \C[x]/((x-1)^2(x+1)^2).
   \]
   (The last equality uses the Chinese Remainder Theorem, which is
   valid because $(x-1)^2$ and $(x+1)^2$ are coprime.)  It follows
   that $M_A$ is cyclic, as claimed.
 \end{itemize}
\end{solution}

%\ip{jordan3}
\begin{exercise}\exlabel{ex-JNF-iv}
 Put $\al=\sqrt{-8}$.  Consider the following matrix over $\C$: 
 \[ A = \bbm 0&0&0&1 \\ 0&0&1&\al \\ 0&1&\al&-4 \\ 1&\al&-4&-\al\ebm.
 \]
 \begin{itemize}
  \item[(a)] What is the characteristic polynomial of $A$?
  \item[(b)] What are the ranks of $A+I$ and $A-I$?
  \item[(c)] What is the Jordan normal form of $A$?
  \item[(d)] Show that $M_A$ is cyclic.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] $(t-1)^2(t+1)^2$
  \item[(b)] $3$ and $3$.
  \item[(c)] It follows from~(a) that the JNF of $A$ contains only
   blocks of type $J(1,k)$ or $J(-1,k)$.  Theorem~14.7(b) tells us
   that the number of blocks of type $J(1,k)$ is $4-\rank(A-I)$, which
   is equal to $1$ by~(b).  We thus have only one block of type
   $J(1,k)$, which has to contribute a factor $(t-1)^2$ in the
   characteristic polynomial, so it must be $J(1,2)$.  Similarly, we
   hav only one block of type $J(-1,k)$ and it must be $J(-1,2)$.
   Thus the JNF of $A$ is $J(1,2)\op J(-1,2)$.

  \item[(d)] It follows from~(c) that 
   \[ M_A\simeq B(1,2)\op B(-1,2) =
       \C[x]/(x-1)^2\op\C[x]/(x+1)^2\simeq \C[x]/((x-1)^2(x+1)^2).
   \]
   (The last equality uses the Chinese Remainder Theorem, which is
   valid because $(x-1)^2$ and $(x+1)^2$ are coprime.)  It follows
   that $M_A$ is cyclic, as claimed.
 \end{itemize}
\end{solution}


\newpage

\section*{Solutions}
\label{apx-solutions}

\includesolutions

\end{document}
